{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, json, time, string, re, pickle, unicodedata, numpy as np, unicodedata\n",
    "import torch.optim as optim, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torch, torch.nn as nn, torch.nn.functional as func, evaluate as ev, time\n",
    "from tqdm import tqdm\n",
    "from torch import matmul\n",
    "from torch.nn.functional import dropout\n",
    "from torch import LongTensor,ByteTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "random_seed = 11\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [00:24, 3525.37it/s]\n",
      "10570it [00:04, 2371.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train examples = 86422\n",
      "Num dev examples = 10493\n"
     ]
    }
   ],
   "source": [
    "#Load processed datasets without topics\n",
    "\n",
    "train_exs = []\n",
    "dev_exs = []\n",
    "\n",
    "f = open(\"data/datasets_without_topics_new/train-v1.1-processed.txt\", 'r')\n",
    "for line in tqdm(f):\n",
    "    example = json.loads(line)\n",
    "    if len(example['ans'])>0:\n",
    "        train_exs.append(example)\n",
    "f.close()\n",
    "\n",
    "f1 = open(\"data/datasets_without_topics_new/dev-v1.1-processed.txt\", 'r')\n",
    "for line in tqdm(f1):\n",
    "    example = json.loads(line)\n",
    "    if len(example['ans'])>0:\n",
    "        dev_exs.append(example)\n",
    "f1.close()\n",
    "\n",
    "time.sleep(1)\n",
    "print('Num train examples = %d' % len(train_exs))\n",
    "print('Num dev examples = %d' % len(dev_exs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Dev val\n",
      "Mon May  7 13:44:01 2018 Build feature dict train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96915/96915 [00:04<00:00, 20178.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  7 13:44:07 2018 word_set\n",
      "#words 123198\n",
      "#chars 756\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "dev_offsets = {ex['id']: ex['ans_pos'] for ex in dev_exs}\n",
    "print(time.ctime() + \" Build word o index mapping\")\n",
    "time.sleep(1)\n",
    "\n",
    "samples = train_exs + dev_exs\n",
    "word2idx = {'<NULL>':0, '<UNK>':1} \n",
    "char2idx = {'<NULL>':0, '<UNK>':1}\n",
    "\n",
    "word_set = set()\n",
    "char_set = set()\n",
    "feature_set = set()\n",
    "for each_example in tqdm(samples):\n",
    "    \n",
    "    for word in each_example['question']:\n",
    "        word_set.add(word)\n",
    "\n",
    "    for word in each_example['ctxt']:\n",
    "        word_set.add(word)\n",
    "\n",
    "    for char in each_example['ques_char']:\n",
    "        char_set.add(char)\n",
    "\n",
    "    for char in each_example['ctxt_char']:\n",
    "        char_set.add(char)\n",
    "\n",
    "        \n",
    "print(time.ctime() + \" word_set\")\n",
    "\n",
    "for i, word in enumerate(word_set):\n",
    "    if word not in word2idx.keys():\n",
    "        word2idx[word] = i+2\n",
    "\n",
    "\n",
    "for i, char in enumerate(char_set):\n",
    "    if char not in char2idx.keys():\n",
    "        char2idx[char] = i+2\n",
    "\n",
    "print('#words', len(word2idx))    \n",
    "print('#chars', len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(word2idx,char2idx,glove_file):\n",
    "    \n",
    "    corpus_words = set()\n",
    "    for key in word2idx.keys():\n",
    "        if key not in {'<NULL>','<UNK>'}:\n",
    "            corpus_words.add(key)\n",
    "    \n",
    "    corpus_chars = set()\n",
    "    for key in char2idx.keys():\n",
    "        if key not in {'<NULL>','<UNK>'}:\n",
    "            corpus_chars.add(key)\n",
    "            \n",
    "    idx2word = {v: k for k, v in char2idx.items()}\n",
    "    idx2char = {v: k for k, v in char2idx.items()}\n",
    "    \n",
    "    glove_big = {}\n",
    "    with open(glove_file, \"rb\") as infile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode()#.lower()\n",
    "            nums = map(float, parts[1:])\n",
    "            if (word in corpus_words) or (word in corpus_chars):\n",
    "                glove_big[word] = list(nums)\n",
    "                \n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    \n",
    "    weights_matrix = np.random.normal(scale=0.6, size=(len(idx2word), 300))#np.zeros((len(idx2word), 300))\n",
    "    words_found = 0\n",
    "\n",
    "    for word in corpus_words:\n",
    "        if word in glove_big.keys():\n",
    "            weights_matrix[word2idx[word]] = glove_big[word]\n",
    "            words_found += 1\n",
    "    print(\"%d words found out of %d\" %(words_found, len(idx2word)))\n",
    "\n",
    "    weights_matrix_char = np.random.normal(scale=0.6, size=(len(idx2char), 300))#np.zeros((len(idx2word), 300))\n",
    "    chars_found = 0\n",
    "\n",
    "    for char in corpus_chars:\n",
    "        if char in glove_big.keys():\n",
    "            weights_matrix[char2idx[char]] = glove_big[char]\n",
    "            chars_found += 1\n",
    "    print(\"%d chars found out of %d\" %(chars_found, len(idx2char)))\n",
    "    \n",
    "    return weights_matrix, weights_matrix_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98175 words found out of 123198\n",
      "262 chars found out of 756\n"
     ]
    }
   ],
   "source": [
    "weights_matrix, weights_matrix_char = load_embeddings(word2idx,char2idx,'data/glove.840B.300d.txt')\n",
    "\n",
    "emb_layer = nn.Embedding(num_embeddings= len(word2idx), embedding_dim = 300, padding_idx=0).cuda()\n",
    "emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "\n",
    "emb_layer_char = nn.Embedding(num_embeddings= len(char2idx), embedding_dim = 300, padding_idx=0).cuda()\n",
    "emb_layer_char.weight.data.copy_(torch.from_numpy(weights_matrix_char))\n",
    "\n",
    "emb_layer.weight.requires_grad = False\n",
    "emb_layer_char.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating features\n",
    "'''\n",
    "class Transform(Dataset):\n",
    "    def __init__(self, example, word2idx, char2idx,status):#feature2idx\n",
    "        self.example = example\n",
    "        self.status = status\n",
    "        self.word2idx =  word2idx\n",
    "        self.char2idx =  char2idx\n",
    "        \n",
    "        self.feature2idx = {'RB': 4,'DT': 5,'NN': 6,'VBZ': 7,'JJ': 8,'LS': 9,'VB': 10,'NNP': 11,'POS': 12,'IN': 13,'CC': 14,'VBG': 15,'PRP': 16,'NNS': 17,'VBN': 18,'TO': 19,'WRB': 20,\n",
    "        'VBD': 21,'CD': 22,'PDT': 23,'WDT': 24,'WP': 25,'VBP': 26,'UH': 27,'ORG': 28,'FACILITY': 29,'GPE': 30,'PERSON': 31,'JJS': 32,'NNPS': 33,\n",
    "        'RP': 34,'LOCATION': 35,'FW': 36,'JJR': 37,'RBS': 38,'MD': 39,'SYM': 40,'EX': 41,'RBR': 42,'GSP': 43}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.example)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        ctxt = self.example[index]['ctxt']\n",
    "        ques = self.example[index]['question']\n",
    "        ctxt_char = self.example[index]['ctxt_char']\n",
    "        question_char = self.example[index]['ques_char']\n",
    "        total_length_context = len(ctxt)\n",
    "        total_length_question = len(ques)\n",
    "        \n",
    "        cpos_d = self.example[index]['ctxt_pos']\n",
    "        cner_d = self.example[index]['ctxt_ner']\n",
    "        qpos_d = self.example[index]['ques_pos']\n",
    "        qner_d = self.example[index]['qner']\n",
    "        \n",
    "        context_cased = set(ctxt) #all words in context\n",
    "        context_uncased = set([word.lower() for word in ctxt]) #all words in context lowercase\n",
    "        word_ques = [self.word2idx.get(word, 1) for word in ques] # get index of question word\n",
    "        word_con = [self.word2idx.get(word, 1) for word in ctxt] # get index of context word\n",
    "        ques_cased = set(ques)\n",
    "        ques_uncased = set([element.lower() for element in ques])\n",
    "        char_ques = [self.char2idx.get(char, 1) for char in question_char]\n",
    "        char_con = [self.char2idx.get(char, 1) for char in ctxt_char]\n",
    "        context_lemmas = set(self.example[index]['ctxt_lemma'])\n",
    "        ques_lemmas = set(self.example[index]['qlemma'])\n",
    "        count_of_word_context = {word.lower(): ctxt.count(word.lower()) for word in ctxt}\n",
    "        count_of_word_question = {word.lower(): ques.count(word.lower()) for word in ques}\n",
    "        \n",
    "        #creating question and context feature vector\n",
    "        context_feature = torch.zeros(len(ctxt), 44)\n",
    "        ques_feature = torch.zeros(len(ques), 44)\n",
    "        \n",
    "        #exact match features being one \n",
    "        for i in range(len(ctxt)):\n",
    "            if ctxt[i] in ques_cased:\n",
    "                context_feature[i][0] = 1.0\n",
    "            if ctxt[i] in ques_uncased:\n",
    "                context_feature[i][1] = 1.0\n",
    "            if ctxt[i] in ques_lemmas:\n",
    "                context_feature[i][2] = 1.0\n",
    "            \n",
    "        for i in range(len(cpos_d)):\n",
    "            f = cpos_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                context_feature[i][self.feature2idx[f]] = 1.0\n",
    "            \n",
    "        for i in range(len(cner_d)):\n",
    "            f = cner_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                context_feature[i][self.feature2idx[f]] = 1.0\n",
    "        \n",
    "        for i in range(len(qpos_d)):\n",
    "            f = qpos_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                ques_feature[i][self.feature2idx[f]] = 1.0\n",
    "            \n",
    "        for i in range(len(qner_d)):\n",
    "            f = qner_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                ques_feature[i][self.feature2idx[f]] = 1.0\n",
    "\n",
    "        for i in range(len(ques)):\n",
    "            if ques[i] in ques_cased:\n",
    "                ques_feature[i][0] = 1.0\n",
    "            if ques[i] in ques_uncased:\n",
    "                ques_feature[i][1] = 1.0\n",
    "            if ques[i] in ques_lemmas:\n",
    "                ques_feature[i][2] = 1.0\n",
    "            \n",
    "        for i in range(total_length_context):\n",
    "            context_feature[i][3] = float(count_of_word_context[ctxt[i].lower()]/(1.0 * total_length_context))\n",
    "        \n",
    "        for i in range(total_length_question):\n",
    "            ques_feature[i][3] = float(count_of_word_question[ques[i].lower()]/ (1.0 * total_length_question))\n",
    "\n",
    "        if self.status == 'train': # take only the first answer\n",
    "            start_positions = LongTensor(1).fill_(self.example[index]['ans'][0][0])\n",
    "            end_positions = LongTensor(1).fill_(self.example[index]['ans'][0][1])\n",
    "        else: # consider all answers while evaluating\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "            for ans in self.example[index]['ans']:\n",
    "                start_positions.append(ans[0])\n",
    "                end_positions.append(ans[1])\n",
    "                \n",
    "        return LongTensor(word_con), LongTensor(char_con), context_feature, LongTensor(word_ques), LongTensor(char_ques), ques_feature, start_positions, end_positions, self.example[index]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10493\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Transform(train_exs, word2idx, char2idx, status='train') #feature2idx,\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "dev_dataset = Transform(dev_exs, word2idx, char2idx, status='test') #feature2idx,\n",
    "dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "print(len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "padding batches to same length\n",
    "'''\n",
    "def module_fn(batch_len, con_features, con_words, con_char,max_len): #\n",
    "    con1_words = LongTensor(batch_len,max_len).zero_() #N X max_len\n",
    "    con1_chars = LongTensor(batch_len,max_len).zero_()\n",
    "    con1_features = torch.zeros(batch_len,max_len,con_features[0].size(1))\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        con1_words[i,0:con_words[i].size(0)].copy_(con_words[i])\n",
    "        con1_chars[i, 0:con_char[i].size(0)].copy_(con_char[i])\n",
    "        con1_features[i,:con_words[i].size(0)].copy_(con_features[i])\n",
    "\n",
    "    return con1_words, con1_chars, con1_features, #con1_masked_val\n",
    "\n",
    "def pad_batch(batch):\n",
    "    \n",
    "    con_words = []\n",
    "    con_char = []\n",
    "    con_features = []\n",
    "    ques_words = []\n",
    "    ques_char = []\n",
    "    ques_features = []\n",
    "    ids = []\n",
    "\n",
    "    length_con = []\n",
    "    length_ques = []\n",
    "    for val in batch:\n",
    "        con_words.append(val[0])\n",
    "        length_con.append(val[0].size(0))\n",
    "        con_char.append(val[1])\n",
    "        con_features.append(val[2])\n",
    "        ques_words.append(val[3])\n",
    "        length_ques.append(val[3].size(0))\n",
    "        ques_char.append(val[4])\n",
    "        ques_features.append(val[5])\n",
    "        ids.append(val[len(val)-1])\n",
    "\n",
    "    if torch.is_tensor(batch[0][6]): #train\n",
    "        abc1 = []\n",
    "        abc2 = []\n",
    "        for val in batch:\n",
    "            abc1.append(val[6])\n",
    "            abc2.append(val[7])\n",
    "        batch_start_positions = torch.cat(abc1)\n",
    "        batch_end_positions = torch.cat(abc2)\n",
    "    else:        #eval mode\n",
    "        batch_start_positions = []\n",
    "        batch_end_positions = []\n",
    "        for val in batch:\n",
    "            batch_start_positions.append(val[6])\n",
    "            batch_end_positions.append(val[7])\n",
    "    return (*module_fn(len(batch), con_features, con_words, con_char,max(length_con))),length_con,(*module_fn(len(batch), ques_features, ques_words, ques_char,max(length_ques))), length_ques, batch_start_positions, batch_end_positions, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=45, sampler=train_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)\n",
    "#change to 45\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "        dev_dataset, batch_size=32, sampler=dev_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention(attn, con, ques): \n",
    "    \n",
    "    if attn == 'interac': \n",
    "        coattn = matmul(con, ques.transpose(2,1))    \n",
    "    else: \n",
    "        coattn = matmul(ques, ques.transpose(2,1))\n",
    "        #set diagonal elements to 0\n",
    "        for i in range(coattn.size(0)):\n",
    "            mask = torch.diag(torch.ones(min(coattn.size(1),coattn.size(2)))).cuda()\n",
    "            coattn[i, :, :].data = mask*0 + (1. - mask)*coattn[i, :, :].data\n",
    "            \n",
    "    attn_dist = func.softmax(coattn, dim=2) #attention distribution of query for jth context word\n",
    "    attended = matmul(attn_dist, ques) #attended query vector for all context words\n",
    "    return attended\n",
    "\n",
    "def encode(rnn, seq, lengths, flag):\n",
    "    \n",
    "    key=sorted(range(len(lengths)), key=lambda k: lengths[k], reverse=True)\n",
    "\n",
    "    seq = seq.index_select(0, Variable(torch.LongTensor(key).cuda())).transpose(0, 1)\n",
    "    rnn_input = nn.utils.rnn.pack_padded_sequence(seq, sorted(lengths, reverse=True))\n",
    "    rev_key = sorted(range(len(key)), key=lambda k: key[k])\n",
    "    dropout_input = func.dropout(rnn_input.data, p=0.2, training=flag)\n",
    "    rnn_input = nn.utils.rnn.PackedSequence(dropout_input, rnn_input.batch_sizes)\n",
    "    output = rnn(rnn_input)[0]\n",
    "    output = nn.utils.rnn.pad_packed_sequence(output)[0].transpose(0, 1).index_select(0, Variable(torch.LongTensor(rev_key).cuda()))\n",
    "    output = func.dropout(output,p=0.2,training=flag)\n",
    "    \n",
    "    return output.contiguous() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.char_rnn = nn.LSTM(input_size=300, hidden_size=50, bidirectional=True)\n",
    "        doc_input_size = 400 + 44\n",
    "        self.encoding_rnn = nn.LSTM(input_size=doc_input_size, hidden_size=100, bidirectional=True)\n",
    "        self.sfu_l1 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_l2 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_self_l1 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_self_l2 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        \n",
    "        self.final_rec_nn = nn.ModuleList([nn.LSTM(input_size=200, hidden_size=100, bidirectional=True),nn.LSTM(input_size=200, hidden_size=100, bidirectional=True)])\n",
    "        #\n",
    "        FCN = nn.Sequential(\n",
    "                nn.Linear(600, 100),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(100, 1),)\n",
    "        \n",
    "        self.FeedForward_s = nn.ModuleList([FCN,FCN])\n",
    "        self.sfu_s_l1 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.sfu_s_l2 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.FeedForward_e = nn.ModuleList([FCN,FCN])\n",
    "        self.sfu_e_l1 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.sfu_e_l2 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        \n",
    "        \n",
    "    def forward(self, con_embed_words, con_embed_chars, con_feat, con_length, ques_embed_words, ques_embed_chars, ques_feat, ques_length):\n",
    "       \n",
    "        con_char_feat = encode(self.char_rnn, con_embed_chars, con_length, self.training) #encode context characters\n",
    "        ques_char_feat = encode(self.char_rnn, ques_embed_chars, ques_length, self.training)#encode question characters\n",
    "        val1 = torch.cat([con_embed_words,con_char_feat,con_feat], 2)#join all context features\n",
    "        encoded_con = encode(self.encoding_rnn, val1, con_length, self.training)#encode context\n",
    "        val2 = torch.cat([ques_embed_words,ques_char_feat,ques_feat], 2)#join all question features\n",
    "        encoded_ques = encode(self.encoding_rnn, val2, ques_length, self.training)#encode question\n",
    "        ctxt = encoded_con\n",
    "        \n",
    "        for i in range(2):\n",
    "            #interactive aligning\n",
    "            attended_ques = Attention('interac', ctxt, encoded_ques)\n",
    "            #compute query aware representation\n",
    "            val_s = torch.cat([ctxt, attended_ques, torch.mul(ctxt, attended_ques), ctxt - attended_ques], 2)\n",
    "            comp = func.tanh(self.sfu_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_l2[i](val_s))\n",
    "            ques_aware_ctxt = gate * comp + (1-gate) * ctxt #queey aware ctxt\n",
    "            #self aligning\n",
    "            attended_ctxt = Attention('self',0,ques_aware_ctxt)\n",
    "            #compute context aware representation\n",
    "            val_s = torch.cat([ques_aware_ctxt, attended_ctxt, torch.mul(ques_aware_ctxt,attended_ctxt), ques_aware_ctxt - attended_ctxt], 2)\n",
    "            comp = func.tanh(self.sfu_self_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_self_l2[i](val_s))\n",
    "            ctxt_aware_ctxt = gate * comp + (1-gate) * attended_ctxt # self-aware\n",
    "            #Aggregating\n",
    "            ctxt = encode(self.final_rec_nn[i], ctxt_aware_ctxt, con_length, self.training)\n",
    "        \n",
    "        #memory Answer Pointer\n",
    "        memory_s = encoded_ques[:,-1,:].resize(encoded_ques.size(0),1, encoded_ques.size(2))\n",
    "         \n",
    "        for i in range(2):\n",
    "            #prob dist for start using memory and fully aware context\n",
    "            start = self.FeedForward_s[i](torch.cat([ctxt, memory_s.expand(-1,ctxt.size(1),-1), torch.mul(ctxt,memory_s.expand(-1,ctxt.size(1),-1))], 2))\n",
    "            start_prob = func.softmax(start.squeeze(2), dim=1) \n",
    "            #evidence vector\n",
    "            evidence_e = matmul(start_prob.unsqueeze(1),ctxt)\n",
    "            \n",
    "            #fuse memory and evidence\n",
    "            val_s = torch.cat([memory_s, evidence_e], 2)\n",
    "            l1_s = func.tanh(self.sfu_s_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_s_l2[i](val_s))\n",
    "            memory_e = gate * l1_s + (1-gate) * memory_s #new memory\n",
    "            \n",
    "            #prob dist for end using new memory and fully aware context\n",
    "            end = self.FeedForward_e[i](torch.cat([ctxt, memory_e.expand(-1,ctxt.size(1),-1), torch.mul(ctxt,memory_e.expand(-1,ctxt.size(1),-1))], 2))\n",
    "            end_prob = func.softmax(end.squeeze(2), dim=1)\n",
    "            #evidence vector\n",
    "            evidence_e = matmul(end_prob.resize(end_prob.size(0),1, end_prob.size(1)),ctxt)\n",
    "            \n",
    "            # fuse to generate new memory\n",
    "            val_e = torch.cat([memory_e, evidence_e], 2)\n",
    "            l1_e = func.tanh(self.sfu_e_l1[i](val_e))\n",
    "            gate_e = func.sigmoid(self.sfu_e_l2[i](val_e))\n",
    "            memory_s = gate_e * l1_e + (1-gate_e) * memory_e\n",
    "\n",
    "        start_prob = func.log_softmax(start.squeeze(2), dim=1) \n",
    "        end_prob = func.log_softmax(end.squeeze(2), dim=1)\n",
    "        \n",
    "        #print(end_prob.size())\n",
    "        #print(start_prob.size())\n",
    "    \n",
    "        return start_prob, end_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mle\n",
    "def findmax(score_s,score_e):\n",
    "    max_len = 15\n",
    "    pred_s = []\n",
    "    pred_e = []\n",
    "    dim = score_s.shape[1]\n",
    "    \n",
    "    for i in range(score_s.shape[0]):\n",
    "        joint = np.zeros((dim,dim))\n",
    "        for start in range(dim):\n",
    "            if start+max_len < dim:\n",
    "                joint[start,start:start+max_len]=score_s[i,:][start]*score_e[i,:][start:start+max_len]\n",
    "            else:\n",
    "                joint[start,start:dim] = score_s[i,:][start]*score_e[i,:][start:dim]\n",
    "                \n",
    "        s_idx, e_idx = np.argwhere(joint.max() == joint)[0,:] \n",
    "        \n",
    "        pred_s.append(s_idx)\n",
    "        pred_e.append(e_idx)\n",
    "        \n",
    "    return pred_s, pred_e\n",
    "\n",
    "def validate(data_loader, network, positions, texts, answers, official):\n",
    "    f1 = 0\n",
    "    em = 0\n",
    "    examples = 0\n",
    "    results = {}\n",
    "    for ex in data_loader:\n",
    "        batch_size = ex[0].size(0)\n",
    "        ex_id = ex[-1]\n",
    "        \n",
    "        #Predicting....\n",
    "        network.eval()\n",
    "\n",
    "        con_words = Variable(ex[0].cuda())\n",
    "        con_chars = Variable(ex[1].cuda())\n",
    "        con_feat = Variable(ex[2].cuda())\n",
    "        ques_words = Variable(ex[4].cuda())\n",
    "        ques_chars = Variable(ex[5].cuda())\n",
    "        ques_feat = Variable(ex[6].cuda())\n",
    "\n",
    "        con_embed_words = func.dropout(emb_layer(con_words), p=0.2)\n",
    "        ques_embed_words = func.dropout(emb_layer(ques_words), p=0.2)\n",
    "        con_embed_chars = func.dropout(emb_layer_char(con_chars), p=0.2)\n",
    "        ques_embed_chars = func.dropout(emb_layer_char(ques_chars), p=0.2)\n",
    "\n",
    "        score_s, score_e = network(con_embed_words,con_embed_chars,con_feat,ex[3],ques_embed_words,ques_embed_chars,ques_feat,ex[7])\n",
    "        score_s.exp_() \n",
    "        score_e.exp_()\n",
    "        pred_s, pred_e = findmax(score_s.data.cpu().numpy(),score_e.data.cpu().numpy())\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            s_posn = positions[ex_id[i]][pred_s[i]][0]\n",
    "            e_posn = positions[ex_id[i]][pred_e[i]][1]\n",
    "            prediction = texts[ex_id[i]][s_posn:e_posn]\n",
    "            \n",
    "            if official:\n",
    "                results[ex_id[i]]=prediction\n",
    "\n",
    "            ground_truths = answers[ex_id[i]]\n",
    "            em += ev.metric_max_over_ground_truths(ev.exact_match_score, prediction, ground_truths)\n",
    "            f1 += ev.metric_max_over_ground_truths(ev.f1_score, prediction, ground_truths)\n",
    "                 \n",
    "        examples += batch_size\n",
    "    \n",
    "    if official:\n",
    "        return f1/examples, em/examples, results\n",
    "    else:\n",
    "        return f1/examples, em/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "reader = Model().cuda()\n",
    "optimizer = optim.Adamax(reader.parameters())\n",
    "torch.cuda.set_device(-1)\n",
    "model_name = 'without_topics_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 2479.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_texts = {}\n",
    "dev_answers = {}\n",
    "f2 = open(\"data/datasets_without_topics_new/dev-v1.1.json\", 'r')\n",
    "examples = json.load(f2)['data']\n",
    "for article in tqdm(examples):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            dev_texts[qa['id']] = paragraph['context']\n",
    "            dev_answers[qa['id']] = list(map(lambda x: x['text'], qa['answers']))\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch-step 0 \t/ 1921 \tloss 0.2492383533053928\n",
      "Epoch 1 Batch-step 50 \t/ 1921 \tloss 11.015367274814182\n",
      "Epoch 1 Batch-step 100 \t/ 1921 \tloss 10.765929667154948\n",
      "Epoch 1 Batch-step 150 \t/ 1921 \tloss 10.801965120103624\n",
      "Epoch 1 Batch-step 200 \t/ 1921 \tloss 10.78068281809489\n",
      "Epoch 1 Batch-step 250 \t/ 1921 \tloss 10.021933258904351\n",
      "Epoch 1 Batch-step 300 \t/ 1921 \tloss 9.456556489732531\n",
      "Epoch 1 Batch-step 350 \t/ 1921 \tloss 8.921796713935004\n",
      "Epoch 1 Batch-step 400 \t/ 1921 \tloss 8.364032310909694\n",
      "Epoch 1 Batch-step 450 \t/ 1921 \tloss 7.897860474056667\n",
      "Epoch 1 Batch-step 500 \t/ 1921 \tloss 7.564931466844347\n",
      "Epoch 1 Batch-step 550 \t/ 1921 \tloss 7.448110442691379\n",
      "Epoch 1 Batch-step 600 \t/ 1921 \tloss 7.234322770436605\n",
      "Epoch 1 Batch-step 650 \t/ 1921 \tloss 7.145387268066406\n",
      "Epoch 1 Batch-step 700 \t/ 1921 \tloss 6.920734341939291\n",
      "Epoch 1 Batch-step 750 \t/ 1921 \tloss 6.777122741275363\n",
      "Epoch 1 Batch-step 800 \t/ 1921 \tloss 6.739352724287245\n",
      "Epoch 1 Batch-step 850 \t/ 1921 \tloss 6.609040090772841\n",
      "Epoch 1 Batch-step 900 \t/ 1921 \tloss 6.582166417439779\n",
      "Epoch 1 Batch-step 950 \t/ 1921 \tloss 6.6977716763814295\n",
      "Epoch 1 Batch-step 1000 \t/ 1921 \tloss 6.387748018900553\n",
      "Epoch 1 Batch-step 1050 \t/ 1921 \tloss 6.346656449635824\n",
      "Epoch 1 Batch-step 1100 \t/ 1921 \tloss 6.075986682044135\n",
      "Epoch 1 Batch-step 1150 \t/ 1921 \tloss 6.272357813517252\n",
      "Epoch 1 Batch-step 1200 \t/ 1921 \tloss 6.018043157789442\n",
      "Epoch 1 Batch-step 1250 \t/ 1921 \tloss 6.008770720163981\n",
      "Epoch 1 Batch-step 1300 \t/ 1921 \tloss 5.710775703854031\n",
      "Epoch 1 Batch-step 1350 \t/ 1921 \tloss 5.6767138904995385\n",
      "Epoch 1 Batch-step 1400 \t/ 1921 \tloss 5.870220141940647\n",
      "Epoch 1 Batch-step 1450 \t/ 1921 \tloss 5.847278987036811\n",
      "Epoch 1 Batch-step 1500 \t/ 1921 \tloss 5.670772510104709\n",
      "Epoch 1 Batch-step 1550 \t/ 1921 \tloss 5.508183447519938\n",
      "Epoch 1 Batch-step 1600 \t/ 1921 \tloss 5.605152087741428\n",
      "Epoch 1 Batch-step 1650 \t/ 1921 \tloss 5.570624303817749\n",
      "Epoch 1 Batch-step 1700 \t/ 1921 \tloss 5.581287341647678\n",
      "Epoch 1 Batch-step 1750 \t/ 1921 \tloss 5.395357884301079\n",
      "Epoch 1 Batch-step 1800 \t/ 1921 \tloss 5.547393565707736\n",
      "Epoch 1 Batch-step 1850 \t/ 1921 \tloss 5.486425685882568\n",
      "Epoch 1 Batch-step 1900 \t/ 1921 \tloss 5.366463783052232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/14 [20:57<4:32:23, 1257.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Dev F1: 0.5572264003645084 EM: 0.4394358143524254 \n",
      "\n",
      "Epoch 2 Batch-step 0 \t/ 1921 \tloss 0.10059004889594184\n",
      "Epoch 2 Batch-step 50 \t/ 1921 \tloss 5.267898888058133\n",
      "Epoch 2 Batch-step 100 \t/ 1921 \tloss 5.2420675489637585\n",
      "Epoch 2 Batch-step 150 \t/ 1921 \tloss 5.229427093929715\n",
      "Epoch 2 Batch-step 200 \t/ 1921 \tloss 5.265156152513292\n",
      "Epoch 2 Batch-step 250 \t/ 1921 \tloss 5.134209235509236\n",
      "Epoch 2 Batch-step 300 \t/ 1921 \tloss 5.187976148393419\n",
      "Epoch 2 Batch-step 350 \t/ 1921 \tloss 5.112019438213772\n",
      "Epoch 2 Batch-step 400 \t/ 1921 \tloss 5.137523179584079\n",
      "Epoch 2 Batch-step 450 \t/ 1921 \tloss 5.000210534201728\n",
      "Epoch 2 Batch-step 500 \t/ 1921 \tloss 5.077812115351359\n",
      "Epoch 2 Batch-step 550 \t/ 1921 \tloss 4.967903184890747\n",
      "Epoch 2 Batch-step 600 \t/ 1921 \tloss 4.941505861282349\n",
      "Epoch 2 Batch-step 650 \t/ 1921 \tloss 5.059855985641479\n",
      "Epoch 2 Batch-step 700 \t/ 1921 \tloss 5.034899022844103\n",
      "Epoch 2 Batch-step 750 \t/ 1921 \tloss 4.811541673872206\n",
      "Epoch 2 Batch-step 800 \t/ 1921 \tloss 4.789248991012573\n",
      "Epoch 2 Batch-step 850 \t/ 1921 \tloss 4.811035670174492\n",
      "Epoch 2 Batch-step 900 \t/ 1921 \tloss 4.773039918475681\n",
      "Epoch 2 Batch-step 950 \t/ 1921 \tloss 4.8268015066782635\n",
      "Epoch 2 Batch-step 1000 \t/ 1921 \tloss 4.764814954333835\n",
      "Epoch 2 Batch-step 1050 \t/ 1921 \tloss 4.894962135950724\n",
      "Epoch 2 Batch-step 1100 \t/ 1921 \tloss 4.6709319485558405\n",
      "Epoch 2 Batch-step 1150 \t/ 1921 \tloss 4.66915692753262\n",
      "Epoch 2 Batch-step 1200 \t/ 1921 \tloss 4.660461187362671\n",
      "Epoch 2 Batch-step 1250 \t/ 1921 \tloss 4.797187185287475\n",
      "Epoch 2 Batch-step 1300 \t/ 1921 \tloss 4.659323247273763\n",
      "Epoch 2 Batch-step 1350 \t/ 1921 \tloss 4.711097515953911\n",
      "Epoch 2 Batch-step 1400 \t/ 1921 \tloss 4.622185066011217\n",
      "Epoch 2 Batch-step 1450 \t/ 1921 \tloss 4.5959349367353655\n",
      "Epoch 2 Batch-step 1500 \t/ 1921 \tloss 4.798948738310072\n",
      "Epoch 2 Batch-step 1550 \t/ 1921 \tloss 4.622518200344509\n",
      "Epoch 2 Batch-step 1600 \t/ 1921 \tloss 4.596434301800198\n",
      "Epoch 2 Batch-step 1650 \t/ 1921 \tloss 4.638695361879137\n",
      "Epoch 2 Batch-step 1700 \t/ 1921 \tloss 4.751032283571031\n",
      "Epoch 2 Batch-step 1750 \t/ 1921 \tloss 4.577086035410563\n",
      "Epoch 2 Batch-step 1800 \t/ 1921 \tloss 4.5530263476901585\n",
      "Epoch 2 Batch-step 1850 \t/ 1921 \tloss 4.521727890438504\n",
      "Epoch 2 Batch-step 1900 \t/ 1921 \tloss 4.428546211454603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 2/14 [41:22<4:08:17, 1241.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Dev F1: 0.6358733111186164 EM: 0.5267321071190317 \n",
      "\n",
      "Epoch 3 Batch-step 0 \t/ 1921 \tloss 0.08257493972778321\n",
      "Epoch 3 Batch-step 50 \t/ 1921 \tloss 4.519598473442925\n",
      "Epoch 3 Batch-step 100 \t/ 1921 \tloss 4.252086136076185\n",
      "Epoch 3 Batch-step 150 \t/ 1921 \tloss 4.291447342766656\n",
      "Epoch 3 Batch-step 200 \t/ 1921 \tloss 4.393562783135308\n",
      "Epoch 3 Batch-step 250 \t/ 1921 \tloss 4.466944906446669\n",
      "Epoch 3 Batch-step 300 \t/ 1921 \tloss 4.3375193277994795\n",
      "Epoch 3 Batch-step 350 \t/ 1921 \tloss 4.334417777591281\n",
      "Epoch 3 Batch-step 400 \t/ 1921 \tloss 4.353164492713081\n",
      "Epoch 3 Batch-step 450 \t/ 1921 \tloss 4.250656339857313\n",
      "Epoch 3 Batch-step 500 \t/ 1921 \tloss 4.3239662647247314\n",
      "Epoch 3 Batch-step 550 \t/ 1921 \tloss 4.261669773525662\n",
      "Epoch 3 Batch-step 600 \t/ 1921 \tloss 4.31877850956387\n",
      "Epoch 3 Batch-step 650 \t/ 1921 \tloss 4.343662802378336\n",
      "Epoch 3 Batch-step 700 \t/ 1921 \tloss 4.144206900066799\n",
      "Epoch 3 Batch-step 750 \t/ 1921 \tloss 4.201587475670708\n",
      "Epoch 3 Batch-step 800 \t/ 1921 \tloss 4.119488795598348\n",
      "Epoch 3 Batch-step 850 \t/ 1921 \tloss 4.284719790352716\n",
      "Epoch 3 Batch-step 900 \t/ 1921 \tloss 4.127389457490709\n",
      "Epoch 3 Batch-step 950 \t/ 1921 \tloss 4.20233088069492\n",
      "Epoch 3 Batch-step 1000 \t/ 1921 \tloss 4.233520396550497\n",
      "Epoch 3 Batch-step 1050 \t/ 1921 \tloss 4.269654708438449\n",
      "Epoch 3 Batch-step 1100 \t/ 1921 \tloss 4.23149659898546\n",
      "Epoch 3 Batch-step 1150 \t/ 1921 \tloss 4.0512788931528725\n",
      "Epoch 3 Batch-step 1200 \t/ 1921 \tloss 4.122766934500801\n",
      "Epoch 3 Batch-step 1250 \t/ 1921 \tloss 4.0922509352366125\n",
      "Epoch 3 Batch-step 1300 \t/ 1921 \tloss 4.0627652592129175\n",
      "Epoch 3 Batch-step 1350 \t/ 1921 \tloss 4.162139876683553\n",
      "Epoch 3 Batch-step 1400 \t/ 1921 \tloss 4.096944840749105\n",
      "Epoch 3 Batch-step 1450 \t/ 1921 \tloss 3.9828690157996283\n",
      "Epoch 3 Batch-step 1500 \t/ 1921 \tloss 3.9772494792938233\n",
      "Epoch 3 Batch-step 1550 \t/ 1921 \tloss 4.147732883029514\n",
      "Epoch 3 Batch-step 1600 \t/ 1921 \tloss 4.092271868387858\n",
      "Epoch 3 Batch-step 1650 \t/ 1921 \tloss 4.065054755740696\n",
      "Epoch 3 Batch-step 1700 \t/ 1921 \tloss 4.2010700861612955\n",
      "Epoch 3 Batch-step 1750 \t/ 1921 \tloss 4.052654875649346\n",
      "Epoch 3 Batch-step 1800 \t/ 1921 \tloss 4.120021284951104\n",
      "Epoch 3 Batch-step 1850 \t/ 1921 \tloss 4.0961104604933\n",
      "Epoch 3 Batch-step 1900 \t/ 1921 \tloss 4.012408023410373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 3/14 [1:01:50<3:46:44, 1236.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Dev F1: 0.6839752946210069 EM: 0.5754312398742019 \n",
      "\n",
      "Epoch 4 Batch-step 0 \t/ 1921 \tloss 0.06412003835042318\n",
      "Epoch 4 Batch-step 50 \t/ 1921 \tloss 3.8275995678371855\n",
      "Epoch 4 Batch-step 100 \t/ 1921 \tloss 3.8564389281802707\n",
      "Epoch 4 Batch-step 150 \t/ 1921 \tloss 3.9700344191657173\n",
      "Epoch 4 Batch-step 200 \t/ 1921 \tloss 3.820510424507989\n",
      "Epoch 4 Batch-step 250 \t/ 1921 \tloss 3.9734182834625242\n",
      "Epoch 4 Batch-step 300 \t/ 1921 \tloss 3.880657434463501\n",
      "Epoch 4 Batch-step 350 \t/ 1921 \tloss 3.90748676194085\n",
      "Epoch 4 Batch-step 400 \t/ 1921 \tloss 3.8154245641496445\n",
      "Epoch 4 Batch-step 450 \t/ 1921 \tloss 3.9102705902523462\n",
      "Epoch 4 Batch-step 500 \t/ 1921 \tloss 3.7852453973558213\n",
      "Epoch 4 Batch-step 550 \t/ 1921 \tloss 3.7477589342329236\n",
      "Epoch 4 Batch-step 600 \t/ 1921 \tloss 3.872595696979099\n",
      "Epoch 4 Batch-step 650 \t/ 1921 \tloss 3.8726293722788494\n",
      "Epoch 4 Batch-step 700 \t/ 1921 \tloss 3.6843491872151692\n",
      "Epoch 4 Batch-step 750 \t/ 1921 \tloss 3.872318564520942\n",
      "Epoch 4 Batch-step 800 \t/ 1921 \tloss 3.777337408065796\n",
      "Epoch 4 Batch-step 850 \t/ 1921 \tloss 3.853951835632324\n",
      "Epoch 4 Batch-step 900 \t/ 1921 \tloss 3.8061752478281656\n",
      "Epoch 4 Batch-step 950 \t/ 1921 \tloss 3.7518038007948133\n",
      "Epoch 4 Batch-step 1000 \t/ 1921 \tloss 3.782269906997681\n",
      "Epoch 4 Batch-step 1050 \t/ 1921 \tloss 3.8059109528859456\n",
      "Epoch 4 Batch-step 1100 \t/ 1921 \tloss 3.8620929770999486\n",
      "Epoch 4 Batch-step 1150 \t/ 1921 \tloss 3.6616180684831408\n",
      "Epoch 4 Batch-step 1200 \t/ 1921 \tloss 3.8241715484195287\n",
      "Epoch 4 Batch-step 1250 \t/ 1921 \tloss 3.688275263044569\n",
      "Epoch 4 Batch-step 1300 \t/ 1921 \tloss 3.8441376368204754\n",
      "Epoch 4 Batch-step 1350 \t/ 1921 \tloss 3.8303563276926678\n",
      "Epoch 4 Batch-step 1400 \t/ 1921 \tloss 3.8846378962198895\n",
      "Epoch 4 Batch-step 1450 \t/ 1921 \tloss 3.7115172651078967\n",
      "Epoch 4 Batch-step 1500 \t/ 1921 \tloss 3.723310422897339\n",
      "Epoch 4 Batch-step 1550 \t/ 1921 \tloss 3.796220048268636\n",
      "Epoch 4 Batch-step 1600 \t/ 1921 \tloss 3.797062306933933\n",
      "Epoch 4 Batch-step 1650 \t/ 1921 \tloss 3.7666372193230524\n",
      "Epoch 4 Batch-step 1700 \t/ 1921 \tloss 3.7223073641459146\n",
      "Epoch 4 Batch-step 1750 \t/ 1921 \tloss 3.8790185928344725\n",
      "Epoch 4 Batch-step 1800 \t/ 1921 \tloss 3.737062682045831\n",
      "Epoch 4 Batch-step 1850 \t/ 1921 \tloss 3.661820639504327\n",
      "Epoch 4 Batch-step 1900 \t/ 1921 \tloss 3.8156495147281224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 4/14 [1:22:19<3:25:49, 1234.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Dev F1: 0.7130260757560126 EM: 0.6072619841799295 \n",
      "\n",
      "Epoch 5 Batch-step 0 \t/ 1921 \tloss 0.055752605862087676\n",
      "Epoch 5 Batch-step 50 \t/ 1921 \tloss 3.520412720574273\n",
      "Epoch 5 Batch-step 100 \t/ 1921 \tloss 3.506334008110894\n",
      "Epoch 5 Batch-step 150 \t/ 1921 \tloss 3.5130696349673802\n",
      "Epoch 5 Batch-step 200 \t/ 1921 \tloss 3.527558019426134\n",
      "Epoch 5 Batch-step 250 \t/ 1921 \tloss 3.4321981112162274\n",
      "Epoch 5 Batch-step 300 \t/ 1921 \tloss 3.6297395441267226\n",
      "Epoch 5 Batch-step 350 \t/ 1921 \tloss 3.533646027247111\n",
      "Epoch 5 Batch-step 400 \t/ 1921 \tloss 3.3913698461320667\n",
      "Epoch 5 Batch-step 450 \t/ 1921 \tloss 3.618481159210205\n",
      "Epoch 5 Batch-step 500 \t/ 1921 \tloss 3.532684432135688\n",
      "Epoch 5 Batch-step 550 \t/ 1921 \tloss 3.45867756207784\n",
      "Epoch 5 Batch-step 600 \t/ 1921 \tloss 3.586442385779487\n",
      "Epoch 5 Batch-step 650 \t/ 1921 \tloss 3.6887791050804988\n",
      "Epoch 5 Batch-step 700 \t/ 1921 \tloss 3.4970370292663575\n",
      "Epoch 5 Batch-step 750 \t/ 1921 \tloss 3.5203095886442397\n",
      "Epoch 5 Batch-step 800 \t/ 1921 \tloss 3.5899276892344156\n",
      "Epoch 5 Batch-step 850 \t/ 1921 \tloss 3.5086661232842338\n",
      "Epoch 5 Batch-step 900 \t/ 1921 \tloss 3.421406470404731\n",
      "Epoch 5 Batch-step 950 \t/ 1921 \tloss 3.557376416524251\n",
      "Epoch 5 Batch-step 1000 \t/ 1921 \tloss 3.522888355784946\n",
      "Epoch 5 Batch-step 1050 \t/ 1921 \tloss 3.5014648384518092\n",
      "Epoch 5 Batch-step 1100 \t/ 1921 \tloss 3.481995958752102\n",
      "Epoch 5 Batch-step 1150 \t/ 1921 \tloss 3.4557954841189913\n",
      "Epoch 5 Batch-step 1200 \t/ 1921 \tloss 3.486884419123332\n",
      "Epoch 5 Batch-step 1250 \t/ 1921 \tloss 3.3830818282233346\n",
      "Epoch 5 Batch-step 1300 \t/ 1921 \tloss 3.488943327797784\n",
      "Epoch 5 Batch-step 1350 \t/ 1921 \tloss 3.5010168340471055\n",
      "Epoch 5 Batch-step 1400 \t/ 1921 \tloss 3.544518407185872\n",
      "Epoch 5 Batch-step 1450 \t/ 1921 \tloss 3.675610992643568\n",
      "Epoch 5 Batch-step 1500 \t/ 1921 \tloss 3.4401815785302055\n",
      "Epoch 5 Batch-step 1550 \t/ 1921 \tloss 3.607922437455919\n",
      "Epoch 5 Batch-step 1600 \t/ 1921 \tloss 3.5134672270880807\n",
      "Epoch 5 Batch-step 1650 \t/ 1921 \tloss 3.5368732187483047\n",
      "Epoch 5 Batch-step 1700 \t/ 1921 \tloss 3.4764676994747585\n",
      "Epoch 5 Batch-step 1750 \t/ 1921 \tloss 3.4210748301612006\n",
      "Epoch 5 Batch-step 1800 \t/ 1921 \tloss 3.5334305392371284\n",
      "Epoch 5 Batch-step 1850 \t/ 1921 \tloss 3.4390018860499065\n",
      "Epoch 5 Batch-step 1900 \t/ 1921 \tloss 3.547147681978014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 5/14 [1:42:48<3:05:03, 1233.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Dev F1: 0.7149573260125451 EM: 0.6144096064042696 \n",
      "\n",
      "Epoch 6 Batch-step 0 \t/ 1921 \tloss 0.06675336625840929\n",
      "Epoch 6 Batch-step 50 \t/ 1921 \tloss 3.3234757635328505\n",
      "Epoch 6 Batch-step 100 \t/ 1921 \tloss 3.3774275408850776\n",
      "Epoch 6 Batch-step 150 \t/ 1921 \tloss 3.2561567041609023\n",
      "Epoch 6 Batch-step 200 \t/ 1921 \tloss 3.276769521501329\n",
      "Epoch 6 Batch-step 250 \t/ 1921 \tloss 3.391748046875\n",
      "Epoch 6 Batch-step 300 \t/ 1921 \tloss 3.4064651754167343\n",
      "Epoch 6 Batch-step 350 \t/ 1921 \tloss 3.318032161394755\n",
      "Epoch 6 Batch-step 400 \t/ 1921 \tloss 3.3353719393412273\n",
      "Epoch 6 Batch-step 450 \t/ 1921 \tloss 3.3012177467346193\n",
      "Epoch 6 Batch-step 500 \t/ 1921 \tloss 3.330852837032742\n",
      "Epoch 6 Batch-step 550 \t/ 1921 \tloss 3.282100378142463\n",
      "Epoch 6 Batch-step 600 \t/ 1921 \tloss 3.3252511103947957\n",
      "Epoch 6 Batch-step 650 \t/ 1921 \tloss 3.3723578506045873\n",
      "Epoch 6 Batch-step 700 \t/ 1921 \tloss 3.3185470660527545\n",
      "Epoch 6 Batch-step 750 \t/ 1921 \tloss 3.268996654616462\n",
      "Epoch 6 Batch-step 800 \t/ 1921 \tloss 3.409217855665419\n",
      "Epoch 6 Batch-step 850 \t/ 1921 \tloss 3.2841987477408514\n",
      "Epoch 6 Batch-step 900 \t/ 1921 \tloss 3.223158340983921\n",
      "Epoch 6 Batch-step 950 \t/ 1921 \tloss 3.212238703833686\n",
      "Epoch 6 Batch-step 1000 \t/ 1921 \tloss 3.279816198348999\n",
      "Epoch 6 Batch-step 1050 \t/ 1921 \tloss 3.167278962665134\n",
      "Epoch 6 Batch-step 1100 \t/ 1921 \tloss 3.1989261070887247\n",
      "Epoch 6 Batch-step 1150 \t/ 1921 \tloss 3.3005515893300372\n",
      "Epoch 6 Batch-step 1200 \t/ 1921 \tloss 3.3516646915011936\n",
      "Epoch 6 Batch-step 1250 \t/ 1921 \tloss 3.2979078345828587\n",
      "Epoch 6 Batch-step 1300 \t/ 1921 \tloss 3.3529028521643744\n",
      "Epoch 6 Batch-step 1350 \t/ 1921 \tloss 3.3547248045603433\n",
      "Epoch 6 Batch-step 1400 \t/ 1921 \tloss 3.3157240496741403\n",
      "Epoch 6 Batch-step 1450 \t/ 1921 \tloss 3.3075474633110895\n",
      "Epoch 6 Batch-step 1500 \t/ 1921 \tloss 3.307709476682875\n",
      "Epoch 6 Batch-step 1550 \t/ 1921 \tloss 3.2873561594221328\n",
      "Epoch 6 Batch-step 1600 \t/ 1921 \tloss 3.265026585261027\n",
      "Epoch 6 Batch-step 1650 \t/ 1921 \tloss 3.2165306435690986\n",
      "Epoch 6 Batch-step 1700 \t/ 1921 \tloss 3.3206740750206842\n",
      "Epoch 6 Batch-step 1750 \t/ 1921 \tloss 3.4067843781577216\n",
      "Epoch 6 Batch-step 1800 \t/ 1921 \tloss 3.2457563055886163\n",
      "Epoch 6 Batch-step 1850 \t/ 1921 \tloss 3.3664510091145834\n",
      "Epoch 6 Batch-step 1900 \t/ 1921 \tloss 3.3732806311713324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 6/14 [2:03:14<2:44:19, 1232.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Dev F1: 0.7384560893123525 EM: 0.6394739350042886 \n",
      "\n",
      "Epoch 7 Batch-step 0 \t/ 1921 \tloss 0.06341627438863119\n",
      "Epoch 7 Batch-step 50 \t/ 1921 \tloss 3.05757499800788\n",
      "Epoch 7 Batch-step 100 \t/ 1921 \tloss 3.176520744959513\n",
      "Epoch 7 Batch-step 150 \t/ 1921 \tloss 3.0909390476014877\n",
      "Epoch 7 Batch-step 200 \t/ 1921 \tloss 3.0083992030885485\n",
      "Epoch 7 Batch-step 250 \t/ 1921 \tloss 3.1648995929294164\n",
      "Epoch 7 Batch-step 300 \t/ 1921 \tloss 3.2593019114600286\n",
      "Epoch 7 Batch-step 350 \t/ 1921 \tloss 3.1038927687538993\n",
      "Epoch 7 Batch-step 400 \t/ 1921 \tloss 3.0344808710945976\n",
      "Epoch 7 Batch-step 450 \t/ 1921 \tloss 3.137146888838874\n",
      "Epoch 7 Batch-step 500 \t/ 1921 \tloss 3.165813832812839\n",
      "Epoch 7 Batch-step 550 \t/ 1921 \tloss 3.2350884861416285\n",
      "Epoch 7 Batch-step 600 \t/ 1921 \tloss 3.164654376771715\n",
      "Epoch 7 Batch-step 650 \t/ 1921 \tloss 3.0914457639058432\n",
      "Epoch 7 Batch-step 700 \t/ 1921 \tloss 3.099823922581143\n",
      "Epoch 7 Batch-step 750 \t/ 1921 \tloss 3.194496356116401\n",
      "Epoch 7 Batch-step 800 \t/ 1921 \tloss 3.1208095841937595\n",
      "Epoch 7 Batch-step 850 \t/ 1921 \tloss 3.0243309603797064\n",
      "Epoch 7 Batch-step 900 \t/ 1921 \tloss 3.143593740463257\n",
      "Epoch 7 Batch-step 950 \t/ 1921 \tloss 3.111772182252672\n",
      "Epoch 7 Batch-step 1000 \t/ 1921 \tloss 3.164501759741041\n",
      "Epoch 7 Batch-step 1050 \t/ 1921 \tloss 3.178789477878147\n",
      "Epoch 7 Batch-step 1100 \t/ 1921 \tloss 3.1534403456581965\n",
      "Epoch 7 Batch-step 1150 \t/ 1921 \tloss 3.03989077674018\n",
      "Epoch 7 Batch-step 1200 \t/ 1921 \tloss 3.190445327758789\n",
      "Epoch 7 Batch-step 1250 \t/ 1921 \tloss 3.1126218318939207\n",
      "Epoch 7 Batch-step 1300 \t/ 1921 \tloss 3.1151637898551092\n",
      "Epoch 7 Batch-step 1350 \t/ 1921 \tloss 3.1555047035217285\n",
      "Epoch 7 Batch-step 1400 \t/ 1921 \tloss 3.258180252710978\n",
      "Epoch 7 Batch-step 1450 \t/ 1921 \tloss 3.111512353685167\n",
      "Epoch 7 Batch-step 1500 \t/ 1921 \tloss 3.139254633585612\n",
      "Epoch 7 Batch-step 1550 \t/ 1921 \tloss 3.2145394325256347\n",
      "Epoch 7 Batch-step 1600 \t/ 1921 \tloss 3.1725644217597115\n",
      "Epoch 7 Batch-step 1650 \t/ 1921 \tloss 3.263554575708177\n",
      "Epoch 7 Batch-step 1700 \t/ 1921 \tloss 3.0384811295403376\n",
      "Epoch 7 Batch-step 1750 \t/ 1921 \tloss 3.073047253820631\n",
      "Epoch 7 Batch-step 1800 \t/ 1921 \tloss 3.2324058373769122\n",
      "Epoch 7 Batch-step 1850 \t/ 1921 \tloss 3.169038433498806\n",
      "Epoch 7 Batch-step 1900 \t/ 1921 \tloss 3.1672091325124105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 7/14 [2:23:41<2:23:41, 1231.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Dev F1: 0.7449841349174506 EM: 0.640617554560183 \n",
      "\n",
      "Epoch 8 Batch-step 0 \t/ 1921 \tloss 0.07235023710462782\n",
      "Epoch 8 Batch-step 50 \t/ 1921 \tloss 3.005459427833557\n",
      "Epoch 8 Batch-step 100 \t/ 1921 \tloss 2.937131897608439\n",
      "Epoch 8 Batch-step 150 \t/ 1921 \tloss 2.9114573107825383\n",
      "Epoch 8 Batch-step 200 \t/ 1921 \tloss 2.9622235774993895\n",
      "Epoch 8 Batch-step 250 \t/ 1921 \tloss 2.9388735506269668\n",
      "Epoch 8 Batch-step 300 \t/ 1921 \tloss 2.842082964049445\n",
      "Epoch 8 Batch-step 350 \t/ 1921 \tloss 3.001330343882243\n",
      "Epoch 8 Batch-step 400 \t/ 1921 \tloss 2.9163356728023953\n",
      "Epoch 8 Batch-step 450 \t/ 1921 \tloss 2.9378595723046197\n",
      "Epoch 8 Batch-step 500 \t/ 1921 \tloss 2.9862898694144353\n",
      "Epoch 8 Batch-step 550 \t/ 1921 \tloss 3.0138169341617163\n",
      "Epoch 8 Batch-step 600 \t/ 1921 \tloss 3.064192451371087\n",
      "Epoch 8 Batch-step 650 \t/ 1921 \tloss 2.9298036601808337\n",
      "Epoch 8 Batch-step 700 \t/ 1921 \tloss 2.995672247144911\n",
      "Epoch 8 Batch-step 750 \t/ 1921 \tloss 2.9520358350541858\n",
      "Epoch 8 Batch-step 800 \t/ 1921 \tloss 2.990252388848199\n",
      "Epoch 8 Batch-step 850 \t/ 1921 \tloss 2.9885031117333307\n",
      "Epoch 8 Batch-step 900 \t/ 1921 \tloss 2.9387957043117945\n",
      "Epoch 8 Batch-step 950 \t/ 1921 \tloss 2.9490592771106297\n",
      "Epoch 8 Batch-step 1000 \t/ 1921 \tloss 2.9085765229331124\n",
      "Epoch 8 Batch-step 1050 \t/ 1921 \tloss 3.060251564449734\n",
      "Epoch 8 Batch-step 1100 \t/ 1921 \tloss 3.0502885553571915\n",
      "Epoch 8 Batch-step 1150 \t/ 1921 \tloss 3.039524679713779\n",
      "Epoch 8 Batch-step 1200 \t/ 1921 \tloss 2.912217100461324\n",
      "Epoch 8 Batch-step 1250 \t/ 1921 \tloss 2.9601582686106362\n",
      "Epoch 8 Batch-step 1300 \t/ 1921 \tloss 3.0695399231380884\n",
      "Epoch 8 Batch-step 1350 \t/ 1921 \tloss 3.160157985157437\n",
      "Epoch 8 Batch-step 1400 \t/ 1921 \tloss 3.0070468902587892\n",
      "Epoch 8 Batch-step 1450 \t/ 1921 \tloss 2.924853033489651\n",
      "Epoch 8 Batch-step 1500 \t/ 1921 \tloss 3.088116396798028\n",
      "Epoch 8 Batch-step 1550 \t/ 1921 \tloss 2.9861702230241565\n",
      "Epoch 8 Batch-step 1600 \t/ 1921 \tloss 2.93549378712972\n",
      "Epoch 8 Batch-step 1650 \t/ 1921 \tloss 3.0526102039549086\n",
      "Epoch 8 Batch-step 1700 \t/ 1921 \tloss 3.099165987968445\n",
      "Epoch 8 Batch-step 1750 \t/ 1921 \tloss 3.0999639802508883\n",
      "Epoch 8 Batch-step 1800 \t/ 1921 \tloss 2.936018016603258\n",
      "Epoch 8 Batch-step 1850 \t/ 1921 \tloss 3.015410656399197\n",
      "Epoch 8 Batch-step 1900 \t/ 1921 \tloss 3.065384938981798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 8/14 [2:44:07<2:03:05, 1230.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Dev F1: 0.7555084068633596 EM: 0.6551034022681788 \n",
      "\n",
      "Epoch 9 Batch-step 0 \t/ 1921 \tloss 0.049666680230034725\n",
      "Epoch 9 Batch-step 50 \t/ 1921 \tloss 2.8445058902104696\n",
      "Epoch 9 Batch-step 100 \t/ 1921 \tloss 2.913045366605123\n",
      "Epoch 9 Batch-step 150 \t/ 1921 \tloss 2.842531108856201\n",
      "Epoch 9 Batch-step 200 \t/ 1921 \tloss 2.826234287685818\n",
      "Epoch 9 Batch-step 250 \t/ 1921 \tloss 2.817633435461256\n",
      "Epoch 9 Batch-step 300 \t/ 1921 \tloss 2.916970295376248\n",
      "Epoch 9 Batch-step 350 \t/ 1921 \tloss 2.813373263676961\n",
      "Epoch 9 Batch-step 400 \t/ 1921 \tloss 2.80348490079244\n",
      "Epoch 9 Batch-step 450 \t/ 1921 \tloss 2.889518404006958\n",
      "Epoch 9 Batch-step 500 \t/ 1921 \tloss 2.797083658642239\n",
      "Epoch 9 Batch-step 550 \t/ 1921 \tloss 2.9129199902216594\n",
      "Epoch 9 Batch-step 600 \t/ 1921 \tloss 2.905803998311361\n",
      "Epoch 9 Batch-step 650 \t/ 1921 \tloss 2.9201700157589383\n",
      "Epoch 9 Batch-step 700 \t/ 1921 \tloss 2.8406342612372506\n",
      "Epoch 9 Batch-step 750 \t/ 1921 \tloss 2.823617182837592\n",
      "Epoch 9 Batch-step 800 \t/ 1921 \tloss 2.9681562582651773\n",
      "Epoch 9 Batch-step 850 \t/ 1921 \tloss 2.948710627026028\n",
      "Epoch 9 Batch-step 900 \t/ 1921 \tloss 2.92519740263621\n",
      "Epoch 9 Batch-step 950 \t/ 1921 \tloss 2.8328301694658067\n",
      "Epoch 9 Batch-step 1000 \t/ 1921 \tloss 2.8382700549231634\n",
      "Epoch 9 Batch-step 1050 \t/ 1921 \tloss 2.8848473601871065\n",
      "Epoch 9 Batch-step 1100 \t/ 1921 \tloss 2.9600674947102865\n",
      "Epoch 9 Batch-step 1150 \t/ 1921 \tloss 3.0427286042107475\n",
      "Epoch 9 Batch-step 1200 \t/ 1921 \tloss 2.8925215933057995\n",
      "Epoch 9 Batch-step 1250 \t/ 1921 \tloss 2.827306154039171\n",
      "Epoch 9 Batch-step 1300 \t/ 1921 \tloss 2.9322592417399087\n",
      "Epoch 9 Batch-step 1350 \t/ 1921 \tloss 2.846658158302307\n",
      "Epoch 9 Batch-step 1400 \t/ 1921 \tloss 2.8266539997524687\n",
      "Epoch 9 Batch-step 1450 \t/ 1921 \tloss 2.9294562260309855\n",
      "Epoch 9 Batch-step 1500 \t/ 1921 \tloss 2.8174632178412544\n",
      "Epoch 9 Batch-step 1550 \t/ 1921 \tloss 3.1438197957144842\n",
      "Epoch 9 Batch-step 1600 \t/ 1921 \tloss 2.822955181863573\n",
      "Epoch 9 Batch-step 1650 \t/ 1921 \tloss 2.9709459887610543\n",
      "Epoch 9 Batch-step 1700 \t/ 1921 \tloss 2.880210140016344\n",
      "Epoch 9 Batch-step 1750 \t/ 1921 \tloss 2.8369896676805286\n",
      "Epoch 9 Batch-step 1800 \t/ 1921 \tloss 2.8571618451012504\n",
      "Epoch 9 Batch-step 1850 \t/ 1921 \tloss 2.845718328158061\n",
      "Epoch 9 Batch-step 1900 \t/ 1921 \tloss 2.9236879454718694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 9/14 [3:04:34<1:42:32, 1230.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Dev F1: 0.7604961156531524 EM: 0.6639664538263604 \n",
      "\n",
      "Epoch 10 Batch-step 0 \t/ 1921 \tloss 0.05082223150465223\n",
      "Epoch 10 Batch-step 50 \t/ 1921 \tloss 2.761578146616618\n",
      "Epoch 10 Batch-step 100 \t/ 1921 \tloss 2.753850367334154\n",
      "Epoch 10 Batch-step 150 \t/ 1921 \tloss 2.6938463979297214\n",
      "Epoch 10 Batch-step 200 \t/ 1921 \tloss 2.841215131017897\n",
      "Epoch 10 Batch-step 250 \t/ 1921 \tloss 2.734647358788384\n",
      "Epoch 10 Batch-step 300 \t/ 1921 \tloss 2.7196409702301025\n",
      "Epoch 10 Batch-step 350 \t/ 1921 \tloss 2.7620435608757865\n",
      "Epoch 10 Batch-step 400 \t/ 1921 \tloss 2.7313134007983737\n",
      "Epoch 10 Batch-step 450 \t/ 1921 \tloss 2.708395791053772\n",
      "Epoch 10 Batch-step 500 \t/ 1921 \tloss 2.7576863606770834\n",
      "Epoch 10 Batch-step 550 \t/ 1921 \tloss 2.7672161950005427\n",
      "Epoch 10 Batch-step 600 \t/ 1921 \tloss 2.8207100523842707\n",
      "Epoch 10 Batch-step 650 \t/ 1921 \tloss 2.625501526726617\n",
      "Epoch 10 Batch-step 700 \t/ 1921 \tloss 2.782444863849216\n",
      "Epoch 10 Batch-step 750 \t/ 1921 \tloss 2.7816501379013063\n",
      "Epoch 10 Batch-step 800 \t/ 1921 \tloss 2.8764572593900892\n",
      "Epoch 10 Batch-step 850 \t/ 1921 \tloss 2.802336327234904\n",
      "Epoch 10 Batch-step 900 \t/ 1921 \tloss 2.824680950906542\n",
      "Epoch 10 Batch-step 950 \t/ 1921 \tloss 2.6863624228371514\n",
      "Epoch 10 Batch-step 1000 \t/ 1921 \tloss 2.7027061939239503\n",
      "Epoch 10 Batch-step 1050 \t/ 1921 \tloss 2.6987656937705147\n",
      "Epoch 10 Batch-step 1100 \t/ 1921 \tloss 2.749172258377075\n",
      "Epoch 10 Batch-step 1150 \t/ 1921 \tloss 2.7694353818893434\n",
      "Epoch 10 Batch-step 1200 \t/ 1921 \tloss 2.8205256753497654\n",
      "Epoch 10 Batch-step 1250 \t/ 1921 \tloss 2.6968525012334186\n",
      "Epoch 10 Batch-step 1300 \t/ 1921 \tloss 2.712239082654317\n",
      "Epoch 10 Batch-step 1350 \t/ 1921 \tloss 2.870347269376119\n",
      "Epoch 10 Batch-step 1400 \t/ 1921 \tloss 2.7989407883750066\n",
      "Epoch 10 Batch-step 1450 \t/ 1921 \tloss 2.7806143946117823\n",
      "Epoch 10 Batch-step 1500 \t/ 1921 \tloss 2.771124166912503\n",
      "Epoch 10 Batch-step 1550 \t/ 1921 \tloss 2.7595408969455297\n",
      "Epoch 10 Batch-step 1600 \t/ 1921 \tloss 2.7962960375679864\n",
      "Epoch 10 Batch-step 1650 \t/ 1921 \tloss 2.7163525263468427\n",
      "Epoch 10 Batch-step 1700 \t/ 1921 \tloss 2.7639481915367976\n",
      "Epoch 10 Batch-step 1750 \t/ 1921 \tloss 2.9176124731699624\n",
      "Epoch 10 Batch-step 1800 \t/ 1921 \tloss 2.8551426463656955\n",
      "Epoch 10 Batch-step 1850 \t/ 1921 \tloss 2.772890316115485\n",
      "Epoch 10 Batch-step 1900 \t/ 1921 \tloss 2.9003206491470337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 10/14 [3:24:57<1:21:59, 1229.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Dev F1: 0.7628221895415577 EM: 0.6649194701229391 \n",
      "\n",
      "Epoch 11 Batch-step 0 \t/ 1921 \tloss 0.045929675632052955\n",
      "Epoch 11 Batch-step 50 \t/ 1921 \tloss 2.620222783088684\n",
      "Epoch 11 Batch-step 100 \t/ 1921 \tloss 2.597046634886\n",
      "Epoch 11 Batch-step 150 \t/ 1921 \tloss 2.570774284998576\n",
      "Epoch 11 Batch-step 200 \t/ 1921 \tloss 2.68904889954461\n",
      "Epoch 11 Batch-step 250 \t/ 1921 \tloss 2.608961741129557\n",
      "Epoch 11 Batch-step 300 \t/ 1921 \tloss 2.781162553363376\n",
      "Epoch 11 Batch-step 350 \t/ 1921 \tloss 2.7239900138643054\n",
      "Epoch 11 Batch-step 400 \t/ 1921 \tloss 2.549790538681878\n",
      "Epoch 11 Batch-step 450 \t/ 1921 \tloss 2.7595021115409004\n",
      "Epoch 11 Batch-step 500 \t/ 1921 \tloss 2.6156135400136313\n",
      "Epoch 11 Batch-step 550 \t/ 1921 \tloss 2.6669771009021335\n",
      "Epoch 11 Batch-step 600 \t/ 1921 \tloss 2.667549890942044\n",
      "Epoch 11 Batch-step 650 \t/ 1921 \tloss 2.736806360880534\n",
      "Epoch 11 Batch-step 700 \t/ 1921 \tloss 2.6240377691056995\n",
      "Epoch 11 Batch-step 750 \t/ 1921 \tloss 2.5874463001887005\n",
      "Epoch 11 Batch-step 800 \t/ 1921 \tloss 2.7549596574571398\n",
      "Epoch 11 Batch-step 850 \t/ 1921 \tloss 2.7634651078118218\n",
      "Epoch 11 Batch-step 900 \t/ 1921 \tloss 2.6012949731614854\n",
      "Epoch 11 Batch-step 950 \t/ 1921 \tloss 2.547893113560147\n",
      "Epoch 11 Batch-step 1000 \t/ 1921 \tloss 2.7891503148608736\n",
      "Epoch 11 Batch-step 1050 \t/ 1921 \tloss 2.6690939929750233\n",
      "Epoch 11 Batch-step 1100 \t/ 1921 \tloss 2.705504814783732\n",
      "Epoch 11 Batch-step 1150 \t/ 1921 \tloss 2.662669626871745\n",
      "Epoch 11 Batch-step 1200 \t/ 1921 \tloss 2.653298841582404\n",
      "Epoch 11 Batch-step 1250 \t/ 1921 \tloss 2.649433814154731\n",
      "Epoch 11 Batch-step 1300 \t/ 1921 \tloss 2.7906640821033055\n",
      "Epoch 11 Batch-step 1350 \t/ 1921 \tloss 2.6615369107988145\n",
      "Epoch 11 Batch-step 1400 \t/ 1921 \tloss 2.6702838950686987\n",
      "Epoch 11 Batch-step 1450 \t/ 1921 \tloss 2.746762024031745\n",
      "Epoch 11 Batch-step 1500 \t/ 1921 \tloss 2.71678528520796\n",
      "Epoch 11 Batch-step 1550 \t/ 1921 \tloss 2.759765428966946\n",
      "Epoch 11 Batch-step 1600 \t/ 1921 \tloss 2.596045700709025\n",
      "Epoch 11 Batch-step 1650 \t/ 1921 \tloss 2.662433131535848\n",
      "Epoch 11 Batch-step 1700 \t/ 1921 \tloss 2.7367509762446085\n",
      "Epoch 11 Batch-step 1750 \t/ 1921 \tloss 2.5690777248806422\n",
      "Epoch 11 Batch-step 1800 \t/ 1921 \tloss 2.623956094847785\n",
      "Epoch 11 Batch-step 1850 \t/ 1921 \tloss 2.7178540335761174\n",
      "Epoch 11 Batch-step 1900 \t/ 1921 \tloss 2.7758738411797417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 11/14 [3:45:19<1:01:27, 1229.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Dev F1: 0.7665419677588778 EM: 0.6755932526446202 \n",
      "\n",
      "Epoch 12 Batch-step 0 \t/ 1921 \tloss 0.05210538970099555\n",
      "Epoch 12 Batch-step 50 \t/ 1921 \tloss 2.594559645652771\n",
      "Epoch 12 Batch-step 100 \t/ 1921 \tloss 2.565210156970554\n",
      "Epoch 12 Batch-step 150 \t/ 1921 \tloss 2.535196566581726\n",
      "Epoch 12 Batch-step 200 \t/ 1921 \tloss 2.657248446676466\n",
      "Epoch 12 Batch-step 250 \t/ 1921 \tloss 2.5915783378813\n",
      "Epoch 12 Batch-step 300 \t/ 1921 \tloss 2.5040978458192615\n",
      "Epoch 12 Batch-step 350 \t/ 1921 \tloss 2.570647184054057\n",
      "Epoch 12 Batch-step 400 \t/ 1921 \tloss 2.632874827914768\n",
      "Epoch 12 Batch-step 450 \t/ 1921 \tloss 2.5294101238250732\n",
      "Epoch 12 Batch-step 500 \t/ 1921 \tloss 2.4726414415571423\n",
      "Epoch 12 Batch-step 550 \t/ 1921 \tloss 2.5894671334160697\n",
      "Epoch 12 Batch-step 600 \t/ 1921 \tloss 2.684851312637329\n",
      "Epoch 12 Batch-step 650 \t/ 1921 \tloss 2.682433417108324\n",
      "Epoch 12 Batch-step 700 \t/ 1921 \tloss 2.57549712922838\n",
      "Epoch 12 Batch-step 750 \t/ 1921 \tloss 2.5233402358161077\n",
      "Epoch 12 Batch-step 800 \t/ 1921 \tloss 2.520186506377326\n",
      "Epoch 12 Batch-step 850 \t/ 1921 \tloss 2.5962886783811783\n",
      "Epoch 12 Batch-step 900 \t/ 1921 \tloss 2.6923438019222683\n",
      "Epoch 12 Batch-step 950 \t/ 1921 \tloss 2.657724971241421\n",
      "Epoch 12 Batch-step 1000 \t/ 1921 \tloss 2.6936950418684216\n",
      "Epoch 12 Batch-step 1050 \t/ 1921 \tloss 2.5180953290727404\n",
      "Epoch 12 Batch-step 1100 \t/ 1921 \tloss 2.6442962752448187\n",
      "Epoch 12 Batch-step 1150 \t/ 1921 \tloss 2.577636551856995\n",
      "Epoch 12 Batch-step 1200 \t/ 1921 \tloss 2.644509810871548\n",
      "Epoch 12 Batch-step 1250 \t/ 1921 \tloss 2.482721296946208\n",
      "Epoch 12 Batch-step 1300 \t/ 1921 \tloss 2.7302744097179836\n",
      "Epoch 12 Batch-step 1350 \t/ 1921 \tloss 2.658205172750685\n",
      "Epoch 12 Batch-step 1400 \t/ 1921 \tloss 2.593536302778456\n",
      "Epoch 12 Batch-step 1450 \t/ 1921 \tloss 2.5444696797264945\n",
      "Epoch 12 Batch-step 1500 \t/ 1921 \tloss 2.62114196618398\n",
      "Epoch 12 Batch-step 1550 \t/ 1921 \tloss 2.632093008359273\n",
      "Epoch 12 Batch-step 1600 \t/ 1921 \tloss 2.7306362549463907\n",
      "Epoch 12 Batch-step 1650 \t/ 1921 \tloss 2.5949539846844143\n",
      "Epoch 12 Batch-step 1700 \t/ 1921 \tloss 2.6325675593482125\n",
      "Epoch 12 Batch-step 1750 \t/ 1921 \tloss 2.618078671561347\n",
      "Epoch 12 Batch-step 1800 \t/ 1921 \tloss 2.7610204060872396\n",
      "Epoch 12 Batch-step 1850 \t/ 1921 \tloss 2.6629739443461102\n",
      "Epoch 12 Batch-step 1900 \t/ 1921 \tloss 2.624055669042799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 12/14 [4:05:47<40:57, 1228.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Dev F1: 0.7653774899988289 EM: 0.6700657581244639 \n",
      "\n",
      "Epoch 13 Batch-step 0 \t/ 1921 \tloss 0.04911959966023763\n",
      "Epoch 13 Batch-step 50 \t/ 1921 \tloss 2.5478295114305283\n",
      "Epoch 13 Batch-step 100 \t/ 1921 \tloss 2.460380628373888\n",
      "Epoch 13 Batch-step 150 \t/ 1921 \tloss 2.428806922170851\n",
      "Epoch 13 Batch-step 200 \t/ 1921 \tloss 2.5149831586413915\n",
      "Epoch 13 Batch-step 250 \t/ 1921 \tloss 2.503249602847629\n",
      "Epoch 13 Batch-step 300 \t/ 1921 \tloss 2.4895364416970147\n",
      "Epoch 13 Batch-step 350 \t/ 1921 \tloss 2.6090779940287274\n",
      "Epoch 13 Batch-step 400 \t/ 1921 \tloss 2.4719182199902003\n",
      "Epoch 13 Batch-step 450 \t/ 1921 \tloss 2.466824475924174\n",
      "Epoch 13 Batch-step 500 \t/ 1921 \tloss 2.3961054669486153\n",
      "Epoch 13 Batch-step 550 \t/ 1921 \tloss 2.5058554119533962\n",
      "Epoch 13 Batch-step 600 \t/ 1921 \tloss 2.517867652575175\n",
      "Epoch 13 Batch-step 650 \t/ 1921 \tloss 2.52305732038286\n",
      "Epoch 13 Batch-step 700 \t/ 1921 \tloss 2.5811277283562553\n",
      "Epoch 13 Batch-step 750 \t/ 1921 \tloss 2.601746646563212\n",
      "Epoch 13 Batch-step 800 \t/ 1921 \tloss 2.6879606114493475\n",
      "Epoch 13 Batch-step 850 \t/ 1921 \tloss 2.4217424154281617\n",
      "Epoch 13 Batch-step 900 \t/ 1921 \tloss 2.5718026320139566\n",
      "Epoch 13 Batch-step 950 \t/ 1921 \tloss 2.4937235090467667\n",
      "Epoch 13 Batch-step 1000 \t/ 1921 \tloss 2.4503259711795384\n",
      "Epoch 13 Batch-step 1050 \t/ 1921 \tloss 2.5653453641467623\n",
      "Epoch 13 Batch-step 1100 \t/ 1921 \tloss 2.4827380259831746\n",
      "Epoch 13 Batch-step 1150 \t/ 1921 \tloss 2.6415158642662897\n",
      "Epoch 13 Batch-step 1200 \t/ 1921 \tloss 2.518746937645806\n",
      "Epoch 13 Batch-step 1250 \t/ 1921 \tloss 2.5404776361253525\n",
      "Epoch 13 Batch-step 1300 \t/ 1921 \tloss 2.450532783402337\n",
      "Epoch 13 Batch-step 1350 \t/ 1921 \tloss 2.600371805826823\n",
      "Epoch 13 Batch-step 1400 \t/ 1921 \tloss 2.5696690320968627\n",
      "Epoch 13 Batch-step 1450 \t/ 1921 \tloss 2.548413348197937\n",
      "Epoch 13 Batch-step 1500 \t/ 1921 \tloss 2.546782970428467\n",
      "Epoch 13 Batch-step 1550 \t/ 1921 \tloss 2.436570103963216\n",
      "Epoch 13 Batch-step 1600 \t/ 1921 \tloss 2.58943829536438\n",
      "Epoch 13 Batch-step 1650 \t/ 1921 \tloss 2.5469790193769666\n",
      "Epoch 13 Batch-step 1700 \t/ 1921 \tloss 2.547096406088935\n",
      "Epoch 13 Batch-step 1750 \t/ 1921 \tloss 2.640689425998264\n",
      "Epoch 13 Batch-step 1800 \t/ 1921 \tloss 2.67294917901357\n",
      "Epoch 13 Batch-step 1850 \t/ 1921 \tloss 2.485357160038418\n",
      "Epoch 13 Batch-step 1900 \t/ 1921 \tloss 2.627469942304823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 13/14 [4:26:16<20:28, 1228.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Dev F1: 0.7660492203665306 EM: 0.6719717907176213 \n",
      "\n",
      "Epoch 14 Batch-step 0 \t/ 1921 \tloss 0.07398667335510253\n",
      "Epoch 14 Batch-step 50 \t/ 1921 \tloss 2.3953263176812065\n",
      "Epoch 14 Batch-step 100 \t/ 1921 \tloss 2.48446745607588\n",
      "Epoch 14 Batch-step 150 \t/ 1921 \tloss 2.3638269080056085\n",
      "Epoch 14 Batch-step 200 \t/ 1921 \tloss 2.44318233066135\n",
      "Epoch 14 Batch-step 250 \t/ 1921 \tloss 2.3838990767796835\n",
      "Epoch 14 Batch-step 300 \t/ 1921 \tloss 2.5087519857618545\n",
      "Epoch 14 Batch-step 350 \t/ 1921 \tloss 2.439531005753411\n",
      "Epoch 14 Batch-step 400 \t/ 1921 \tloss 2.4702256043752033\n",
      "Epoch 14 Batch-step 450 \t/ 1921 \tloss 2.415962627198961\n",
      "Epoch 14 Batch-step 500 \t/ 1921 \tloss 2.4715407450993854\n",
      "Epoch 14 Batch-step 550 \t/ 1921 \tloss 2.3990577512317235\n",
      "Epoch 14 Batch-step 600 \t/ 1921 \tloss 2.3737881183624268\n",
      "Epoch 14 Batch-step 650 \t/ 1921 \tloss 2.401637159453498\n",
      "Epoch 14 Batch-step 700 \t/ 1921 \tloss 2.4356501791212293\n",
      "Epoch 14 Batch-step 750 \t/ 1921 \tloss 2.4758390797509087\n",
      "Epoch 14 Batch-step 800 \t/ 1921 \tloss 2.3727400753233168\n",
      "Epoch 14 Batch-step 850 \t/ 1921 \tloss 2.4193343030081853\n",
      "Epoch 14 Batch-step 900 \t/ 1921 \tloss 2.393825544251336\n",
      "Epoch 14 Batch-step 950 \t/ 1921 \tloss 2.507859155866835\n",
      "Epoch 14 Batch-step 1000 \t/ 1921 \tloss 2.3848418871561687\n",
      "Epoch 14 Batch-step 1050 \t/ 1921 \tloss 2.5564233859380088\n",
      "Epoch 14 Batch-step 1100 \t/ 1921 \tloss 2.584190328915914\n",
      "Epoch 14 Batch-step 1150 \t/ 1921 \tloss 2.498987054824829\n",
      "Epoch 14 Batch-step 1200 \t/ 1921 \tloss 2.527993994288974\n",
      "Epoch 14 Batch-step 1250 \t/ 1921 \tloss 2.415442697207133\n",
      "Epoch 14 Batch-step 1300 \t/ 1921 \tloss 2.3774611976411606\n",
      "Epoch 14 Batch-step 1350 \t/ 1921 \tloss 2.5329524093204077\n",
      "Epoch 14 Batch-step 1400 \t/ 1921 \tloss 2.3809129529529147\n",
      "Epoch 14 Batch-step 1450 \t/ 1921 \tloss 2.3295061667760213\n",
      "Epoch 14 Batch-step 1500 \t/ 1921 \tloss 2.50483865208096\n",
      "Epoch 14 Batch-step 1550 \t/ 1921 \tloss 2.5091333574718897\n",
      "Epoch 14 Batch-step 1600 \t/ 1921 \tloss 2.452843597200182\n",
      "Epoch 14 Batch-step 1650 \t/ 1921 \tloss 2.4626923481623333\n",
      "Epoch 14 Batch-step 1700 \t/ 1921 \tloss 2.550632529788547\n",
      "Epoch 14 Batch-step 1750 \t/ 1921 \tloss 2.602689750989278\n",
      "Epoch 14 Batch-step 1800 \t/ 1921 \tloss 2.576801159646776\n",
      "Epoch 14 Batch-step 1850 \t/ 1921 \tloss 2.5699614524841308\n",
      "Epoch 14 Batch-step 1900 \t/ 1921 \tloss 2.5120208501815795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 14/14 [4:46:41<00:00, 1228.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Dev F1: 0.7698624255604388 EM: 0.6749261412370151 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size = 45\n",
    "num_epochs = 14 \n",
    "best_f1 = 0\n",
    "\n",
    "loss_list = []\n",
    "f1_list = []\n",
    "em_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in tqdm(range(0, num_epochs)):\n",
    "    loss_list.append([])\n",
    "    train_loss = 0\n",
    "    for idx, ex in enumerate(train_loader): \n",
    "\n",
    "        reader.train()\n",
    "\n",
    "        con_words = Variable(ex[0].cuda())\n",
    "        con_chars = Variable(ex[1].cuda())\n",
    "        con_feat = Variable(ex[2].cuda())\n",
    "        ques_words = Variable(ex[4].cuda())\n",
    "        ques_chars = Variable(ex[5].cuda())\n",
    "        ques_feat = Variable(ex[6].cuda())\n",
    "        target_start = Variable(ex[8].cuda())\n",
    "        target_end = Variable(ex[9].cuda())\n",
    "        \n",
    "        con_embed_words = func.dropout(emb_layer(con_words), p=0.2)\n",
    "        ques_embed_words = func.dropout(emb_layer(ques_words), p=0.2)\n",
    "        con_embed_chars = func.dropout(emb_layer_char(con_chars), p=0.2)\n",
    "        ques_embed_chars = func.dropout(emb_layer_char(ques_chars), p=0.2)\n",
    "        \n",
    "        pred_start, pred_end = reader(con_embed_words,con_embed_chars,con_feat,ex[3],ques_embed_words,ques_embed_chars,ques_feat,ex[7])\n",
    "        \n",
    "        loss = func.nll_loss(pred_start, target_start) + func.nll_loss(pred_end, target_end)#calculate loss\n",
    "        optimizer.zero_grad() #set gradients to zero for each iteration\n",
    "        loss.backward() #backpropagate\n",
    "        torch.nn.utils.clip_grad_norm(reader.parameters(), max_norm=10)#clip gradients to avoid exploding gradients\n",
    "        optimizer.step() #update parameters\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        if(idx % 50 == 0):\n",
    "            loss_epoch_internal = train_loss/batch_size\n",
    "            loss_list[epoch].append(loss_epoch_internal)\n",
    "            print(\"Epoch\", epoch + 1, \"Batch-step\", idx, \"\\t/\", len(train_loader), \"\\tloss\", loss_epoch_internal)\n",
    "            train_loss=0\n",
    "#             if(idx == 50):\n",
    "#                 break\n",
    "            if(idx % 200 == 0):\n",
    "                with open('graph_elem.pkl', 'wb') as f:\n",
    "                    pickle.dump([loss_list, f1_list, em_list, epoch_list], f)\n",
    "\n",
    "    time_id = str(int(time.time()))\n",
    "    f1, em, results = validate(dev_loader, reader, dev_offsets, dev_texts, dev_answers, official=True)\n",
    "    print(\"Epoch:\", epoch + 1, \"Dev F1:\", f1, \"EM:\", em, \"\\n\")\n",
    "    f1_list.append(f1)\n",
    "    em_list.append(em)\n",
    "    epoch_list.append(epoch)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        model_name_cur = model_name+\"_\"+time_id+\"_epoch_\"+str(epoch+1)+\"_F1_\"+str(round(f1*100,2)).replace('.','_')+\".mdl\"\n",
    "        torch.save(reader.state_dict(), \"data/models/\"+model_name_cur+'.pt')\n",
    "        best_f1 = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = []\n",
    "for l in loss_list:\n",
    "    loss.append(l[1])\n",
    "\n",
    "plt.plot(epoch_list, loss)\n",
    "plt.title('NLL loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, f1_list, 'b', label='max F1 (' + str(str(format(max(f1_list),'.5f'))+')'))\n",
    "plt.plot(epoch_list, em_list, 'g', label='EM (' + str(str(format(max(em_list),'.5f'))+')'))\n",
    "plt.title('COMS-4995: SQuAD F1-EM')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 / EM score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'without_topics_final_1525717860_epoch_14_F1_76_99.mdl_predictions.preds'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_cur+'_predictions.preds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.load_state_dict(torch.load(\"data/models/\"+model_name_cur+'.pt'))\n",
    "f1, em, results = validate(dev_loader, reader, dev_offsets, dev_texts, dev_answers, official=True)\n",
    "\n",
    "with open(\"data/models/\"+model_name_cur+'_predictions.preds','w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7698624255604388"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6749261412370151"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On why questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whys_id = []\n",
    "whys = []\n",
    "for sample in dev_exs:\n",
    "    ques = [ques.lower() for ques in sample['question']]\n",
    "    if 'why' in ques:\n",
    "        whys_id.append(sample['id'])\n",
    "        whys.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'who','when','why',''\n",
    "why_offsets = {k: dev_offsets[k] for k in dev_offsets.keys() if k in whys_id}\n",
    "why_texts = {k: dev_texts[k] for k in dev_texts.keys() if k in whys_id}\n",
    "why_answers = {k: dev_answers[k] for k in dev_answers.keys() if k in whys_id}\n",
    "\n",
    "#whys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "whys_dataset = Transform(whys, word2idx, char2idx,status='test')\n",
    "whys_sampler = torch.utils.data.sampler.SequentialSampler(whys_dataset)\n",
    "whys_loader = torch.utils.data.DataLoader(\n",
    "        whys_dataset, batch_size=32, sampler=whys_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.load_state_dict(torch.load(\"data/models/\"+model_name_cur+'.pt'))\n",
    "f1, em, results = validate(whys_loader, reader, why_offsets, why_texts, why_answers, official=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6586105288403903"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35443037974683544"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'57337ddc4776f41900660bbe': 'to clean them of plants and sediments',\n",
       " '573399b54776f41900660e66': 'solid economic growth',\n",
       " '5733a32bd058e614000b5f36': 'their disastrous financial situation',\n",
       " '5733140a4776f419006606e4': 'because it has survived many wars, conflicts and invasions throughout its long history',\n",
       " '5733266d4776f41900660715': \"Due to its central location between the Commonwealth's capitals of Kraków and Vilnius\",\n",
       " '57332e48d058e614000b5763': 'Stalin was hostile to the idea of an independent Poland',\n",
       " '5733638fd058e614000b59e9': 'restored by the communist authorities after the war',\n",
       " '573368e54776f41900660a53': 'The species richness is mainly due to the location of Warsaw within the border region',\n",
       " '56e0c1617aa994140058e6d6': 'to attend school at the Higher Real Gymnasium',\n",
       " '56e0c2bc231d4119001ac389': 'this contact with nature made him stronger, both physically and mentally',\n",
       " '56dfa7887aa994140058dfaa': 'to hide the fact that he dropped out of school',\n",
       " '56dfaa047aa994140058dfbd': 'under police guard for not having a residence permit',\n",
       " '56e0cd33231d4119001ac3bf': 'not having a residence permit',\n",
       " '56dfac8e231d4119001abc5c': 'he never studied Greek, a required subject',\n",
       " '56e0d9e0231d4119001ac43d': 'avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes',\n",
       " '56e057e1231d4119001ac046': 'rotating magnetic field-based induction motor',\n",
       " '56e08b3c231d4119001ac2a4': 'noticed damaged film in his laboratory in previous experiments',\n",
       " '56e10296cd28a01900c67424': 'to complete the construction of Wardenclyffe',\n",
       " '56e11a73e3433e1400422bf1': 'it stimulated his brain cells',\n",
       " '56e12005cd28a01900c67619': 'because of her weight',\n",
       " '56e74e4800c9c71400d76f77': 'to serve and protect the public interest through certifying',\n",
       " '56e769dc00c9c71400d770e8': 'shortage of male teachers',\n",
       " '56f7fde8a6d7ea1400e17368': \"to sell indulgences to raise money to rebuild St. Peter's Basilica in Rome\",\n",
       " '56f826a7a6d7ea1400e17429': 'Recent scholars consider the evidence for these words to be unreliable',\n",
       " '56f855caaef2371900625ff5': 'his decision to marry surprised many',\n",
       " '56f86d30a6d7ea1400e17608': 'removing impediments and difficulties so that other people may read it without hindrance',\n",
       " '56f8720eaef2371900626090': 'the first individuals to be martyred by the Roman Catholic Church for Lutheran views',\n",
       " '56f87392aef237190062609d': 'because of the perceived difficulty of its tune',\n",
       " '5705eccb52bb8914006896b9': 'Palm Springs',\n",
       " '571099b2b654c5140001f9b5': 'protest against the occupation of Prussia by Napoleon in 1806-07',\n",
       " '571c4132dd7acb1400e4c0b3': 'he published his findings first',\n",
       " '571cc8815efbb31900334df0': 'due to their higher oxygen content',\n",
       " '571ce5055efbb31900334e2a': 'no damage due to the low total pressures used',\n",
       " '5725b5a689a1e219009abd28': 'to avoid being targeted by the boycott',\n",
       " '5725b76389a1e219009abd4c': \"oil was priced in dollars, oil producers' real income decreased\",\n",
       " '5725b76389a1e219009abd4e': \"oil producers' real income decreased\",\n",
       " '5725bad5271a42140099d0c1': '\"Of course [the price of oil] is going to rise',\n",
       " '5725bcb6271a42140099d0eb': 'response to American aid to Israel',\n",
       " '5725bcb6271a42140099d0ed': \"by five percent from September's output\",\n",
       " '5725bcb6271a42140099d0ef': 'curbed exports to various countries',\n",
       " '57264cac708984140094c1b4': 'to encourage investment',\n",
       " '57264cac708984140094c1b5': 'creating greater scarcity',\n",
       " '57264d9edd62a815002e8100': 'to coordinate the response to the embargo',\n",
       " '57265bdfdd62a815002e829e': 'increase',\n",
       " '57265e11708984140094c3bd': 'pushing prices down',\n",
       " '5726c3da708984140094d0da': 'Dutch law said only people established in the Netherlands could give legal advice',\n",
       " '572a0bfaaf94a219006aa77d': 'data sampling is biased away from the center of the Amazon basin',\n",
       " '572a07c11d046914007796d9': 'to protect their tribal lands from commercial interests',\n",
       " '57264a0ef1498d1400e8db42': 'enough to withstand waves and swirling sediment particles',\n",
       " '57268a37f1498d1400e8e33c': 'dead ends\" in marine food chains',\n",
       " '57268da7f1498d1400e8e39c': 'Because of their soft, gelatinous bodies',\n",
       " '5725cfd0271a42140099d226': 'railroad and worried about flooding',\n",
       " '5725fb8138643c19005acf3f': 'To avoid interference',\n",
       " '5726398589a1e219009ac58a': 'Routing a packet requires the node to look up the connection id in a table',\n",
       " '5726414e271a42140099d7e5': 'Michigan Educational Research Information Triad',\n",
       " '5726516a708984140094c224': 'lack of reliable statistics from this period',\n",
       " '57265285708984140094c25b': 'extensive samples from other mass graves',\n",
       " '57265285708984140094c25d': 'rat population was insufficient to account for a bubonic plague pandemic',\n",
       " '572663a9f1498d1400e8ddf2': 'indicated by asterisks',\n",
       " '57268220f1498d1400e8e218': 'to provide better absolute bounds on the timing and rates of deposition',\n",
       " '57267076708984140094c604': 'to spearhead the regeneration of the North-East',\n",
       " '57269c26f1498d1400e8e4cd': 'the result of its colouring',\n",
       " '5726e37ef1498d1400e8eeda': 'everyday clothing from previous eras has not generally survived',\n",
       " '57269344f1498d1400e8e441': 'increase their independence and strengthen legislation to limit foreign ownership of broadcasting properties',\n",
       " '5726caaaf1498d1400e8eb5f': 'respective issues with technical problems and flight delays',\n",
       " '5726caaaf1498d1400e8eb60': 'respective issues with technical problems and flight delays',\n",
       " '57276f82dd62a815002e9cd2': 'immensely popular',\n",
       " '5726b1d95951b619008f7ace': 'stating that he did not want disloyal men in his army',\n",
       " '5727311d5951b619008f86af': 'Jochi had attempted to protect Urgench from destruction',\n",
       " '57273b1a5951b619008f870a': 'because they were nomads',\n",
       " '57274126dd62a815002e9a27': 'to avoid trivialization',\n",
       " '5726ed6cf1498d1400e8f00c': 'to avoid the \"inconvenience\" of visiting a doctor or to obtain medications',\n",
       " '5726f7715951b619008f838d': 'high risk of a conflict of interest and/or the avoidance of absolute powers',\n",
       " '5726f7715951b619008f838e': 'he or she can then sell more medications to the patient',\n",
       " '5728d4c03acd2414000dffa1': 'rebelled against what they deem to be unfair laws',\n",
       " '572818f54b864d190016446d': 'simply covert lawbreaking',\n",
       " '5728df634b864d1900164fe6': 'rebellion is much more destructive',\n",
       " '5728e07e3acd2414000e00ea': \"to exert pressure to get one's political wishes on some other issue\",\n",
       " '5728e8212ca10214002daa70': 'lack of understanding of the legal ramifications',\n",
       " '5728eb1a3acd2414000e01c6': 'does not infringe the rights of others',\n",
       " '5728ebcb3acd2414000e01db': 'One defendant accused of illegally protesting nuclear power',\n",
       " '5728ed94ff5b5019007da97c': 'reminding their countrymen of injustice',\n",
       " '5728ed94ff5b5019007da97f': 'as part of a rule connected with civil disobedience',\n",
       " '5728f50baf94a219006a9e56': 'civil disobedience',\n",
       " '5728f50baf94a219006a9e57': 'her statement suggested a lack of remorse',\n",
       " '5728f50baf94a219006a9e58': 'even a likelihood of repeating her illegal actions',\n",
       " '5728fb6a1d04691400778ef6': 'neither conscientious nor of social benefit',\n",
       " '5728fb6a1d04691400778ef8': 'a protestor who attempts to escape punishment',\n",
       " '5728fc9e1d04691400778f15': 'admonitions not to',\n",
       " '5727d1c93acd2414000ded41': 'to disadvantage low-income and under-represented minority applicants applying to selective universities',\n",
       " '572810ec2ca10214002d9d08': 'The road crossed the St. Johns River at a narrow point',\n",
       " '5727e6cbff5b5019007d97f2': 'by using net wealth (adding up assets and subtracting debts',\n",
       " '5729d36b1d0469140077960b': 'due to a greater tendency to take on debts',\n",
       " '5729d44b1d04691400779613': 'Inherited wealth',\n",
       " '5729d609af94a219006aa662': 'under competitive pressure to reduce costs and maximize profits',\n",
       " '5729da0faf94a219006aa677': 'expendable nature of the worker in relation to his or her particular job',\n",
       " '5729f60caf94a219006aa6f0': 'human capital is neglected for high-end consumption',\n",
       " '572a0ecb1d0469140077971a': 'because it is a waste of resources',\n",
       " '572a1c943f37b319004786e3': 'higher quality housing increased',\n",
       " '572a2224af94a219006aa826': 'fear of their lives',\n",
       " '5727f2583acd2414000df08b': 'to a malfunction in the chameleon circuit',\n",
       " '5727f44c2ca10214002d9a34': 'the programme was not permitted to contain any \"bug-eyed monsters',\n",
       " '57282f204b864d190016468e': 'uncompleted due to the death of Elisabeth Sladen',\n",
       " '57284456ff5b5019007da05f': 'Hutchins eliminated varsity football from the university in an attempt to emphasize academics over athletics',\n",
       " '57284618ff5b5019007da0a9': 'students enrolled at Shimer were enabled to transfer automatically to the University of Chicago',\n",
       " '572864542ca10214002da2e2': 'University President Robert Maynard Hutchins de-emphasized varsity athletics in 1939 and dropped football',\n",
       " '572872822ca10214002da375': 'the Mongols beyond the Middle Kingdom saw them as too Chinese',\n",
       " '57287338ff5b5019007da234': 'fear of betrayal',\n",
       " '572881022ca10214002da41a': 'it ensured a high income and medical ethics were compatible with Confucian virtues',\n",
       " '572885023acd2414000dfa85': 'southern China withstood and fought to the last before caving in',\n",
       " '572885023acd2414000dfa86': 'southern China withstood and fought to the last before caving in',\n",
       " '5728855d3acd2414000dfa90': 'the Uighurs surrendered peacefully without violently resisting',\n",
       " '572914441d04691400779026': 'it infringed on democratic freedoms',\n",
       " '572914441d04691400779028': 'democratic freedoms',\n",
       " '572915e43f37b31900478006': 'less subject to public scrutiny and notoriety',\n",
       " '57291a7b1d04691400779040': 'stimulating the growth of local seed production and agro-dealer networks for distribution and marketing',\n",
       " '57292046af94a219006aa0be': 'an economic development programme it hopes will put the country',\n",
       " '572926653f37b3190047807b': 'the new structure would enable school drop-outs at all levels',\n",
       " '572927d06aef051400154ae1': 'open to all irrespective of age, literacy level',\n",
       " '572928bf6aef051400154af3': 'the defection of a number of Kenyan athletes to represent other countries',\n",
       " '572928bf6aef051400154af4': 'economic or financial factors',\n",
       " '57293f8a6aef051400154be1': 'requested by governments',\n",
       " '57295b5b1d04691400779316': \"due to accessory pigments that override the chlorophylls' green colors\",\n",
       " '57296de03f37b3190047839d': \"to increase the chloroplast's surface area for cross-membrane transport\",\n",
       " '57296eb01d04691400779439': 'essential for translation initiation in most chloroplasts and prokaryotes',\n",
       " '57297103af94a219006aa425': 'They help transfer and dissipate excess energy',\n",
       " '5729735c3f37b319004783fe': 'they can take shelter behind each other or spread out',\n",
       " '572976791d046914007794b1': 'no sugar',\n",
       " '57297991af94a219006aa4b7': 'chloroplasts have caught attention by developers of genetically modified crops',\n",
       " '57296d571d04691400779417': 'because one can include arbitrarily many instances of 1 in any factorization',\n",
       " '572fe393947a6a140053cdbe': \"the river's natural course\",\n",
       " '572f5875947a6a140053c89c': 'strong sedimentation',\n",
       " '572fe53104bcaa1900d76e6b': 'to counteract the constant flooding and strong sedimentation in the western Rhine Delta',\n",
       " '572f59b4a23a5019007fc587': 'the greater density of cold water',\n",
       " '573003dd947a6a140053cf43': 'Rates of sea-level rise had dropped so far',\n",
       " '573003dd947a6a140053cf45': 'ongoing tectonic subsidence',\n",
       " '57300c67947a6a140053cff3': 'it became generally accepted and found its way into numerous textbooks and official publications',\n",
       " '572facb0a23a5019007fc865': 'due to the outbreak of the First World War',\n",
       " '572fbea404bcaa1900d76c5c': 'reflects the desire to encourage consensus amongst elected members',\n",
       " '572fc659b2c2fd1400568449': \"may be of interest to a particular area such as a member's own constituency\",\n",
       " '572fcb6da23a5019007fc9f3': 'ability to alter income tax in Scotland by up to 3 pence in the pound',\n",
       " '572fd264b2c2fd14005684aa': 'it becomes an Act of the Scottish Parliament',\n",
       " '572fd8efb2c2fd14005684fd': 'due to their dispersed population and distance from the Scottish Parliament in Edinburgh',\n",
       " '572ff86004bcaa1900d76f69': 'to maintain their legitimacy',\n",
       " '572ffe6fb2c2fd14005686f1': 'to avoid prohibitively costly dowry demands, legal assistance, sports facilities',\n",
       " '57302cd004bcaa1900d772da': 'complained of its failure to consult and \"notorious intransigence',\n",
       " '57309bfb8ab72b1400f9c5e9': 'constantly expand investment',\n",
       " '57309921396df919000961f8': 'being methodical and exceptionally detailed in their Bible study, opinions and disciplined lifestyle',\n",
       " '57309d31396df91900096211': 'having a voice and vote in the administration of the church',\n",
       " '57309d31396df91900096213': 'tensions over slavery and the power of bishops in the denomination',\n",
       " '5730aaa88ab72b1400f9c64e': 'As a result of the American Revolution',\n",
       " '5730c8a1f6cb411900e2449f': 'Southern Methodist University for the George W. Bush Presidential Library',\n",
       " '5733d13e4776f419006612c6': 'a combination of poor management, internal divisions',\n",
       " '5733f062d058e614000b6636': 'superior to that of the British',\n",
       " '5733f309d058e614000b664a': 'to gain the support of the British and regain authority over his own people',\n",
       " '573408ef4776f4190066175a': 'was able to negotiate the retention of Saint Pierre and Miquelon',\n",
       " '573749741c4567190057445d': 'inertia',\n",
       " '5737a84dc3c5551400e51f5a': 'gradient of potentials'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at some predictions\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
