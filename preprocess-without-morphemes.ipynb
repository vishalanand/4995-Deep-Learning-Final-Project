{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, argparse, os, ujson as json, time, nltk, random\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:42<00:00,  1.12it/s]\n",
      "100%|██████████| 442/442 [06:09<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, questions, feature_names, no_top_words):\n",
    "    ret = []\n",
    "    for rand_words_line in questions:\n",
    "        rand_words = rand_words_line.split()\n",
    "        max_val = {}\n",
    "        null_sentence = \"\"\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            topic_sorted = topic.argsort()[:-len(topic) - 1:-1]\n",
    "            sentence = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "            if(null_sentence == \"\"):\n",
    "                null_sentence = sentence\n",
    "            for idx, x in enumerate(topic_sorted):\n",
    "                if(feature_names[idx] in rand_words):\n",
    "                    max_val[x] = sentence\n",
    "\n",
    "        if(len(max_val) == 0):\n",
    "            ret.append(null_sentence)\n",
    "        for k in reversed(sorted(max_val.keys())):\n",
    "            ret.append(max_val[k])\n",
    "            break\n",
    "    return ret\n",
    "\n",
    "def call_strat(documents, ques, no_features, no_topics, no_top_words):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "    return display_topics(lda, ques, tf_feature_names, no_top_words)\n",
    "\n",
    "def parse_documents(raw_input, no_features, no_topics, no_top_words):\n",
    "    json_data=open(raw_input)\n",
    "    data = json.load(json_data)['data']\n",
    "    json_data.close()\n",
    "    output_dataset = {'qids': [], 'questions': [], 'answers': [], 'contexts': [], 'qid2cid': []}\n",
    "    article_topics = []\n",
    "    for article in tqdm(data):\n",
    "        docs = []\n",
    "        ques = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            docs.append(paragraph['context'])\n",
    "            for qa in paragraph['qas']:\n",
    "                ques.append(qa['question'])\n",
    "        article_topics.append(call_strat(docs, ques, no_features, no_topics, no_top_words))\n",
    "    return article_topics\n",
    "\n",
    "def main(raw_input):\n",
    "    no_features = 1000\n",
    "    no_topics = 20\n",
    "    no_top_words = 5\n",
    "    return parse_documents(raw_input, no_features, no_topics, no_top_words)\n",
    "\n",
    "topic_models_dev = main(\"./data/datasets_without_topics_new/dev-v1.1.json\")\n",
    "topic_models_train = main(\"./data/datasets_without_topics_new/train-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to ./data/datasets_without_topics_new/dev-v1.1-processed.txt\n",
      "\r",
      "  0%|          | 0/10570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data1 48\n",
      "Length of process_data_ctx 2067\n",
      "Length of process_data_q 10570\n",
      "Lenght of texts: 10570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:48<00:00, 219.76it/s]\n",
      "  0%|          | 3/2067 [00:00<01:22, 24.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of texts: 2067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1541/2067 [01:15<00:25, 20.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2067/2067 [01:40<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 150.9741 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to ./data/datasets_without_topics_new/train-v1.1-processed.txt\n",
      "\r",
      "  0%|          | 0/87599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data1 442\n",
      "Length of process_data_ctx 18896\n",
      "Length of process_data_q 87599\n",
      "Lenght of texts: 87599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 383/87599 [00:01<07:07, 204.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n",
      "Tokens empty\n",
      "Not empty\n",
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 469/87599 [00:02<07:10, 202.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n",
      "Tokens empty\n",
      "Not empty\n",
      "Tokens empty\n",
      "Not empty\n",
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 68210/87599 [04:54<01:23, 231.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 83992/87599 [06:01<00:15, 232.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [06:15<00:00, 233.13it/s]\n",
      "  0%|          | 2/18896 [00:00<21:32, 14.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of texts: 18896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 11376/18896 [08:19<05:30, 22.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens empty\n",
      "Not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18896/18896 [14:30<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1257.8500 (s)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(texts):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    outputs = []\n",
    "    print(\"Lenght of texts:\", len(texts))\n",
    "    for text in tqdm(texts):\n",
    "        chars_l = []\n",
    "        lemma_l = []\n",
    "        ner_tags = []\n",
    "        offsets_l = []\n",
    "        #tokens = [token for token in nltk.wordpunct_tokenize(text)]\n",
    "        #tokens = [token for token in nltk.word_tokenize(text)]\n",
    "        #text = \"Who was Beyonce's duet with in ''Beautiful Liar''? video school campaign music concert\"\n",
    "        tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').replace('\\n', ' ').replace('\"', '\\\"') for token in nltk.word_tokenize(text)]\n",
    "        words = tokens\n",
    "        tags_l = nltk.pos_tag(words)\n",
    "        pos_tags = [x[1] for x in tags_l]\n",
    "        \n",
    "        for word in words:\n",
    "            chars_l.append(word[0])\n",
    "            lemma_l.append(wordnet_lemmatizer.lemmatize(word.lower()))\n",
    "        \n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(words)):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                for i in range(len(chunk)):\n",
    "                    ner_tags.append(chunk.label().replace('ORGANIZATION', 'ORG'))\n",
    "            else:\n",
    "                ner_tags.append('')\n",
    "        \n",
    "        cs = get_char_word_loc_mapping(text, words)\n",
    "        span = []\n",
    "        last_span = -1\n",
    "        if(bool(cs) == False):\n",
    "            #print(\"Tokens empty\", text)\n",
    "            print(\"Tokens empty\")\n",
    "            chars_l = []\n",
    "            lemma_l = []\n",
    "            ner_tags = []\n",
    "            offsets_l = []\n",
    "            tokens = [token for token in nltk.wordpunct_tokenize(text)]\n",
    "            \n",
    "            words = tokens\n",
    "            tags_l = nltk.pos_tag(words)\n",
    "            pos_tags = [x[1] for x in tags_l]\n",
    "\n",
    "            for word in words:\n",
    "                chars_l.append(word[0])\n",
    "                lemma_l.append(wordnet_lemmatizer.lemmatize(word.lower()))\n",
    "\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(words)):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    for i in range(len(chunk)):\n",
    "                        ner_tags.append(chunk.label().replace('ORGANIZATION', 'ORG'))\n",
    "                else:\n",
    "                    ner_tags.append('')\n",
    "\n",
    "            cs = get_char_word_loc_mapping(text, words)\n",
    "            span = []\n",
    "            last_span = -1\n",
    "            if(bool(cs) == False):\n",
    "                print(\"Still empty\", text)\n",
    "            else:\n",
    "                print(\"Not empty\")\n",
    "                for key, val in cs.items():\n",
    "                    (i, j) = val\n",
    "                    if((last_span == -1) or (last_span <= key)):\n",
    "                        span.append((key, key + len(i)))\n",
    "                        last_span = key + len(i)\n",
    "        else:\n",
    "            for key, val in cs.items():\n",
    "                (i, j) = val\n",
    "                if((last_span == -1) or (last_span <= key)):\n",
    "                    span.append((key, key + len(i)))\n",
    "                    last_span = key + len(i)\n",
    "        \n",
    "        output = {'words':words,'chars':chars_l,'offsets':span,'pos':pos_tags,'lemma':lemma_l,'ner': ner_tags, 'cs': cs}\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "def find_answer(offsets, begin_offset, end_offset):\n",
    "    start = [i for i, tok in enumerate(offsets) if tok[0] == begin_offset]\n",
    "    end = [i for i, tok in enumerate(offsets) if tok[1] == end_offset]\n",
    "    if(len(start) == 1 and len(end) == 1):\n",
    "        return start[0], end[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_dataset(data):\n",
    "    cnt = 10\n",
    "    print(\"Length of process_data_ctx\", len(data['contexts']))\n",
    "    print(\"Length of process_data_q\", len(data['questions']))\n",
    "    q_tokens = tokenize(data['questions'])\n",
    "    c_tokens = tokenize(data['contexts'])\n",
    "    abc = []\n",
    "    f = open('out1.txt', 'w')\n",
    "    for idx in range(len(data['qids'])):\n",
    "        marker = c_tokens[data['qid2cid'][idx]]['offsets']\n",
    "        ans_tokens = []\n",
    "        if len(data['answers']) > 0:\n",
    "            for ans_idx, ans in enumerate(data['answers'][idx]):\n",
    "                found = find_answer(marker, ans['answer_start'], ans['answer_start'] + len(ans['text']))\n",
    "                if found:\n",
    "                    ans_tokens.append(found)\n",
    "                else:\n",
    "                    abc.append(1)\n",
    "                    f.write(str(len(abc)) + \" Not found for \" + str(ans_idx) + str(data['contexts'][data['qid2cid'][idx]][ans['answer_start']: (ans['answer_start'] + len(ans['text']))]) + \"\\t\" + str(ans['answer_start']) + \" \" + str(ans['answer_start'] + len(ans['text'])) + \"\\n\")\n",
    "                    f.write(json.dumps(c_tokens[data['qid2cid'][idx]]['cs']) + \"\\n\")\n",
    "                    f.write(json.dumps(marker) + \"\\n\\n\")\n",
    "        yield {\n",
    "           'id':data['qids'][idx], 'question': q_tokens[idx]['words'],\n",
    "           'ques_char': q_tokens[idx]['chars'],\n",
    "           'ctxt': c_tokens[data['qid2cid'][idx]]['words'],\n",
    "           'ctxt_char': c_tokens[data['qid2cid'][idx]]['chars'],\n",
    "           'ans_pos': c_tokens[data['qid2cid'][idx]]['offsets'],\n",
    "           'ans': ans_tokens, 'qlemma': q_tokens[idx]['lemma'],\n",
    "           'ques_pos': q_tokens[idx]['pos'], 'qner': q_tokens[idx]['ner'],\n",
    "           'ctxt_lemma': c_tokens[data['qid2cid'][idx]]['lemma'],\n",
    "           'ctxt_pos': c_tokens[data['qid2cid'][idx]]['pos'],\n",
    "           'ctxt_ner': c_tokens[data['qid2cid'][idx]]['ner'],\n",
    "        }\n",
    "\n",
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    acc = ''\n",
    "    current_token_idx = 0\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context):\n",
    "        if char != u' ' and char != u'\\n' and char!=chr(8201) and char !=chr(12288) and char!=chr(8239):\n",
    "            acc += char\n",
    "            context_token = context_tokens[current_token_idx]\n",
    "            if acc == context_token:\n",
    "                syn_start = char_idx - len(acc) + 1\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx)\n",
    "                acc = ''\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping\n",
    "\n",
    "topic_models_dev = []\n",
    "topic_models_train = []\n",
    "for args_split, topic_models in [(\"dev-v1.1\", topic_models_dev), (\"train-v1.1\", topic_models_train)]:\n",
    "    t0 = time.time()\n",
    "    in_file = os.path.join(\"./data/datasets_without_topics_new\", args_split + '.json')\n",
    "    json_data=open(in_file)\n",
    "    data = json.load(json_data)['data']\n",
    "    json_data.close()\n",
    "    print(\"Length of data1\", len(data))\n",
    "    output_dataset = {'qids': [], 'questions': [], 'answers': [], 'contexts': [], 'qid2cid': []}\n",
    "    for article_idx, article in enumerate(data):\n",
    "        article_q_cnt = 0\n",
    "        for paragraph_idx, paragraph in enumerate(article['paragraphs']):\n",
    "            output_dataset['contexts'].append(paragraph['context'])\n",
    "            for qa_idx, qa in enumerate(paragraph['qas']):\n",
    "                output_dataset['qids'].append(qa['id'])\n",
    "                #output_dataset['questions'].append(qa['question'] + \" \" + topic_models[article_idx][article_q_cnt])\n",
    "                output_dataset['questions'].append(qa['question'])# + \" \" + topic_models[article_idx][article_q_cnt])\n",
    "                output_dataset['qid2cid'].append(len(output_dataset['contexts']) - 1)\n",
    "                if 'answers' in qa:\n",
    "                    output_dataset['answers'].append(qa['answers'])\n",
    "                article_q_cnt = article_q_cnt + 1\n",
    "    dataset = output_dataset\n",
    "    out_file = os.path.join(\"./data/datasets_without_topics_new\", args_split + \"-processed\" + \".txt\")\n",
    "    print('Writing to %s' % out_file, file=sys.stderr)\n",
    "    with open(out_file, 'w') as f:\n",
    "        for ex in process_dataset(dataset):\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    print('Time: %.4f (s)' % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
