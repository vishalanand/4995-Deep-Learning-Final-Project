{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, json, time, string, re, pickle, unicodedata, numpy as np, unicodedata\n",
    "import torch.optim as optim, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torch, torch.nn as nn, torch.nn.functional as func, evaluate as ev, time\n",
    "from tqdm import tqdm\n",
    "from torch import matmul\n",
    "from torch.nn.functional import dropout\n",
    "from torch import LongTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "random_seed = 11\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [00:23, 3772.49it/s]\n",
      "10570it [00:04, 2588.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train examples = 86422\n",
      "Num dev examples = 10493\n"
     ]
    }
   ],
   "source": [
    "#Load processed datasets with topics\n",
    "\n",
    "train = []\n",
    "dev = []\n",
    "\n",
    "f = open(\"data/datasets_topics_new/train-v1.1-processed.txt\", 'r')\n",
    "for line in tqdm(f):\n",
    "    example = json.loads(line)\n",
    "    if len(example['ans']) > 0:\n",
    "        train.append(example)\n",
    "f.close()\n",
    "\n",
    "f1 = open(\"data/datasets_topics_new/dev-v1.1-processed.txt\", 'r')\n",
    "for line in tqdm(f1):\n",
    "    example = json.loads(line)\n",
    "    if len(example['ans']) > 0:\n",
    "        dev.append(example)\n",
    "f1.close()\n",
    "\n",
    "time.sleep(1)\n",
    "print('Num train examples = %d' % len(train))\n",
    "print('Num dev examples = %d' % len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  7 19:03:57 2018 Build word o index mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96915/96915 [00:04<00:00, 20722.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  7 19:04:03 2018 word_set\n",
      "#words 125516\n",
      "#chars 757\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "dev_offsets = {ex['id']: ex['ans_pos'] for ex in dev}\n",
    "print(time.ctime() + \" Build word o index mapping\")\n",
    "time.sleep(1)\n",
    "\n",
    "all_samples = train + dev\n",
    "word2idx = {'<NULL>':0, '<UNK>':1} \n",
    "char2idx = {'<NULL>':0, '<UNK>':1}\n",
    "\n",
    "word_set = set()\n",
    "char_set = set()\n",
    "feature_set = set()\n",
    "for each_example in tqdm(all_samples):\n",
    "    \n",
    "    for word in each_example['question']:\n",
    "        word_set.add(word)\n",
    "\n",
    "    for word in each_example['ctxt']:\n",
    "        word_set.add(word)\n",
    "\n",
    "    for char in each_example['ques_char']:\n",
    "        char_set.add(char)\n",
    "\n",
    "    for char in each_example['ctxt_char']:\n",
    "        char_set.add(char)\n",
    "\n",
    "        \n",
    "print(time.ctime() + \" word_set\")\n",
    "\n",
    "for i, word in enumerate(word_set):\n",
    "    if word not in word2idx.keys():\n",
    "        word2idx[word] = i+2\n",
    "\n",
    "\n",
    "for i, char in enumerate(char_set):\n",
    "    if char not in char2idx.keys():\n",
    "        char2idx[char] = i+2\n",
    "\n",
    "print('#words', len(word2idx))    \n",
    "print('#chars', len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(word2idx,char2idx,glove_file):\n",
    "    \n",
    "    corpus_words = set()\n",
    "    for key in word2idx.keys():\n",
    "        if key not in {'<NULL>','<UNK>'}:\n",
    "            corpus_words.add(key)\n",
    "    \n",
    "    corpus_chars = set()\n",
    "    for key in char2idx.keys():\n",
    "        if key not in {'<NULL>','<UNK>'}:\n",
    "            corpus_chars.add(key)\n",
    "            \n",
    "    idx2word = {v: k for k, v in char2idx.items()}\n",
    "    idx2char = {v: k for k, v in char2idx.items()}\n",
    "    \n",
    "    glove_big = {}\n",
    "    with open(glove_file, \"rb\") as infile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode()#.lower()\n",
    "            nums = map(float, parts[1:])\n",
    "            if (word in corpus_words) or (word in corpus_chars):\n",
    "                glove_big[word] = list(nums)\n",
    "                \n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    \n",
    "    weights_matrix = np.random.normal(scale=0.6, size=(len(idx2word), 300))#np.zeros((len(idx2word), 300))\n",
    "    words_found = 0\n",
    "\n",
    "    for word in corpus_words:\n",
    "        if word in glove_big.keys():\n",
    "            weights_matrix[word2idx[word]] = glove_big[word]\n",
    "            words_found += 1\n",
    "    print(\"%d words found out of %d\" %(words_found, len(idx2word)))\n",
    "\n",
    "    weights_matrix_char = np.random.normal(scale=0.6, size=(len(idx2char), 300))#np.zeros((len(idx2word), 300))\n",
    "    chars_found = 0\n",
    "\n",
    "    for char in corpus_chars:\n",
    "        if char in glove_big.keys():\n",
    "            weights_matrix[char2idx[char]] = glove_big[char]\n",
    "            chars_found += 1\n",
    "    print(\"%d chars found out of %d\" %(chars_found, len(idx2char)))\n",
    "    \n",
    "    return weights_matrix, weights_matrix_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100005 words found out of 125516\n",
      "263 chars found out of 757\n"
     ]
    }
   ],
   "source": [
    "weights_matrix, weights_matrix_char = load_embeddings(word2idx,char2idx,'data/glove.840B.300d.txt')\n",
    "\n",
    "emb_layer = nn.Embedding(num_embeddings= len(word2idx), embedding_dim = 300, padding_idx=0).cuda()\n",
    "emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "\n",
    "emb_layer_char = nn.Embedding(num_embeddings= len(char2idx), embedding_dim = 300, padding_idx=0).cuda()\n",
    "emb_layer_char.weight.data.copy_(torch.from_numpy(weights_matrix_char))\n",
    "\n",
    "emb_layer.weight.requires_grad = False\n",
    "emb_layer_char.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating features\n",
    "'''\n",
    "class Transform(Dataset):\n",
    "    def __init__(self, example, word2idx, char2idx,status):#feature2idx\n",
    "        self.example = example\n",
    "        self.status = status\n",
    "        self.word2idx =  word2idx\n",
    "        self.char2idx =  char2idx\n",
    "        \n",
    "        self.feature2idx = {'RB': 4,'DT': 5,'NN': 6,'VBZ': 7,'JJ': 8,'LS': 9,'VB': 10,'NNP': 11,'POS': 12,'IN': 13,'CC': 14,'VBG': 15,'PRP': 16,'NNS': 17,'VBN': 18,'TO': 19,'WRB': 20,\n",
    "        'VBD': 21,'CD': 22,'PDT': 23,'WDT': 24,'WP': 25,'VBP': 26,'UH': 27,'ORG': 28,'FACILITY': 29,'GPE': 30,'PERSON': 31,'JJS': 32,'NNPS': 33,\n",
    "        'RP': 34,'LOCATION': 35,'FW': 36,'JJR': 37,'RBS': 38,'MD': 39,'SYM': 40,'EX': 41,'RBR': 42,'GSP': 43}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.example)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        ctxt = self.example[index]['ctxt']\n",
    "        ques = self.example[index]['question']\n",
    "        ctxt_char = self.example[index]['ctxt_char']\n",
    "        question_char = self.example[index]['ques_char']\n",
    "        total_length_context = len(ctxt)\n",
    "        total_length_question = len(ques)\n",
    "        \n",
    "        cpos_d = self.example[index]['ctxt_pos']\n",
    "        cner_d = self.example[index]['ctxt_ner']\n",
    "        qpos_d = self.example[index]['ques_pos']\n",
    "        qner_d = self.example[index]['qner']\n",
    "        \n",
    "        context_cased = set(ctxt) #all words in context\n",
    "        context_uncased = set([word.lower() for word in ctxt]) #all words in context lowercase\n",
    "        word_ques = [self.word2idx.get(word, 1) for word in ques] # get index of question word\n",
    "        word_con = [self.word2idx.get(word, 1) for word in ctxt] # get index of context word\n",
    "        ques_cased = set(ques)\n",
    "        ques_uncased = set([element.lower() for element in ques])\n",
    "        char_ques = [self.char2idx.get(char, 1) for char in question_char]\n",
    "        char_con = [self.char2idx.get(char, 1) for char in ctxt_char]\n",
    "        context_lemmas = set(self.example[index]['ctxt_lemma'])\n",
    "        ques_lemmas = set(self.example[index]['qlemma'])\n",
    "        count_of_word_context = {word.lower(): ctxt.count(word.lower()) for word in ctxt}\n",
    "        count_of_word_question = {word.lower(): ques.count(word.lower()) for word in ques}\n",
    "        \n",
    "        #creating question and context feature vector\n",
    "        context_feature = torch.zeros(len(ctxt), 44)\n",
    "        ques_feature = torch.zeros(len(ques), 44)\n",
    "        \n",
    "        #exact match features being one \n",
    "        for i in range(len(ctxt)):\n",
    "            if ctxt[i] in ques_cased:\n",
    "                context_feature[i][0] = 1.0\n",
    "            if ctxt[i] in ques_uncased:\n",
    "                context_feature[i][1] = 1.0\n",
    "            if ctxt[i] in ques_lemmas:\n",
    "                context_feature[i][2] = 1.0\n",
    "            \n",
    "        for i in range(len(cpos_d)):\n",
    "            f = cpos_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                context_feature[i][self.feature2idx[f]] = 1.0\n",
    "            \n",
    "        for i in range(len(cner_d)):\n",
    "            f = cner_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                context_feature[i][self.feature2idx[f]] = 1.0\n",
    "        \n",
    "        for i in range(len(qpos_d)):\n",
    "            f = qpos_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                ques_feature[i][self.feature2idx[f]] = 1.0\n",
    "            \n",
    "        for i in range(len(qner_d)):\n",
    "            f = qner_d[i]\n",
    "            if f in self.feature2idx:\n",
    "                ques_feature[i][self.feature2idx[f]] = 1.0\n",
    "\n",
    "        for i in range(len(ques)):\n",
    "            if ques[i] in ques_cased:\n",
    "                ques_feature[i][0] = 1.0\n",
    "            if ques[i] in ques_uncased:\n",
    "                ques_feature[i][1] = 1.0\n",
    "            if ques[i] in ques_lemmas:\n",
    "                ques_feature[i][2] = 1.0\n",
    "            \n",
    "        for i in range(total_length_context):\n",
    "            context_feature[i][3] = float(count_of_word_context[ctxt[i].lower()]/(1.0 * total_length_context))\n",
    "        \n",
    "        for i in range(total_length_question):\n",
    "            ques_feature[i][3] = float(count_of_word_question[ques[i].lower()]/ (1.0 * total_length_question))\n",
    "\n",
    "        if self.status == 'train': # take only the first answer\n",
    "            start_positions = LongTensor(1).fill_(self.example[index]['ans'][0][0])\n",
    "            end_positions = LongTensor(1).fill_(self.example[index]['ans'][0][1])\n",
    "        else: # consider all answers while evaluating\n",
    "            start_positions = []\n",
    "            end_positions = []\n",
    "            for ans in self.example[index]['ans']:\n",
    "                start_positions.append(ans[0])\n",
    "                end_positions.append(ans[1])\n",
    "                \n",
    "        return LongTensor(word_con), LongTensor(char_con), context_feature, LongTensor(word_ques), LongTensor(char_ques), ques_feature, start_positions, end_positions, self.example[index]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10493\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Transform(train, word2idx, char2idx, status='train') #feature2idx,\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "dev_dataset = Transform(dev, word2idx, char2idx, status='test') #feature2idx,\n",
    "dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "print(len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "padding batches to same length\n",
    "'''\n",
    "def module_fn(batch_len, con_features, con_words, con_char,max_len): #\n",
    "    con1_words = LongTensor(batch_len,max_len).zero_() #N X max_len\n",
    "    con1_chars = LongTensor(batch_len,max_len).zero_()\n",
    "    con1_features = torch.zeros(batch_len,max_len,con_features[0].size(1))\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        con1_words[i,0:con_words[i].size(0)].copy_(con_words[i])\n",
    "        con1_chars[i, 0:con_char[i].size(0)].copy_(con_char[i])\n",
    "        con1_features[i,:con_words[i].size(0)].copy_(con_features[i])\n",
    "\n",
    "    return con1_words, con1_chars, con1_features, #con1_masked_val\n",
    "\n",
    "def pad_batch(batch):\n",
    "    \n",
    "    con_words = []\n",
    "    con_char = []\n",
    "    con_features = []\n",
    "    ques_words = []\n",
    "    ques_char = []\n",
    "    ques_features = []\n",
    "    ids = []\n",
    "\n",
    "    length_con = []\n",
    "    length_ques = []\n",
    "    for val in batch:\n",
    "        con_words.append(val[0])\n",
    "        length_con.append(val[0].size(0))\n",
    "        con_char.append(val[1])\n",
    "        con_features.append(val[2])\n",
    "        ques_words.append(val[3])\n",
    "        length_ques.append(val[3].size(0))\n",
    "        ques_char.append(val[4])\n",
    "        ques_features.append(val[5])\n",
    "        ids.append(val[len(val)-1])\n",
    "\n",
    "    if torch.is_tensor(batch[0][6]): #train\n",
    "        abc1 = []\n",
    "        abc2 = []\n",
    "        for val in batch:\n",
    "            abc1.append(val[6])\n",
    "            abc2.append(val[7])\n",
    "        batch_start_positions = torch.cat(abc1)\n",
    "        batch_end_positions = torch.cat(abc2)\n",
    "    else:        #eval mode\n",
    "        batch_start_positions = []\n",
    "        batch_end_positions = []\n",
    "        for val in batch:\n",
    "            batch_start_positions.append(val[6])\n",
    "            batch_end_positions.append(val[7])\n",
    "    return (*module_fn(len(batch), con_features, con_words, con_char,max(length_con))),length_con,(*module_fn(len(batch), ques_features, ques_words, ques_char,max(length_ques))), length_ques, batch_start_positions, batch_end_positions, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=45, sampler=train_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "        dev_dataset, batch_size=32, sampler=dev_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention(attn, con, ques): \n",
    "    \n",
    "    if attn == 'interac': \n",
    "        coattn = matmul(con, ques.transpose(2,1))    \n",
    "    else: \n",
    "        coattn = matmul(ques, ques.transpose(2,1))\n",
    "        #set diagonal elements to 0\n",
    "        for i in range(coattn.size(0)):\n",
    "            mask = torch.diag(torch.ones(min(coattn.size(1),coattn.size(2)))).cuda()\n",
    "            coattn[i, :, :].data = mask*0 + (1. - mask)*coattn[i, :, :].data\n",
    "            \n",
    "    attn_dist = func.softmax(coattn, dim=2) #attention distribution of query for jth context word\n",
    "    attended = matmul(attn_dist, ques) #attended query vector for all context words\n",
    "    return attended\n",
    "\n",
    "def encode(rnn, seq, lengths, flag):\n",
    "    \n",
    "    key=sorted(range(len(lengths)), key=lambda k: lengths[k], reverse=True)\n",
    "\n",
    "    seq = seq.index_select(0, Variable(torch.LongTensor(key).cuda())).transpose(0, 1)\n",
    "    rnn_input = nn.utils.rnn.pack_padded_sequence(seq, sorted(lengths, reverse=True))\n",
    "    rev_key = sorted(range(len(key)), key=lambda k: key[k])\n",
    "    dropout_input = func.dropout(rnn_input.data, p=0.2, training=flag)\n",
    "    rnn_input = nn.utils.rnn.PackedSequence(dropout_input, rnn_input.batch_sizes)\n",
    "    output = rnn(rnn_input)[0]\n",
    "    output = nn.utils.rnn.pad_packed_sequence(output)[0].transpose(0, 1).index_select(0, Variable(torch.LongTensor(rev_key).cuda()))\n",
    "    output = func.dropout(output,p=0.2,training=flag)\n",
    "    \n",
    "    return output.contiguous() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.char_rnn = nn.LSTM(input_size=300, hidden_size=50, bidirectional=True)\n",
    "        doc_input_size = 400 + 44\n",
    "        self.encoding_rnn = nn.LSTM(input_size=doc_input_size, hidden_size=100, bidirectional=True)\n",
    "        self.sfu_l1 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_l2 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_self_l1 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        self.sfu_self_l2 = nn.ModuleList([nn.Linear(800,200),nn.Linear(800,200)])\n",
    "        \n",
    "        self.final_rec_nn = nn.ModuleList([nn.LSTM(input_size=200, hidden_size=100, bidirectional=True),nn.LSTM(input_size=200, hidden_size=100, bidirectional=True)])\n",
    "        #\n",
    "        FCN = nn.Sequential(\n",
    "                nn.Linear(600, 100),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(100, 1),)\n",
    "        \n",
    "        self.FeedForward_s = nn.ModuleList([FCN,FCN])\n",
    "        self.sfu_s_l1 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.sfu_s_l2 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.FeedForward_e = nn.ModuleList([FCN,FCN])\n",
    "        self.sfu_e_l1 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        self.sfu_e_l2 = nn.ModuleList([nn.Linear(400, 200),nn.Linear(400, 200)])\n",
    "        \n",
    "        \n",
    "    def forward(self, con_embed_words, con_embed_chars, con_feat, con_length, ques_embed_words, ques_embed_chars, ques_feat, ques_length):\n",
    "       \n",
    "        con_char_feat = encode(self.char_rnn, con_embed_chars, con_length, self.training) #encode context characters\n",
    "        ques_char_feat = encode(self.char_rnn, ques_embed_chars, ques_length, self.training)#encode question characters\n",
    "        val1 = torch.cat([con_embed_words,con_char_feat,con_feat], 2)#join all context features\n",
    "        encoded_con = encode(self.encoding_rnn, val1, con_length, self.training)#encode context\n",
    "        val2 = torch.cat([ques_embed_words,ques_char_feat,ques_feat], 2)#join all question features\n",
    "        encoded_ques = encode(self.encoding_rnn, val2, ques_length, self.training)#encode question\n",
    "        ctxt = encoded_con\n",
    "        \n",
    "        for i in range(2):\n",
    "            #interactive aligning\n",
    "            attended_ques = Attention('interac', ctxt, encoded_ques)\n",
    "            #compute query aware representation\n",
    "            val_s = torch.cat([ctxt, attended_ques, torch.mul(ctxt, attended_ques), ctxt - attended_ques], 2)\n",
    "            comp = func.tanh(self.sfu_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_l2[i](val_s))\n",
    "            ques_aware_ctxt = gate * comp + (1-gate) * ctxt #queey aware ctxt\n",
    "            #self aligning\n",
    "            attended_ctxt = Attention('self',0,ques_aware_ctxt)\n",
    "            #compute context aware representation\n",
    "            val_s = torch.cat([ques_aware_ctxt, attended_ctxt, torch.mul(ques_aware_ctxt,attended_ctxt), ques_aware_ctxt - attended_ctxt], 2)\n",
    "            comp = func.tanh(self.sfu_self_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_self_l2[i](val_s))\n",
    "            ctxt_aware_ctxt = gate * comp + (1-gate) * attended_ctxt # self-aware\n",
    "            #Aggregating\n",
    "            ctxt = encode(self.final_rec_nn[i], ctxt_aware_ctxt, con_length, self.training)\n",
    "        \n",
    "        #memory Answer Pointer\n",
    "        memory_s = encoded_ques[:,-1,:].resize(encoded_ques.size(0),1, encoded_ques.size(2))\n",
    "         \n",
    "        for i in range(2):\n",
    "            #prob dist for start using memory and fully aware context\n",
    "            start = self.FeedForward_s[i](torch.cat([ctxt, memory_s.expand(-1,ctxt.size(1),-1), torch.mul(ctxt,memory_s.expand(-1,ctxt.size(1),-1))], 2))\n",
    "            start_prob = func.softmax(start.squeeze(2), dim=1) \n",
    "            #evidence vector for start\n",
    "            evidence_s = matmul(start_prob.unsqueeze(1),ctxt)\n",
    "            \n",
    "            #fuse memory and evidence\n",
    "            val_s = torch.cat([memory_s, evidence_s], 2)\n",
    "            l1_s = func.tanh(self.sfu_s_l1[i](val_s))\n",
    "            gate = func.sigmoid(self.sfu_s_l2[i](val_s))\n",
    "            memory_e = gate * l1_s + (1-gate) * memory_s #new memory\n",
    "            \n",
    "            #prob dist for end using new memory and fully aware context\n",
    "            end = self.FeedForward_e[i](torch.cat([ctxt, memory_e.expand(-1,ctxt.size(1),-1), torch.mul(ctxt,memory_e.expand(-1,ctxt.size(1),-1))], 2))\n",
    "            end_prob = func.softmax(end.squeeze(2), dim=1)\n",
    "            #evidence vector for end\n",
    "            evidence_e = matmul(end_prob.resize(end_prob.size(0),1, end_prob.size(1)),ctxt)\n",
    "            \n",
    "            # fuse to generate new memory\n",
    "            val_e = torch.cat([memory_e, evidence_e], 2)\n",
    "            l1_e = func.tanh(self.sfu_e_l1[i](val_e))\n",
    "            gate_e = func.sigmoid(self.sfu_e_l2[i](val_e))\n",
    "            memory_s = gate_e * l1_e + (1-gate_e) * memory_e\n",
    "\n",
    "        start_prob = func.log_softmax(start.squeeze(2), dim=1) \n",
    "        end_prob = func.log_softmax(end.squeeze(2), dim=1)\n",
    "        \n",
    "        #print(end_prob.size())\n",
    "        #print(start_prob.size())\n",
    "    \n",
    "        return start_prob, end_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "reader = Model().cuda()\n",
    "optimizer = optim.Adamax(reader.parameters())\n",
    "torch.cuda.set_device(-1)\n",
    "model_name = 'with_topics_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mle\n",
    "def findmax(score_s,score_e):\n",
    "    max_len = 15\n",
    "    pred_s = []\n",
    "    pred_e = []\n",
    "    dim = score_s.shape[1]\n",
    "    \n",
    "    for i in range(score_s.shape[0]):\n",
    "        joint = np.zeros((dim,dim))\n",
    "        for start in range(dim):\n",
    "            if start+max_len < dim:\n",
    "                joint[start,start:start+max_len]=score_s[i,:][start]*score_e[i,:][start:start+max_len]\n",
    "            else:\n",
    "                joint[start,start:dim] = score_s[i,:][start]*score_e[i,:][start:dim]\n",
    "                \n",
    "        s_idx, e_idx = np.argwhere(joint.max() == joint)[0,:] \n",
    "        \n",
    "        pred_s.append(s_idx)\n",
    "        pred_e.append(e_idx)\n",
    "        \n",
    "    return pred_s, pred_e\n",
    "\n",
    "def validate(data_loader, network, positions, texts, answers, official):\n",
    "    f1 = 0\n",
    "    em = 0\n",
    "    examples = 0\n",
    "    results = {}\n",
    "    for ex in data_loader:\n",
    "        batch_size = ex[0].size(0)\n",
    "        ex_id = ex[-1]\n",
    "        \n",
    "        #Predicting....\n",
    "        network.eval()\n",
    "\n",
    "        con_words = Variable(ex[0].cuda())\n",
    "        con_chars = Variable(ex[1].cuda())\n",
    "        con_feat = Variable(ex[2].cuda())\n",
    "        ques_words = Variable(ex[4].cuda())\n",
    "        ques_chars = Variable(ex[5].cuda())\n",
    "        ques_feat = Variable(ex[6].cuda())\n",
    "\n",
    "        con_embed_words = func.dropout(emb_layer(con_words), p=0.2)\n",
    "        ques_embed_words = func.dropout(emb_layer(ques_words), p=0.2)\n",
    "        con_embed_chars = func.dropout(emb_layer_char(con_chars), p=0.2)\n",
    "        ques_embed_chars = func.dropout(emb_layer_char(ques_chars), p=0.2)\n",
    "\n",
    "        score_s, score_e = network(con_embed_words,con_embed_chars,con_feat,ex[3],ques_embed_words,ques_embed_chars,ques_feat,ex[7])\n",
    "        score_s.exp_() \n",
    "        score_e.exp_()\n",
    "        pred_s, pred_e = findmax(score_s.data.cpu().numpy(),score_e.data.cpu().numpy())\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            s_posn = positions[ex_id[i]][pred_s[i]][0]\n",
    "            e_posn = positions[ex_id[i]][pred_e[i]][1]\n",
    "            prediction = texts[ex_id[i]][s_posn:e_posn]\n",
    "            \n",
    "            if official:\n",
    "                results[ex_id[i]]=prediction\n",
    "\n",
    "            ground_truths = answers[ex_id[i]]\n",
    "            em += ev.metric_max_over_ground_truths(ev.exact_match_score, prediction, ground_truths)\n",
    "            f1 += ev.metric_max_over_ground_truths(ev.f1_score, prediction, ground_truths)\n",
    "                 \n",
    "        examples += batch_size\n",
    "    \n",
    "    if official:\n",
    "        return f1/examples, em/examples, results\n",
    "    else:\n",
    "        return f1/examples, em/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 2498.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_texts = {}\n",
    "dev_answers = {}\n",
    "f2 = open(\"data/datasets_topics_new/dev-v1.1.json\", 'r')\n",
    "examples = json.load(f2)['data']\n",
    "for article in tqdm(examples):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            dev_texts[qa['id']] = paragraph['context']\n",
    "            dev_answers[qa['id']] = list(map(lambda x: x['text'], qa['answers']))\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch-step 0 \t/ 1921 \tloss 0.2547504213121202\n",
      "Epoch 1 Batch-step 50 \t/ 1921 \tloss 11.05772533416748\n",
      "Epoch 1 Batch-step 100 \t/ 1921 \tloss 10.836053699917263\n",
      "Epoch 1 Batch-step 150 \t/ 1921 \tloss 10.766353437635633\n",
      "Epoch 1 Batch-step 200 \t/ 1921 \tloss 10.741842036777072\n",
      "Epoch 1 Batch-step 250 \t/ 1921 \tloss 10.440506299336752\n",
      "Epoch 1 Batch-step 300 \t/ 1921 \tloss 9.555918439229329\n",
      "Epoch 1 Batch-step 350 \t/ 1921 \tloss 9.187462732526992\n",
      "Epoch 1 Batch-step 400 \t/ 1921 \tloss 8.761592303382026\n",
      "Epoch 1 Batch-step 450 \t/ 1921 \tloss 8.352630509270561\n",
      "Epoch 1 Batch-step 500 \t/ 1921 \tloss 7.9335609118143715\n",
      "Epoch 1 Batch-step 550 \t/ 1921 \tloss 7.776992162068685\n",
      "Epoch 1 Batch-step 600 \t/ 1921 \tloss 7.499982293446859\n",
      "Epoch 1 Batch-step 650 \t/ 1921 \tloss 7.272015232510037\n",
      "Epoch 1 Batch-step 700 \t/ 1921 \tloss 7.11052762137519\n",
      "Epoch 1 Batch-step 750 \t/ 1921 \tloss 7.051740349663628\n",
      "Epoch 1 Batch-step 800 \t/ 1921 \tloss 7.037395519680447\n",
      "Epoch 1 Batch-step 850 \t/ 1921 \tloss 6.951922713385688\n",
      "Epoch 1 Batch-step 900 \t/ 1921 \tloss 6.785843319363064\n",
      "Epoch 1 Batch-step 950 \t/ 1921 \tloss 6.664664830101861\n",
      "Epoch 1 Batch-step 1000 \t/ 1921 \tloss 6.567775821685791\n",
      "Epoch 1 Batch-step 1050 \t/ 1921 \tloss 6.741851213243272\n",
      "Epoch 1 Batch-step 1100 \t/ 1921 \tloss 6.47295470767551\n",
      "Epoch 1 Batch-step 1150 \t/ 1921 \tloss 6.41072743733724\n",
      "Epoch 1 Batch-step 1200 \t/ 1921 \tloss 6.503171316782633\n",
      "Epoch 1 Batch-step 1250 \t/ 1921 \tloss 6.196225526597765\n",
      "Epoch 1 Batch-step 1300 \t/ 1921 \tloss 6.081576124827067\n",
      "Epoch 1 Batch-step 1350 \t/ 1921 \tloss 6.005834272172716\n",
      "Epoch 1 Batch-step 1400 \t/ 1921 \tloss 6.034924284617106\n",
      "Epoch 1 Batch-step 1450 \t/ 1921 \tloss 5.886436250474718\n",
      "Epoch 1 Batch-step 1500 \t/ 1921 \tloss 5.748745812310113\n",
      "Epoch 1 Batch-step 1550 \t/ 1921 \tloss 5.766090085771348\n",
      "Epoch 1 Batch-step 1600 \t/ 1921 \tloss 5.789394124348958\n",
      "Epoch 1 Batch-step 1650 \t/ 1921 \tloss 5.543479050530328\n",
      "Epoch 1 Batch-step 1700 \t/ 1921 \tloss 5.742928791046142\n",
      "Epoch 1 Batch-step 1750 \t/ 1921 \tloss 5.549667660395304\n",
      "Epoch 1 Batch-step 1800 \t/ 1921 \tloss 5.542486180199517\n",
      "Epoch 1 Batch-step 1850 \t/ 1921 \tloss 5.482247426774767\n",
      "Epoch 1 Batch-step 1900 \t/ 1921 \tloss 5.504645188649495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/14 [20:27<4:26:02, 1227.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Dev F1: 0.5529584567066161 EM: 0.4353378442771371 \n",
      "\n",
      "Epoch 2 Batch-step 0 \t/ 1921 \tloss 0.11871244642469618\n",
      "Epoch 2 Batch-step 50 \t/ 1921 \tloss 5.272496361202664\n",
      "Epoch 2 Batch-step 100 \t/ 1921 \tloss 5.3293135748969185\n",
      "Epoch 2 Batch-step 150 \t/ 1921 \tloss 5.432217968834771\n",
      "Epoch 2 Batch-step 200 \t/ 1921 \tloss 5.269993766148885\n",
      "Epoch 2 Batch-step 250 \t/ 1921 \tloss 5.323973359002007\n",
      "Epoch 2 Batch-step 300 \t/ 1921 \tloss 5.27479461034139\n",
      "Epoch 2 Batch-step 350 \t/ 1921 \tloss 5.139448923534817\n",
      "Epoch 2 Batch-step 400 \t/ 1921 \tloss 5.290340810351902\n",
      "Epoch 2 Batch-step 450 \t/ 1921 \tloss 5.089646848042806\n",
      "Epoch 2 Batch-step 500 \t/ 1921 \tloss 5.148573674096001\n",
      "Epoch 2 Batch-step 550 \t/ 1921 \tloss 5.06991785897149\n",
      "Epoch 2 Batch-step 600 \t/ 1921 \tloss 4.962438011169434\n",
      "Epoch 2 Batch-step 650 \t/ 1921 \tloss 5.17413428624471\n",
      "Epoch 2 Batch-step 700 \t/ 1921 \tloss 4.866912937164306\n",
      "Epoch 2 Batch-step 750 \t/ 1921 \tloss 5.028095287746853\n",
      "Epoch 2 Batch-step 800 \t/ 1921 \tloss 4.832735045750936\n",
      "Epoch 2 Batch-step 850 \t/ 1921 \tloss 4.944064415825737\n",
      "Epoch 2 Batch-step 900 \t/ 1921 \tloss 4.914213471942478\n",
      "Epoch 2 Batch-step 950 \t/ 1921 \tloss 4.905371703041924\n",
      "Epoch 2 Batch-step 1000 \t/ 1921 \tloss 4.991663217544556\n",
      "Epoch 2 Batch-step 1050 \t/ 1921 \tloss 4.839294830958049\n",
      "Epoch 2 Batch-step 1100 \t/ 1921 \tloss 4.789917500813802\n",
      "Epoch 2 Batch-step 1150 \t/ 1921 \tloss 4.780280849668714\n",
      "Epoch 2 Batch-step 1200 \t/ 1921 \tloss 4.837155495749579\n",
      "Epoch 2 Batch-step 1250 \t/ 1921 \tloss 4.671187840567695\n",
      "Epoch 2 Batch-step 1300 \t/ 1921 \tloss 4.822016885545518\n",
      "Epoch 2 Batch-step 1350 \t/ 1921 \tloss 4.73414438035753\n",
      "Epoch 2 Batch-step 1400 \t/ 1921 \tloss 4.866690921783447\n",
      "Epoch 2 Batch-step 1450 \t/ 1921 \tloss 4.644398074679905\n",
      "Epoch 2 Batch-step 1500 \t/ 1921 \tloss 4.66827834977044\n",
      "Epoch 2 Batch-step 1550 \t/ 1921 \tloss 4.753539790047539\n",
      "Epoch 2 Batch-step 1600 \t/ 1921 \tloss 4.628608920839098\n",
      "Epoch 2 Batch-step 1650 \t/ 1921 \tloss 4.773508866628011\n",
      "Epoch 2 Batch-step 1700 \t/ 1921 \tloss 4.629316298166911\n",
      "Epoch 2 Batch-step 1750 \t/ 1921 \tloss 4.5945188999176025\n",
      "Epoch 2 Batch-step 1800 \t/ 1921 \tloss 4.511521212259928\n",
      "Epoch 2 Batch-step 1850 \t/ 1921 \tloss 4.595326693852742\n",
      "Epoch 2 Batch-step 1900 \t/ 1921 \tloss 4.445661211013794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 2/14 [40:58<4:05:49, 1229.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Dev F1: 0.6392619177165166 EM: 0.5277804250452682 \n",
      "\n",
      "Epoch 3 Batch-step 0 \t/ 1921 \tloss 0.10859688652886285\n",
      "Epoch 3 Batch-step 50 \t/ 1921 \tloss 4.53066594335768\n",
      "Epoch 3 Batch-step 100 \t/ 1921 \tloss 4.46955935160319\n",
      "Epoch 3 Batch-step 150 \t/ 1921 \tloss 4.290693034066094\n",
      "Epoch 3 Batch-step 200 \t/ 1921 \tloss 4.266759220759074\n",
      "Epoch 3 Batch-step 250 \t/ 1921 \tloss 4.2634041044447155\n",
      "Epoch 3 Batch-step 300 \t/ 1921 \tloss 4.30872418085734\n",
      "Epoch 3 Batch-step 350 \t/ 1921 \tloss 4.30916232003106\n",
      "Epoch 3 Batch-step 400 \t/ 1921 \tloss 4.395767174826728\n",
      "Epoch 3 Batch-step 450 \t/ 1921 \tloss 4.3163291719224715\n",
      "Epoch 3 Batch-step 500 \t/ 1921 \tloss 4.431751033994886\n",
      "Epoch 3 Batch-step 550 \t/ 1921 \tloss 4.43188067012363\n",
      "Epoch 3 Batch-step 600 \t/ 1921 \tloss 4.285953844918145\n",
      "Epoch 3 Batch-step 650 \t/ 1921 \tloss 4.309066147274441\n",
      "Epoch 3 Batch-step 700 \t/ 1921 \tloss 4.274071386125352\n",
      "Epoch 3 Batch-step 750 \t/ 1921 \tloss 4.408430512746175\n",
      "Epoch 3 Batch-step 800 \t/ 1921 \tloss 4.261750671598646\n",
      "Epoch 3 Batch-step 850 \t/ 1921 \tloss 4.028994793362088\n",
      "Epoch 3 Batch-step 900 \t/ 1921 \tloss 4.253281826443143\n",
      "Epoch 3 Batch-step 950 \t/ 1921 \tloss 4.183720790015327\n",
      "Epoch 3 Batch-step 1000 \t/ 1921 \tloss 4.12222482363383\n",
      "Epoch 3 Batch-step 1050 \t/ 1921 \tloss 4.212146695454916\n",
      "Epoch 3 Batch-step 1100 \t/ 1921 \tloss 4.039780277676052\n",
      "Epoch 3 Batch-step 1150 \t/ 1921 \tloss 4.156146743562487\n",
      "Epoch 3 Batch-step 1200 \t/ 1921 \tloss 4.163514269722833\n",
      "Epoch 3 Batch-step 1250 \t/ 1921 \tloss 4.2783492724100745\n",
      "Epoch 3 Batch-step 1300 \t/ 1921 \tloss 4.153619734446208\n",
      "Epoch 3 Batch-step 1350 \t/ 1921 \tloss 4.127852503458659\n",
      "Epoch 3 Batch-step 1400 \t/ 1921 \tloss 4.150835609436035\n",
      "Epoch 3 Batch-step 1450 \t/ 1921 \tloss 4.005800686942207\n",
      "Epoch 3 Batch-step 1500 \t/ 1921 \tloss 4.10395884513855\n",
      "Epoch 3 Batch-step 1550 \t/ 1921 \tloss 4.1273802174462215\n",
      "Epoch 3 Batch-step 1600 \t/ 1921 \tloss 3.9828233665890163\n",
      "Epoch 3 Batch-step 1650 \t/ 1921 \tloss 3.9487047725253634\n",
      "Epoch 3 Batch-step 1700 \t/ 1921 \tloss 4.179757865269979\n",
      "Epoch 3 Batch-step 1750 \t/ 1921 \tloss 3.9574462678697375\n",
      "Epoch 3 Batch-step 1800 \t/ 1921 \tloss 3.9164472421010337\n",
      "Epoch 3 Batch-step 1850 \t/ 1921 \tloss 4.006263224283854\n",
      "Epoch 3 Batch-step 1900 \t/ 1921 \tloss 4.019938029183281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 3/14 [1:01:30<3:45:31, 1230.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Dev F1: 0.6930568973345579 EM: 0.5811493376536738 \n",
      "\n",
      "Epoch 4 Batch-step 0 \t/ 1921 \tloss 0.07078394360012479\n",
      "Epoch 4 Batch-step 50 \t/ 1921 \tloss 3.998022821214464\n",
      "Epoch 4 Batch-step 100 \t/ 1921 \tloss 3.7304390059577095\n",
      "Epoch 4 Batch-step 150 \t/ 1921 \tloss 3.7769895023769804\n",
      "Epoch 4 Batch-step 200 \t/ 1921 \tloss 3.794175211588542\n",
      "Epoch 4 Batch-step 250 \t/ 1921 \tloss 3.9088405821058485\n",
      "Epoch 4 Batch-step 300 \t/ 1921 \tloss 3.855577802658081\n",
      "Epoch 4 Batch-step 350 \t/ 1921 \tloss 3.8561819553375245\n",
      "Epoch 4 Batch-step 400 \t/ 1921 \tloss 3.7040399180518255\n",
      "Epoch 4 Batch-step 450 \t/ 1921 \tloss 3.8292695787217883\n",
      "Epoch 4 Batch-step 500 \t/ 1921 \tloss 3.7311985280778672\n",
      "Epoch 4 Batch-step 550 \t/ 1921 \tloss 3.794071658452352\n",
      "Epoch 4 Batch-step 600 \t/ 1921 \tloss 3.775224701563517\n",
      "Epoch 4 Batch-step 650 \t/ 1921 \tloss 3.7721218321058485\n",
      "Epoch 4 Batch-step 700 \t/ 1921 \tloss 3.92039459016588\n",
      "Epoch 4 Batch-step 750 \t/ 1921 \tloss 3.7338969495561387\n",
      "Epoch 4 Batch-step 800 \t/ 1921 \tloss 3.6337882889641655\n",
      "Epoch 4 Batch-step 850 \t/ 1921 \tloss 3.785793277952406\n",
      "Epoch 4 Batch-step 900 \t/ 1921 \tloss 3.7417528523339167\n",
      "Epoch 4 Batch-step 950 \t/ 1921 \tloss 3.8075922860039606\n",
      "Epoch 4 Batch-step 1000 \t/ 1921 \tloss 3.633043644163344\n",
      "Epoch 4 Batch-step 1050 \t/ 1921 \tloss 3.7582009527418347\n",
      "Epoch 4 Batch-step 1100 \t/ 1921 \tloss 3.7188954406314427\n",
      "Epoch 4 Batch-step 1150 \t/ 1921 \tloss 3.711779652701484\n",
      "Epoch 4 Batch-step 1200 \t/ 1921 \tloss 3.79184939066569\n",
      "Epoch 4 Batch-step 1250 \t/ 1921 \tloss 3.735307741165161\n",
      "Epoch 4 Batch-step 1300 \t/ 1921 \tloss 3.7447128348880345\n",
      "Epoch 4 Batch-step 1350 \t/ 1921 \tloss 3.7618063396877712\n",
      "Epoch 4 Batch-step 1400 \t/ 1921 \tloss 3.6278107908036974\n",
      "Epoch 4 Batch-step 1450 \t/ 1921 \tloss 3.772173802057902\n",
      "Epoch 4 Batch-step 1500 \t/ 1921 \tloss 3.6822104771931965\n",
      "Epoch 4 Batch-step 1550 \t/ 1921 \tloss 3.765927871068319\n",
      "Epoch 4 Batch-step 1600 \t/ 1921 \tloss 3.7449826558430988\n",
      "Epoch 4 Batch-step 1650 \t/ 1921 \tloss 3.689780754513211\n",
      "Epoch 4 Batch-step 1700 \t/ 1921 \tloss 3.6966202470991347\n",
      "Epoch 4 Batch-step 1750 \t/ 1921 \tloss 3.64959814813402\n",
      "Epoch 4 Batch-step 1800 \t/ 1921 \tloss 3.6389088047875298\n",
      "Epoch 4 Batch-step 1850 \t/ 1921 \tloss 3.6867931207021076\n",
      "Epoch 4 Batch-step 1900 \t/ 1921 \tloss 3.6411946137746174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 4/14 [1:22:02<3:25:05, 1230.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Dev F1: 0.7176756516865082 EM: 0.611931764033165 \n",
      "\n",
      "Epoch 5 Batch-step 0 \t/ 1921 \tloss 0.07035887506273057\n",
      "Epoch 5 Batch-step 50 \t/ 1921 \tloss 3.3469825108846027\n",
      "Epoch 5 Batch-step 100 \t/ 1921 \tloss 3.4456782976786298\n",
      "Epoch 5 Batch-step 150 \t/ 1921 \tloss 3.5198371516333684\n",
      "Epoch 5 Batch-step 200 \t/ 1921 \tloss 3.5734121799468994\n",
      "Epoch 5 Batch-step 250 \t/ 1921 \tloss 3.4597330464257134\n",
      "Epoch 5 Batch-step 300 \t/ 1921 \tloss 3.5595535861121284\n",
      "Epoch 5 Batch-step 350 \t/ 1921 \tloss 3.600385565227932\n",
      "Epoch 5 Batch-step 400 \t/ 1921 \tloss 3.4370111677381727\n",
      "Epoch 5 Batch-step 450 \t/ 1921 \tloss 3.5875237464904783\n",
      "Epoch 5 Batch-step 500 \t/ 1921 \tloss 3.423928123050266\n",
      "Epoch 5 Batch-step 550 \t/ 1921 \tloss 3.4401857588026257\n",
      "Epoch 5 Batch-step 600 \t/ 1921 \tloss 3.3109937296973335\n",
      "Epoch 5 Batch-step 650 \t/ 1921 \tloss 3.4586577733357746\n",
      "Epoch 5 Batch-step 700 \t/ 1921 \tloss 3.476505735185411\n",
      "Epoch 5 Batch-step 750 \t/ 1921 \tloss 3.4421739101409914\n",
      "Epoch 5 Batch-step 800 \t/ 1921 \tloss 3.5363928106096054\n",
      "Epoch 5 Batch-step 850 \t/ 1921 \tloss 3.5102094544304743\n",
      "Epoch 5 Batch-step 900 \t/ 1921 \tloss 3.512082216474745\n",
      "Epoch 5 Batch-step 950 \t/ 1921 \tloss 3.471454546186659\n",
      "Epoch 5 Batch-step 1000 \t/ 1921 \tloss 3.512634860144721\n",
      "Epoch 5 Batch-step 1050 \t/ 1921 \tloss 3.4429696612887914\n",
      "Epoch 5 Batch-step 1100 \t/ 1921 \tloss 3.3501933813095093\n",
      "Epoch 5 Batch-step 1150 \t/ 1921 \tloss 3.4784795814090304\n",
      "Epoch 5 Batch-step 1200 \t/ 1921 \tloss 3.394875611199273\n",
      "Epoch 5 Batch-step 1250 \t/ 1921 \tloss 3.5090644121170045\n",
      "Epoch 5 Batch-step 1300 \t/ 1921 \tloss 3.340527968936496\n",
      "Epoch 5 Batch-step 1350 \t/ 1921 \tloss 3.4694004641638863\n",
      "Epoch 5 Batch-step 1400 \t/ 1921 \tloss 3.3937042660183376\n",
      "Epoch 5 Batch-step 1450 \t/ 1921 \tloss 3.39677488538954\n",
      "Epoch 5 Batch-step 1500 \t/ 1921 \tloss 3.330374198489719\n",
      "Epoch 5 Batch-step 1550 \t/ 1921 \tloss 3.2304306930965847\n",
      "Epoch 5 Batch-step 1600 \t/ 1921 \tloss 3.3107113308376737\n",
      "Epoch 5 Batch-step 1650 \t/ 1921 \tloss 3.3607279936472576\n",
      "Epoch 5 Batch-step 1700 \t/ 1921 \tloss 3.5203368398878307\n",
      "Epoch 5 Batch-step 1750 \t/ 1921 \tloss 3.36027479701572\n",
      "Epoch 5 Batch-step 1800 \t/ 1921 \tloss 3.3845532894134522\n",
      "Epoch 5 Batch-step 1850 \t/ 1921 \tloss 3.3421584182315405\n",
      "Epoch 5 Batch-step 1900 \t/ 1921 \tloss 3.333159489101834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 5/14 [1:42:30<3:04:30, 1230.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Dev F1: 0.7422138626172001 EM: 0.6417611741160774 \n",
      "\n",
      "Epoch 6 Batch-step 0 \t/ 1921 \tloss 0.0765817748175727\n",
      "Epoch 6 Batch-step 50 \t/ 1921 \tloss 3.2764962832132976\n",
      "Epoch 6 Batch-step 100 \t/ 1921 \tloss 3.1655679490831163\n",
      "Epoch 6 Batch-step 150 \t/ 1921 \tloss 3.1126551893022327\n",
      "Epoch 6 Batch-step 200 \t/ 1921 \tloss 3.221733988655938\n",
      "Epoch 6 Batch-step 250 \t/ 1921 \tloss 3.263576300938924\n",
      "Epoch 6 Batch-step 300 \t/ 1921 \tloss 3.1977649052937824\n",
      "Epoch 6 Batch-step 350 \t/ 1921 \tloss 3.266881407631768\n",
      "Epoch 6 Batch-step 400 \t/ 1921 \tloss 3.259074526362949\n",
      "Epoch 6 Batch-step 450 \t/ 1921 \tloss 3.2506847434573705\n",
      "Epoch 6 Batch-step 500 \t/ 1921 \tloss 3.17359540992313\n",
      "Epoch 6 Batch-step 550 \t/ 1921 \tloss 3.0828008307351005\n",
      "Epoch 6 Batch-step 600 \t/ 1921 \tloss 3.2552939785851374\n",
      "Epoch 6 Batch-step 650 \t/ 1921 \tloss 3.1857826656765407\n",
      "Epoch 6 Batch-step 700 \t/ 1921 \tloss 3.2986079639858668\n",
      "Epoch 6 Batch-step 750 \t/ 1921 \tloss 3.194117122226291\n",
      "Epoch 6 Batch-step 800 \t/ 1921 \tloss 3.3238950146569146\n",
      "Epoch 6 Batch-step 850 \t/ 1921 \tloss 3.207103061676025\n",
      "Epoch 6 Batch-step 900 \t/ 1921 \tloss 3.4415832095675998\n",
      "Epoch 6 Batch-step 950 \t/ 1921 \tloss 3.0859252982669405\n",
      "Epoch 6 Batch-step 1000 \t/ 1921 \tloss 3.1726094669765894\n",
      "Epoch 6 Batch-step 1050 \t/ 1921 \tloss 3.1401881641811795\n",
      "Epoch 6 Batch-step 1100 \t/ 1921 \tloss 3.327910563680861\n",
      "Epoch 6 Batch-step 1150 \t/ 1921 \tloss 3.2629785537719727\n",
      "Epoch 6 Batch-step 1200 \t/ 1921 \tloss 3.2682970152960884\n",
      "Epoch 6 Batch-step 1250 \t/ 1921 \tloss 3.1186686568790014\n",
      "Epoch 6 Batch-step 1300 \t/ 1921 \tloss 3.2196298705206976\n",
      "Epoch 6 Batch-step 1350 \t/ 1921 \tloss 3.273950966199239\n",
      "Epoch 6 Batch-step 1400 \t/ 1921 \tloss 3.303824636671278\n",
      "Epoch 6 Batch-step 1450 \t/ 1921 \tloss 3.2715513017442492\n",
      "Epoch 6 Batch-step 1500 \t/ 1921 \tloss 3.1591981728871663\n",
      "Epoch 6 Batch-step 1700 \t/ 1921 \tloss 3.2293146557278103\n",
      "Epoch 6 Batch-step 1750 \t/ 1921 \tloss 3.3283989561928644\n",
      "Epoch 6 Batch-step 1800 \t/ 1921 \tloss 3.1976542419857448\n",
      "Epoch 6 Batch-step 1850 \t/ 1921 \tloss 3.29577898979187\n",
      "Epoch 6 Batch-step 1900 \t/ 1921 \tloss 3.0881780783335366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 6/14 [2:03:00<2:44:00, 1230.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Dev F1: 0.7495933956845299 EM: 0.6504336224149433 \n",
      "\n",
      "Epoch 7 Batch-step 0 \t/ 1921 \tloss 0.05175872378879123\n",
      "Epoch 7 Batch-step 50 \t/ 1921 \tloss 3.238072633743286\n",
      "Epoch 7 Batch-step 100 \t/ 1921 \tloss 3.1191572666168215\n",
      "Epoch 7 Batch-step 150 \t/ 1921 \tloss 2.9737478256225587\n",
      "Epoch 7 Batch-step 200 \t/ 1921 \tloss 3.084012677934435\n",
      "Epoch 7 Batch-step 250 \t/ 1921 \tloss 3.026157644059923\n",
      "Epoch 7 Batch-step 300 \t/ 1921 \tloss 3.0491312821706136\n",
      "Epoch 7 Batch-step 350 \t/ 1921 \tloss 3.0593273745642766\n",
      "Epoch 7 Batch-step 400 \t/ 1921 \tloss 2.904367611143324\n",
      "Epoch 7 Batch-step 450 \t/ 1921 \tloss 3.0725754525926376\n",
      "Epoch 7 Batch-step 500 \t/ 1921 \tloss 2.935639667510986\n",
      "Epoch 7 Batch-step 550 \t/ 1921 \tloss 2.9958475059933134\n",
      "Epoch 7 Batch-step 600 \t/ 1921 \tloss 3.0572371986177234\n",
      "Epoch 7 Batch-step 650 \t/ 1921 \tloss 2.9628492487801448\n",
      "Epoch 7 Batch-step 700 \t/ 1921 \tloss 3.141590640279982\n",
      "Epoch 7 Batch-step 750 \t/ 1921 \tloss 3.1183847109476726\n",
      "Epoch 7 Batch-step 800 \t/ 1921 \tloss 3.05525320370992\n",
      "Epoch 7 Batch-step 850 \t/ 1921 \tloss 3.074932919608222\n",
      "Epoch 7 Batch-step 900 \t/ 1921 \tloss 3.116856892903646\n",
      "Epoch 7 Batch-step 950 \t/ 1921 \tloss 2.985371478398641\n",
      "Epoch 7 Batch-step 1000 \t/ 1921 \tloss 3.0446470843421087\n",
      "Epoch 7 Batch-step 1050 \t/ 1921 \tloss 3.0668870051701864\n",
      "Epoch 7 Batch-step 1100 \t/ 1921 \tloss 3.1757216427061294\n",
      "Epoch 7 Batch-step 1150 \t/ 1921 \tloss 3.052545189857483\n",
      "Epoch 7 Batch-step 1200 \t/ 1921 \tloss 3.0652382956610786\n",
      "Epoch 7 Batch-step 1250 \t/ 1921 \tloss 2.9856925858391654\n",
      "Epoch 7 Batch-step 1300 \t/ 1921 \tloss 3.162144176165263\n",
      "Epoch 7 Batch-step 1350 \t/ 1921 \tloss 2.9288777828216555\n",
      "Epoch 7 Batch-step 1400 \t/ 1921 \tloss 3.0379326396518285\n",
      "Epoch 7 Batch-step 1450 \t/ 1921 \tloss 3.100493982103136\n",
      "Epoch 7 Batch-step 1500 \t/ 1921 \tloss 3.1653373506334095\n",
      "Epoch 7 Batch-step 1550 \t/ 1921 \tloss 3.147078522046407\n",
      "Epoch 7 Batch-step 1600 \t/ 1921 \tloss 2.992116922802395\n",
      "Epoch 7 Batch-step 1650 \t/ 1921 \tloss 3.1324796040852863\n",
      "Epoch 7 Batch-step 1700 \t/ 1921 \tloss 3.0759437561035154\n",
      "Epoch 7 Batch-step 1750 \t/ 1921 \tloss 3.1017733097076414\n",
      "Epoch 7 Batch-step 1800 \t/ 1921 \tloss 3.0264937188890246\n",
      "Epoch 7 Batch-step 1850 \t/ 1921 \tloss 2.9693529182010225\n",
      "Epoch 7 Batch-step 1900 \t/ 1921 \tloss 3.1133172618018254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 7/14 [2:23:32<2:23:32, 1230.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Dev F1: 0.7526001927803987 EM: 0.652053750119127 \n",
      "\n",
      "Epoch 8 Batch-step 0 \t/ 1921 \tloss 0.059061532550387916\n",
      "Epoch 8 Batch-step 50 \t/ 1921 \tloss 2.9383343696594237\n",
      "Epoch 8 Batch-step 100 \t/ 1921 \tloss 2.859563700358073\n",
      "Epoch 8 Batch-step 150 \t/ 1921 \tloss 2.9136952744589912\n",
      "Epoch 8 Batch-step 200 \t/ 1921 \tloss 2.8277170658111572\n",
      "Epoch 8 Batch-step 250 \t/ 1921 \tloss 2.906349547704061\n",
      "Epoch 8 Batch-step 300 \t/ 1921 \tloss 2.9388420767254297\n",
      "Epoch 8 Batch-step 350 \t/ 1921 \tloss 2.891832865609063\n",
      "Epoch 8 Batch-step 400 \t/ 1921 \tloss 2.9477302683724296\n",
      "Epoch 8 Batch-step 450 \t/ 1921 \tloss 2.9244398434956866\n",
      "Epoch 8 Batch-step 500 \t/ 1921 \tloss 2.942857461505466\n",
      "Epoch 8 Batch-step 550 \t/ 1921 \tloss 2.8787273195054794\n",
      "Epoch 8 Batch-step 600 \t/ 1921 \tloss 2.9213425636291506\n",
      "Epoch 8 Batch-step 650 \t/ 1921 \tloss 2.864962159262763\n",
      "Epoch 8 Batch-step 700 \t/ 1921 \tloss 2.932773968908522\n",
      "Epoch 8 Batch-step 750 \t/ 1921 \tloss 2.938024732801649\n",
      "Epoch 8 Batch-step 800 \t/ 1921 \tloss 3.0238775279786854\n",
      "Epoch 8 Batch-step 850 \t/ 1921 \tloss 2.8460669782426624\n",
      "Epoch 8 Batch-step 900 \t/ 1921 \tloss 2.903516247537401\n",
      "Epoch 8 Batch-step 950 \t/ 1921 \tloss 2.84667809009552\n",
      "Epoch 8 Batch-step 1000 \t/ 1921 \tloss 2.834025086296929\n",
      "Epoch 8 Batch-step 1050 \t/ 1921 \tloss 2.9366787036259967\n",
      "Epoch 8 Batch-step 1100 \t/ 1921 \tloss 2.7604568587409126\n",
      "Epoch 8 Batch-step 1150 \t/ 1921 \tloss 2.860300495889452\n",
      "Epoch 8 Batch-step 1200 \t/ 1921 \tloss 2.8984215100606283\n",
      "Epoch 8 Batch-step 1250 \t/ 1921 \tloss 2.9118192169401382\n",
      "Epoch 8 Batch-step 1300 \t/ 1921 \tloss 2.923715935813056\n",
      "Epoch 8 Batch-step 1350 \t/ 1921 \tloss 2.9030279954274496\n",
      "Epoch 8 Batch-step 1400 \t/ 1921 \tloss 2.9349081092410616\n",
      "Epoch 8 Batch-step 1450 \t/ 1921 \tloss 2.9803212536705863\n",
      "Epoch 8 Batch-step 1500 \t/ 1921 \tloss 2.8429277181625365\n",
      "Epoch 8 Batch-step 1550 \t/ 1921 \tloss 2.991049053933885\n",
      "Epoch 8 Batch-step 1600 \t/ 1921 \tloss 3.0850168148676556\n",
      "Epoch 8 Batch-step 1650 \t/ 1921 \tloss 3.0405059337615965\n",
      "Epoch 8 Batch-step 1700 \t/ 1921 \tloss 2.846950782669915\n",
      "Epoch 8 Batch-step 1750 \t/ 1921 \tloss 3.016321733262804\n",
      "Epoch 8 Batch-step 1800 \t/ 1921 \tloss 2.8511173513200547\n",
      "Epoch 8 Batch-step 1850 \t/ 1921 \tloss 2.8664816273583305\n",
      "Epoch 8 Batch-step 1900 \t/ 1921 \tloss 2.885642083485921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 8/14 [2:43:59<2:02:59, 1229.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Dev F1: 0.7625197445282095 EM: 0.6670161059754122 \n",
      "\n",
      "Epoch 9 Batch-step 0 \t/ 1921 \tloss 0.04977151023017035\n",
      "Epoch 9 Batch-step 50 \t/ 1921 \tloss 2.9188483926985\n",
      "Epoch 9 Batch-step 100 \t/ 1921 \tloss 2.772373104095459\n",
      "Epoch 9 Batch-step 150 \t/ 1921 \tloss 2.720060772365994\n",
      "Epoch 9 Batch-step 200 \t/ 1921 \tloss 2.79817451900906\n",
      "Epoch 9 Batch-step 250 \t/ 1921 \tloss 2.8793669197294447\n",
      "Epoch 9 Batch-step 300 \t/ 1921 \tloss 2.7321633630328708\n",
      "Epoch 9 Batch-step 350 \t/ 1921 \tloss 2.64027325047387\n",
      "Epoch 9 Batch-step 400 \t/ 1921 \tloss 2.6926157898373075\n",
      "Epoch 9 Batch-step 450 \t/ 1921 \tloss 2.7059286488427055\n",
      "Epoch 9 Batch-step 500 \t/ 1921 \tloss 2.7426025523079764\n",
      "Epoch 9 Batch-step 550 \t/ 1921 \tloss 2.8465079148610433\n",
      "Epoch 9 Batch-step 600 \t/ 1921 \tloss 2.8121174706353083\n",
      "Epoch 9 Batch-step 650 \t/ 1921 \tloss 2.793894910812378\n",
      "Epoch 9 Batch-step 700 \t/ 1921 \tloss 2.7264138327704535\n",
      "Epoch 9 Batch-step 750 \t/ 1921 \tloss 2.7777887238396537\n",
      "Epoch 9 Batch-step 800 \t/ 1921 \tloss 2.901790486441718\n",
      "Epoch 9 Batch-step 850 \t/ 1921 \tloss 2.870597865846422\n",
      "Epoch 9 Batch-step 900 \t/ 1921 \tloss 2.8972327682707046\n",
      "Epoch 9 Batch-step 950 \t/ 1921 \tloss 2.830038497183058\n",
      "Epoch 9 Batch-step 1000 \t/ 1921 \tloss 2.9060175869199965\n",
      "Epoch 9 Batch-step 1050 \t/ 1921 \tloss 2.8387117942174274\n",
      "Epoch 9 Batch-step 1100 \t/ 1921 \tloss 2.7678987238142225\n",
      "Epoch 9 Batch-step 1150 \t/ 1921 \tloss 2.782988323105706\n",
      "Epoch 9 Batch-step 1200 \t/ 1921 \tloss 2.8228944725460474\n",
      "Epoch 9 Batch-step 1250 \t/ 1921 \tloss 2.779016433821784\n",
      "Epoch 9 Batch-step 1300 \t/ 1921 \tloss 2.8231999370786878\n",
      "Epoch 9 Batch-step 1350 \t/ 1921 \tloss 2.747784982787238\n",
      "Epoch 9 Batch-step 1400 \t/ 1921 \tloss 2.7600993712743125\n",
      "Epoch 9 Batch-step 1450 \t/ 1921 \tloss 2.82373079723782\n",
      "Epoch 9 Batch-step 1500 \t/ 1921 \tloss 2.8083322975370617\n",
      "Epoch 9 Batch-step 1550 \t/ 1921 \tloss 2.7541461997561987\n",
      "Epoch 9 Batch-step 1600 \t/ 1921 \tloss 2.7947426557540895\n",
      "Epoch 9 Batch-step 1650 \t/ 1921 \tloss 2.827572382820977\n",
      "Epoch 9 Batch-step 1700 \t/ 1921 \tloss 2.7775937610202366\n",
      "Epoch 9 Batch-step 1750 \t/ 1921 \tloss 2.827075160874261\n",
      "Epoch 9 Batch-step 1800 \t/ 1921 \tloss 2.822051805920071\n",
      "Epoch 9 Batch-step 1850 \t/ 1921 \tloss 2.8206410964330035\n",
      "Epoch 9 Batch-step 1900 \t/ 1921 \tloss 2.7710598998599583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 9/14 [3:04:32<1:42:31, 1230.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Dev F1: 0.7634442400724794 EM: 0.6649194701229391 \n",
      "\n",
      "Epoch 10 Batch-step 0 \t/ 1921 \tloss 0.05460311041937934\n",
      "Epoch 10 Batch-step 50 \t/ 1921 \tloss 2.5933505323198105\n",
      "Epoch 10 Batch-step 100 \t/ 1921 \tloss 2.7622719022962783\n",
      "Epoch 10 Batch-step 150 \t/ 1921 \tloss 2.6616420216030545\n",
      "Epoch 10 Batch-step 200 \t/ 1921 \tloss 2.610416407055325\n",
      "Epoch 10 Batch-step 250 \t/ 1921 \tloss 2.707951625188192\n",
      "Epoch 10 Batch-step 300 \t/ 1921 \tloss 2.7325144529342653\n",
      "Epoch 10 Batch-step 350 \t/ 1921 \tloss 2.589998330010308\n",
      "Epoch 10 Batch-step 400 \t/ 1921 \tloss 2.6111724456151326\n",
      "Epoch 10 Batch-step 450 \t/ 1921 \tloss 2.6869969844818113\n",
      "Epoch 10 Batch-step 500 \t/ 1921 \tloss 2.710002695189582\n",
      "Epoch 10 Batch-step 550 \t/ 1921 \tloss 2.571962266498142\n",
      "Epoch 10 Batch-step 600 \t/ 1921 \tloss 2.6759458700815837\n",
      "Epoch 10 Batch-step 650 \t/ 1921 \tloss 2.7445180416107178\n",
      "Epoch 10 Batch-step 700 \t/ 1921 \tloss 2.7420851283603245\n",
      "Epoch 10 Batch-step 750 \t/ 1921 \tloss 2.721871503194173\n",
      "Epoch 10 Batch-step 800 \t/ 1921 \tloss 2.4983168681462606\n",
      "Epoch 10 Batch-step 850 \t/ 1921 \tloss 2.7970972114139134\n",
      "Epoch 10 Batch-step 900 \t/ 1921 \tloss 2.7308453612857395\n",
      "Epoch 10 Batch-step 950 \t/ 1921 \tloss 2.6681057929992678\n",
      "Epoch 10 Batch-step 1000 \t/ 1921 \tloss 2.663849581612481\n",
      "Epoch 10 Batch-step 1050 \t/ 1921 \tloss 2.62774608929952\n",
      "Epoch 10 Batch-step 1100 \t/ 1921 \tloss 2.806720471382141\n",
      "Epoch 10 Batch-step 1150 \t/ 1921 \tloss 2.834670978122287\n",
      "Epoch 10 Batch-step 1200 \t/ 1921 \tloss 2.6552746295928955\n",
      "Epoch 10 Batch-step 1250 \t/ 1921 \tloss 2.74649854765998\n",
      "Epoch 10 Batch-step 1300 \t/ 1921 \tloss 2.7312999116049874\n",
      "Epoch 10 Batch-step 1350 \t/ 1921 \tloss 2.7600041230519614\n",
      "Epoch 10 Batch-step 1400 \t/ 1921 \tloss 2.7598369465933907\n",
      "Epoch 10 Batch-step 1450 \t/ 1921 \tloss 2.731610560417175\n",
      "Epoch 10 Batch-step 1500 \t/ 1921 \tloss 2.820635551876492\n",
      "Epoch 10 Batch-step 1550 \t/ 1921 \tloss 2.750427754720052\n",
      "Epoch 10 Batch-step 1600 \t/ 1921 \tloss 2.8202213181389704\n",
      "Epoch 10 Batch-step 1650 \t/ 1921 \tloss 2.642289588186476\n",
      "Epoch 10 Batch-step 1700 \t/ 1921 \tloss 2.6895867188771567\n",
      "Epoch 10 Batch-step 1750 \t/ 1921 \tloss 2.7300715605417887\n",
      "Epoch 10 Batch-step 1800 \t/ 1921 \tloss 2.758025868733724\n",
      "Epoch 10 Batch-step 1850 \t/ 1921 \tloss 2.6362836413913304\n",
      "Epoch 10 Batch-step 1900 \t/ 1921 \tloss 2.6693270206451416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 10/14 [3:25:02<1:22:00, 1230.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Dev F1: 0.7689220727408721 EM: 0.6756885542742781 \n",
      "\n",
      "Epoch 11 Batch-step 0 \t/ 1921 \tloss 0.050475353664822045\n",
      "Epoch 11 Batch-step 50 \t/ 1921 \tloss 2.554087315665351\n",
      "Epoch 11 Batch-step 100 \t/ 1921 \tloss 2.5844193776448567\n",
      "Epoch 11 Batch-step 150 \t/ 1921 \tloss 2.5328456825680203\n",
      "Epoch 11 Batch-step 200 \t/ 1921 \tloss 2.6047031296624077\n",
      "Epoch 11 Batch-step 250 \t/ 1921 \tloss 2.6461966064241196\n",
      "Epoch 11 Batch-step 300 \t/ 1921 \tloss 2.6882030063205296\n",
      "Epoch 11 Batch-step 350 \t/ 1921 \tloss 2.576542851659987\n",
      "Epoch 11 Batch-step 400 \t/ 1921 \tloss 2.668660857942369\n",
      "Epoch 11 Batch-step 450 \t/ 1921 \tloss 2.623298989401923\n",
      "Epoch 11 Batch-step 500 \t/ 1921 \tloss 2.5961091942257353\n",
      "Epoch 11 Batch-step 550 \t/ 1921 \tloss 2.662731628947788\n",
      "Epoch 11 Batch-step 600 \t/ 1921 \tloss 2.564129442638821\n",
      "Epoch 11 Batch-step 650 \t/ 1921 \tloss 2.5919566551844277\n",
      "Epoch 11 Batch-step 700 \t/ 1921 \tloss 2.6249983946482343\n",
      "Epoch 11 Batch-step 750 \t/ 1921 \tloss 2.586639396349589\n",
      "Epoch 11 Batch-step 800 \t/ 1921 \tloss 2.5742302073372736\n",
      "Epoch 11 Batch-step 850 \t/ 1921 \tloss 2.668440302213033\n",
      "Epoch 11 Batch-step 900 \t/ 1921 \tloss 2.5652856190999347\n",
      "Epoch 11 Batch-step 950 \t/ 1921 \tloss 2.623148806889852\n",
      "Epoch 11 Batch-step 1000 \t/ 1921 \tloss 2.6499142434861924\n",
      "Epoch 11 Batch-step 1050 \t/ 1921 \tloss 2.69952974319458\n",
      "Epoch 11 Batch-step 1100 \t/ 1921 \tloss 2.6410866737365724\n",
      "Epoch 11 Batch-step 1150 \t/ 1921 \tloss 2.5168252653545804\n",
      "Epoch 11 Batch-step 1200 \t/ 1921 \tloss 2.6055771191914876\n",
      "Epoch 11 Batch-step 1250 \t/ 1921 \tloss 2.6242268376880222\n",
      "Epoch 11 Batch-step 1300 \t/ 1921 \tloss 2.531562108463711\n",
      "Epoch 11 Batch-step 1350 \t/ 1921 \tloss 2.674129846360948\n",
      "Epoch 11 Batch-step 1400 \t/ 1921 \tloss 2.737750607066684\n",
      "Epoch 11 Batch-step 1450 \t/ 1921 \tloss 2.635341517130534\n",
      "Epoch 11 Batch-step 1500 \t/ 1921 \tloss 2.6948368363910253\n",
      "Epoch 11 Batch-step 1550 \t/ 1921 \tloss 2.5881973028182985\n",
      "Epoch 11 Batch-step 1600 \t/ 1921 \tloss 2.5942025184631348\n",
      "Epoch 11 Batch-step 1650 \t/ 1921 \tloss 2.6445293188095094\n",
      "Epoch 11 Batch-step 1700 \t/ 1921 \tloss 2.6324849790996976\n",
      "Epoch 11 Batch-step 1750 \t/ 1921 \tloss 2.7135981559753417\n",
      "Epoch 11 Batch-step 1800 \t/ 1921 \tloss 2.646768765979343\n",
      "Epoch 11 Batch-step 1850 \t/ 1921 \tloss 2.558123779296875\n",
      "Epoch 11 Batch-step 1900 \t/ 1921 \tloss 2.618588678042094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 11/14 [3:45:29<1:01:29, 1229.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Dev F1: 0.7699888692038909 EM: 0.6732107119031735 \n",
      "\n",
      "Epoch 12 Batch-step 0 \t/ 1921 \tloss 0.046400361590915255\n",
      "Epoch 12 Batch-step 50 \t/ 1921 \tloss 2.588837676578098\n",
      "Epoch 12 Batch-step 100 \t/ 1921 \tloss 2.5123135381274753\n",
      "Epoch 12 Batch-step 150 \t/ 1921 \tloss 2.5514962885114882\n",
      "Epoch 12 Batch-step 200 \t/ 1921 \tloss 2.5665765629874335\n",
      "Epoch 12 Batch-step 250 \t/ 1921 \tloss 2.451261456807454\n",
      "Epoch 12 Batch-step 300 \t/ 1921 \tloss 2.62489693959554\n",
      "Epoch 12 Batch-step 350 \t/ 1921 \tloss 2.517939069535997\n",
      "Epoch 12 Batch-step 400 \t/ 1921 \tloss 2.461516931321886\n",
      "Epoch 12 Batch-step 450 \t/ 1921 \tloss 2.4256924947102863\n",
      "Epoch 12 Batch-step 500 \t/ 1921 \tloss 2.5098141696718006\n",
      "Epoch 12 Batch-step 550 \t/ 1921 \tloss 2.59414930873447\n",
      "Epoch 12 Batch-step 600 \t/ 1921 \tloss 2.548217855559455\n",
      "Epoch 12 Batch-step 650 \t/ 1921 \tloss 2.4568927155600653\n",
      "Epoch 12 Batch-step 700 \t/ 1921 \tloss 2.598736378881666\n",
      "Epoch 12 Batch-step 750 \t/ 1921 \tloss 2.5847307390636867\n",
      "Epoch 12 Batch-step 800 \t/ 1921 \tloss 2.5292661322487726\n",
      "Epoch 12 Batch-step 850 \t/ 1921 \tloss 2.534456812010871\n",
      "Epoch 12 Batch-step 900 \t/ 1921 \tloss 2.59248767958747\n",
      "Epoch 12 Batch-step 950 \t/ 1921 \tloss 2.495826254950629\n",
      "Epoch 12 Batch-step 1000 \t/ 1921 \tloss 2.3721485137939453\n",
      "Epoch 12 Batch-step 1050 \t/ 1921 \tloss 2.4918136570188736\n",
      "Epoch 12 Batch-step 1100 \t/ 1921 \tloss 2.4757905695173474\n",
      "Epoch 12 Batch-step 1150 \t/ 1921 \tloss 2.518921966022915\n",
      "Epoch 12 Batch-step 1200 \t/ 1921 \tloss 2.415619071324666\n",
      "Epoch 12 Batch-step 1250 \t/ 1921 \tloss 2.581351889504327\n",
      "Epoch 12 Batch-step 1300 \t/ 1921 \tloss 2.6068614297442965\n",
      "Epoch 12 Batch-step 1350 \t/ 1921 \tloss 2.477141343222724\n",
      "Epoch 12 Batch-step 1400 \t/ 1921 \tloss 2.5674779256184896\n",
      "Epoch 12 Batch-step 1450 \t/ 1921 \tloss 2.545941956837972\n",
      "Epoch 12 Batch-step 1500 \t/ 1921 \tloss 2.553098718325297\n",
      "Epoch 12 Batch-step 1550 \t/ 1921 \tloss 2.580319200621711\n",
      "Epoch 12 Batch-step 1600 \t/ 1921 \tloss 2.528247258398268\n",
      "Epoch 12 Batch-step 1650 \t/ 1921 \tloss 2.695759958691067\n",
      "Epoch 12 Batch-step 1700 \t/ 1921 \tloss 2.677567113770379\n",
      "Epoch 12 Batch-step 1750 \t/ 1921 \tloss 2.498662389649285\n",
      "Epoch 12 Batch-step 1800 \t/ 1921 \tloss 2.5522644917170205\n",
      "Epoch 12 Batch-step 1850 \t/ 1921 \tloss 2.67821229034\n",
      "Epoch 12 Batch-step 1900 \t/ 1921 \tloss 2.55260947810279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 12/14 [4:06:03<41:00, 1230.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Dev F1: 0.7694450282721841 EM: 0.6716858858286476 \n",
      "\n",
      "Epoch 13 Batch-step 0 \t/ 1921 \tloss 0.041313036282857256\n",
      "Epoch 13 Batch-step 50 \t/ 1921 \tloss 2.4027711947758994\n",
      "Epoch 13 Batch-step 100 \t/ 1921 \tloss 2.4258758730358547\n",
      "Epoch 13 Batch-step 150 \t/ 1921 \tloss 2.403064643012153\n",
      "Epoch 13 Batch-step 200 \t/ 1921 \tloss 2.4482817782296076\n",
      "Epoch 13 Batch-step 250 \t/ 1921 \tloss 2.3567126830418905\n",
      "Epoch 13 Batch-step 300 \t/ 1921 \tloss 2.5296547492345174\n",
      "Epoch 13 Batch-step 350 \t/ 1921 \tloss 2.413178155157301\n",
      "Epoch 13 Batch-step 400 \t/ 1921 \tloss 2.4176357057359485\n",
      "Epoch 13 Batch-step 450 \t/ 1921 \tloss 2.4021617703967624\n",
      "Epoch 13 Batch-step 500 \t/ 1921 \tloss 2.399919761551751\n",
      "Epoch 13 Batch-step 550 \t/ 1921 \tloss 2.434067251947191\n",
      "Epoch 13 Batch-step 600 \t/ 1921 \tloss 2.3672659158706666\n",
      "Epoch 13 Batch-step 650 \t/ 1921 \tloss 2.4263577699661254\n",
      "Epoch 13 Batch-step 700 \t/ 1921 \tloss 2.5833066039615207\n",
      "Epoch 13 Batch-step 750 \t/ 1921 \tloss 2.4590166833665634\n",
      "Epoch 13 Batch-step 800 \t/ 1921 \tloss 2.446540337138706\n",
      "Epoch 13 Batch-step 850 \t/ 1921 \tloss 2.4622075822618275\n",
      "Epoch 13 Batch-step 900 \t/ 1921 \tloss 2.5155415376027426\n",
      "Epoch 13 Batch-step 950 \t/ 1921 \tloss 2.53457379606035\n",
      "Epoch 13 Batch-step 1000 \t/ 1921 \tloss 2.4709568421045938\n",
      "Epoch 13 Batch-step 1050 \t/ 1921 \tloss 2.5092692534128824\n",
      "Epoch 13 Batch-step 1100 \t/ 1921 \tloss 2.5438057608074613\n",
      "Epoch 13 Batch-step 1150 \t/ 1921 \tloss 2.3830794625812106\n",
      "Epoch 13 Batch-step 1200 \t/ 1921 \tloss 2.5781168010499744\n",
      "Epoch 13 Batch-step 1250 \t/ 1921 \tloss 2.5058191272947523\n",
      "Epoch 13 Batch-step 1300 \t/ 1921 \tloss 2.443927033742269\n",
      "Epoch 13 Batch-step 1350 \t/ 1921 \tloss 2.4827362060546876\n",
      "Epoch 13 Batch-step 1400 \t/ 1921 \tloss 2.3850961208343504\n",
      "Epoch 13 Batch-step 1450 \t/ 1921 \tloss 2.5647063414255777\n",
      "Epoch 13 Batch-step 1500 \t/ 1921 \tloss 2.598147675726149\n",
      "Epoch 13 Batch-step 1550 \t/ 1921 \tloss 2.4590010192659166\n",
      "Epoch 13 Batch-step 1600 \t/ 1921 \tloss 2.4800504949357776\n",
      "Epoch 13 Batch-step 1650 \t/ 1921 \tloss 2.459778814845615\n",
      "Epoch 13 Batch-step 1700 \t/ 1921 \tloss 2.4721698919932047\n",
      "Epoch 13 Batch-step 1750 \t/ 1921 \tloss 2.4378166410658095\n",
      "Epoch 13 Batch-step 1800 \t/ 1921 \tloss 2.5247726519902547\n",
      "Epoch 13 Batch-step 1850 \t/ 1921 \tloss 2.5835013151168824\n",
      "Epoch 13 Batch-step 1900 \t/ 1921 \tloss 2.469671763314141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 13/14 [4:26:33<20:30, 1230.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Dev F1: 0.7756202933940382 EM: 0.6810254455351187 \n",
      "\n",
      "Epoch 14 Batch-step 0 \t/ 1921 \tloss 0.03926796383327908\n",
      "Epoch 14 Batch-step 50 \t/ 1921 \tloss 2.2753374523586696\n",
      "Epoch 14 Batch-step 100 \t/ 1921 \tloss 2.380259484714932\n",
      "Epoch 14 Batch-step 150 \t/ 1921 \tloss 2.3116933425267536\n",
      "Epoch 14 Batch-step 200 \t/ 1921 \tloss 2.3812929842207167\n",
      "Epoch 14 Batch-step 250 \t/ 1921 \tloss 2.270442271232605\n",
      "Epoch 14 Batch-step 300 \t/ 1921 \tloss 2.2740043666627674\n",
      "Epoch 14 Batch-step 350 \t/ 1921 \tloss 2.3492498715718586\n",
      "Epoch 14 Batch-step 400 \t/ 1921 \tloss 2.3948164727952745\n",
      "Epoch 14 Batch-step 450 \t/ 1921 \tloss 2.3710155142678153\n",
      "Epoch 14 Batch-step 500 \t/ 1921 \tloss 2.3640154043833417\n",
      "Epoch 14 Batch-step 550 \t/ 1921 \tloss 2.3679350349638195\n",
      "Epoch 14 Batch-step 600 \t/ 1921 \tloss 2.3455643282996284\n",
      "Epoch 14 Batch-step 650 \t/ 1921 \tloss 2.402607176038954\n",
      "Epoch 14 Batch-step 700 \t/ 1921 \tloss 2.427031347486708\n",
      "Epoch 14 Batch-step 750 \t/ 1921 \tloss 2.3767616934246485\n",
      "Epoch 14 Batch-step 800 \t/ 1921 \tloss 2.385462194018894\n",
      "Epoch 14 Batch-step 850 \t/ 1921 \tloss 2.4521679560343426\n",
      "Epoch 14 Batch-step 900 \t/ 1921 \tloss 2.424877071380615\n",
      "Epoch 14 Batch-step 950 \t/ 1921 \tloss 2.5199718475341797\n",
      "Epoch 14 Batch-step 1000 \t/ 1921 \tloss 2.2884920795758563\n",
      "Epoch 14 Batch-step 1050 \t/ 1921 \tloss 2.394727200931973\n",
      "Epoch 14 Batch-step 1100 \t/ 1921 \tloss 2.3790334357155696\n",
      "Epoch 14 Batch-step 1150 \t/ 1921 \tloss 2.339042862256368\n",
      "Epoch 14 Batch-step 1200 \t/ 1921 \tloss 2.5024638308419123\n",
      "Epoch 14 Batch-step 1250 \t/ 1921 \tloss 2.446344508065118\n",
      "Epoch 14 Batch-step 1300 \t/ 1921 \tloss 2.443120453092787\n",
      "Epoch 14 Batch-step 1350 \t/ 1921 \tloss 2.422513090239631\n",
      "Epoch 14 Batch-step 1400 \t/ 1921 \tloss 2.462916625870599\n",
      "Epoch 14 Batch-step 1450 \t/ 1921 \tloss 2.4996919949849445\n",
      "Epoch 14 Batch-step 1500 \t/ 1921 \tloss 2.458798583348592\n",
      "Epoch 14 Batch-step 1550 \t/ 1921 \tloss 2.551599709192912\n",
      "Epoch 14 Batch-step 1600 \t/ 1921 \tloss 2.527832351790534\n",
      "Epoch 14 Batch-step 1650 \t/ 1921 \tloss 2.3966510984632703\n",
      "Epoch 14 Batch-step 1700 \t/ 1921 \tloss 2.420375410715739\n",
      "Epoch 14 Batch-step 1750 \t/ 1921 \tloss 2.4247730281617907\n",
      "Epoch 14 Batch-step 1800 \t/ 1921 \tloss 2.4995397991604276\n",
      "Epoch 14 Batch-step 1850 \t/ 1921 \tloss 2.4459530035654704\n",
      "Epoch 14 Batch-step 1900 \t/ 1921 \tloss 2.443725307782491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 14/14 [4:47:00<00:00, 1230.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Dev F1: 0.7693307487617544 EM: 0.6759744591632517 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size = 45\n",
    "num_epochs = 14 \n",
    "best_f1 = 0\n",
    "\n",
    "loss_list = []\n",
    "f1_list = []\n",
    "em_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in tqdm(range(0, num_epochs)):\n",
    "    loss_list.append([])\n",
    "    train_loss = 0\n",
    "    for idx, sample in enumerate(train_loader): \n",
    "\n",
    "        reader.train()\n",
    "\n",
    "        con_words = Variable(sample[0].cuda())\n",
    "        con_chars = Variable(sample[1].cuda())\n",
    "        con_feat = Variable(sample[2].cuda())\n",
    "        ques_words = Variable(sample[4].cuda())\n",
    "        ques_chars = Variable(sample[5].cuda())\n",
    "        ques_feat = Variable(sample[6].cuda())\n",
    "        target_start = Variable(sample[8].cuda())\n",
    "        target_end = Variable(sample[9].cuda())\n",
    "        \n",
    "        con_embed_words = func.dropout(emb_layer(con_words), p=0.2)\n",
    "        ques_embed_words = func.dropout(emb_layer(ques_words), p=0.2)\n",
    "        con_embed_chars = func.dropout(emb_layer_char(con_chars), p=0.2)\n",
    "        ques_embed_chars = func.dropout(emb_layer_char(ques_chars), p=0.2)\n",
    "        \n",
    "        pred_start, pred_end = reader(con_embed_words,con_embed_chars,con_feat,sample[3],ques_embed_words,ques_embed_chars,ques_feat,sample[7])\n",
    "        \n",
    "        loss = func.nll_loss(pred_start, target_start) + func.nll_loss(pred_end, target_end)#calculate loss\n",
    "        optimizer.zero_grad() #set gradients to zero for each iteration\n",
    "        loss.backward() #backpropagate\n",
    "        torch.nn.utils.clip_grad_norm(reader.parameters(), max_norm=10)#clip gradients to avoid exploding gradients\n",
    "        optimizer.step() #update parameters\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        if(idx % 50 == 0):\n",
    "            loss_epoch_internal = train_loss/batch_size\n",
    "            loss_list[epoch].append(loss_epoch_internal)\n",
    "            print(\"Epoch\", epoch + 1, \"Batch-step\", idx, \"\\t/\", len(train_loader), \"\\tloss\", loss_epoch_internal)\n",
    "            train_loss=0\n",
    "#             if(idx == 50):\n",
    "#                 break\n",
    "            if(idx % 200 == 0):\n",
    "                with open('graph_elem.pkl', 'wb') as f:\n",
    "                    pickle.dump([loss_list, f1_list, em_list, epoch_list], f)\n",
    "\n",
    "    time_id = str(int(time.time()))\n",
    "    f1, em, results = validate(dev_loader, reader, dev_offsets, dev_texts, dev_answers, official=True)\n",
    "    print(\"Epoch:\", epoch + 1, \"Dev F1:\", f1, \"EM:\", em, \"\\n\")\n",
    "    f1_list.append(f1)\n",
    "    em_list.append(em)\n",
    "    epoch_list.append(epoch)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        model_name_cur = model_name+\"_\"+time_id+\"_epoch_\"+str(epoch+1)+\"_F1_\"+str(round(f1*100,2)).replace('.','_')+\".mdl\"\n",
    "        torch.save(reader.state_dict(), \"data/models/\"+model_name_cur+'.pt')\n",
    "        best_f1 = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = []\n",
    "for l in loss_list:\n",
    "    loss.append(l[1])\n",
    "\n",
    "plt.plot(epoch_list, loss)\n",
    "#plt.title('NLL loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, f1_list, 'b', label='max F1 (' + str(str(format(max(f1_list),'.5f'))+')'))\n",
    "plt.plot(epoch_list, em_list, 'g', label='EM (' + str(str(format(max(em_list),'.5f'))+')'))\n",
    "plt.title('SQuAD F1-EM')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 / EM score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with_topics_best_1525735845_epoch_13_F1_77_56.mdl_predictions.preds'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_cur+'_predictions.preds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on development data\n",
    "reader.load_state_dict(torch.load(\"data/models/\"+model_name_cur+'.pt'))\n",
    "f1, em, results = validate(dev_loader, reader, dev_offsets, dev_texts, dev_answers, official=True)\n",
    "\n",
    "with open(\"data/models/\"+model_name_cur+'_predictions.preds','w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7756202933940382"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810254455351187"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On why questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whys_id = []\n",
    "whys = []\n",
    "for sample in dev:\n",
    "    ques = [ques.lower() for ques in sample['question']]\n",
    "    if 'why' in ques:\n",
    "        whys_id.append(sample['id'])\n",
    "        whys.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'who','when','why',''\n",
    "why_offsets = {k: dev_offsets[k] for k in dev_offsets.keys() if k in whys_id}\n",
    "why_texts = {k: dev_texts[k] for k in dev_texts.keys() if k in whys_id}\n",
    "why_answers = {k: dev_answers[k] for k in dev_answers.keys() if k in whys_id}\n",
    "\n",
    "#whys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "whys_dataset = Transform(whys, word2idx, char2idx,status='test')\n",
    "whys_sampler = torch.utils.data.sampler.SequentialSampler(whys_dataset)\n",
    "whys_loader = torch.utils.data.DataLoader(\n",
    "        whys_dataset, batch_size=32, sampler=whys_sampler, num_workers=5, collate_fn=pad_batch, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.load_state_dict(torch.load(\"data/models/\"+model_name_cur+'.pt'))\n",
    "f1, em, results = validate(whys_loader, reader, why_offsets, why_texts, why_answers, official=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6401967853037452"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3924050632911392"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'57337ddc4776f41900660bbe': 'to clean them of plants and sediments',\n",
       " '573399b54776f41900660e66': 'solid economic growth',\n",
       " '5733a32bd058e614000b5f36': 'their disastrous financial situation',\n",
       " '5733140a4776f419006606e4': 'it has survived many wars, conflicts and invasions throughout its long history',\n",
       " '5733266d4776f41900660715': 'Due to its central location',\n",
       " '57332e48d058e614000b5763': 'Stalin was hostile to the idea of an independent Poland',\n",
       " '5733638fd058e614000b59e9': 'not restored by the communist authorities',\n",
       " '573368e54776f41900660a53': 'to the location of Warsaw within the border region of several big floral regions',\n",
       " '56e0c1617aa994140058e6d6': 'to attend school at the Higher Real Gymnasium',\n",
       " '56e0c2bc231d4119001ac389': 'this contact with nature made him stronger, both physically and mentally',\n",
       " '56dfa7887aa994140058dfaa': 'to hide the fact that he dropped out of school',\n",
       " '56dfaa047aa994140058dfbd': 'not having a residence permit',\n",
       " '56e0cd33231d4119001ac3bf': 'not having a residence permit',\n",
       " '56dfac8e231d4119001abc5c': 'he never studied Greek, a required subject',\n",
       " '56e0d9e0231d4119001ac43d': 'avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes',\n",
       " '56e057e1231d4119001ac046': 'control the market',\n",
       " '56e08b3c231d4119001ac2a4': 'kinds after he had noticed damaged film in his laboratory in previous experiments',\n",
       " '56e10296cd28a01900c67424': 'complete the construction of Wardenclyffe',\n",
       " '56e11a73e3433e1400422bf1': 'squished his toes one hundred times for each foot every night',\n",
       " '56e12005cd28a01900c67619': 'her weight',\n",
       " '56e74e4800c9c71400d76f77': 'enforcing the standards of practice for the teaching profession',\n",
       " '56e769dc00c9c71400d770e8': 'Fears of being labelled a pedophile or hebephile',\n",
       " '56f7fde8a6d7ea1400e17368': 'sell indulgences',\n",
       " '56f826a7a6d7ea1400e17429': 'these words to be unreliable',\n",
       " '56f855caaef2371900625ff5': 'Biblical grounds',\n",
       " '56f86d30a6d7ea1400e17608': 'removing impediments and difficulties so that other people may read it without hindrance.\"',\n",
       " '56f8720eaef2371900626090': 'This behavior started with his learning',\n",
       " '56f87392aef237190062609d': 'the perceived difficulty of its tune',\n",
       " '5705eccb52bb8914006896b9': 'Palm Springs',\n",
       " '571099b2b654c5140001f9b5': 'protest against the occupation of Prussia by Napoleon in 1806-07',\n",
       " '571c4132dd7acb1400e4c0b3': 'priority in the discovery',\n",
       " '571cc8815efbb31900334df0': 'their higher oxygen content',\n",
       " '571ce5055efbb31900334e2a': 'no damage due to the low total pressures',\n",
       " '5725b5a689a1e219009abd28': 'avoid being targeted by the boycott',\n",
       " '5725b76389a1e219009abd4c': \"oil was priced in dollars, oil producers' real income decreased\",\n",
       " '5725b76389a1e219009abd4e': \"oil producers' real income decreased\",\n",
       " '5725bad5271a42140099d0c1': '\"Of course [the price of oil] is going to rise',\n",
       " '5725bcb6271a42140099d0eb': 'American aid to Israel',\n",
       " '5725bcb6271a42140099d0ed': 'embargo',\n",
       " '5725bcb6271a42140099d0ef': 'principal hostile country',\n",
       " '57264cac708984140094c1b4': 'to encourage investment',\n",
       " '57264cac708984140094c1b5': 'creating greater scarcity',\n",
       " '57264d9edd62a815002e8100': 'to coordinate the response to the embargo',\n",
       " '57265bdfdd62a815002e829e': 'increase',\n",
       " '57265e11708984140094c3bd': 'pushing prices down, shrinking',\n",
       " '5726c3da708984140094d0da': 'because Dutch law said only people established in the Netherlands could give legal advice',\n",
       " '572a0bfaaf94a219006aa77d': 'data sampling is biased away from the center of the Amazon basin',\n",
       " '572a07c11d046914007796d9': 'protect their tribal lands from commercial interests',\n",
       " '57264a0ef1498d1400e8db42': 'enough to withstand waves and swirling sediment particles',\n",
       " '57268a37f1498d1400e8e33c': 'dead ends\" in marine food chains',\n",
       " '57268da7f1498d1400e8e39c': 'Because of their soft, gelatinous bodies',\n",
       " '5725cfd0271a42140099d226': 'flooding',\n",
       " '5725fb8138643c19005acf3f': 'To avoid interference',\n",
       " '5726398589a1e219009ac58a': 'Routing a packet requires the node to look up the connection id in a table',\n",
       " '5726414e271a42140099d7e5': 'Michigan Educational Research Information Triad',\n",
       " '5726516a708984140094c224': 'lack of reliable statistics',\n",
       " '57265285708984140094c25b': 'the symptoms of the Black Death are not unique',\n",
       " '57265285708984140094c25d': 'the symptoms of the Black Death are not unique',\n",
       " '572663a9f1498d1400e8ddf2': 'another reason for expanding the fourth scale',\n",
       " '57268220f1498d1400e8e218': 'provide better absolute bounds on the timing and rates of deposition',\n",
       " '57267076708984140094c604': 'to spearhead the regeneration of the North-East',\n",
       " '57269c26f1498d1400e8e4cd': 'the result of its colouring',\n",
       " '5726e37ef1498d1400e8eeda': 'everyday clothing from previous eras has not generally survived',\n",
       " '57269344f1498d1400e8e441': 'increase their independence and strengthen legislation to limit foreign ownership of broadcasting properties',\n",
       " '5726caaaf1498d1400e8eb5f': 'respective issues with technical problems and flight delays',\n",
       " '5726caaaf1498d1400e8eb60': 'respective issues with technical problems and flight delays',\n",
       " '57276f82dd62a815002e9cd2': 'immensely popular\"',\n",
       " '5726b1d95951b619008f7ace': 'he did not want disloyal men in his army',\n",
       " '5727311d5951b619008f86af': 'I would be doing a service if I killed my father when he is hunting',\n",
       " '57273b1a5951b619008f870a': 'nomads',\n",
       " '57274126dd62a815002e9a27': 'avoid trivialization',\n",
       " '5726ed6cf1498d1400e8f00c': 'to avoid the \"inconvenience\"',\n",
       " '5726f7715951b619008f838d': 'the high risk of a conflict of interest and/or the avoidance of absolute powers',\n",
       " '5726f7715951b619008f838e': 'because he or she can then sell more medications to the patient',\n",
       " '5728d4c03acd2414000dffa1': 'they deem to be unfair laws',\n",
       " '572818f54b864d190016446d': 'covert lawbreaking',\n",
       " '5728df634b864d1900164fe6': 'appeal to constitutional defects',\n",
       " '5728e07e3acd2414000e00ea': 'they are judged \"wrong\" by an individual conscience',\n",
       " '5728e8212ca10214002daa70': 'fear of seeming rude',\n",
       " '5728eb1a3acd2414000e01c6': 'does not infringe the rights of others',\n",
       " '5728ebcb3acd2414000e01db': '\"Guilt implies wrong-doing',\n",
       " '5728ed94ff5b5019007da97c': 'a way of reminding their countrymen of injustice',\n",
       " '5728ed94ff5b5019007da97f': 'the spirit of protest should be maintained all the way',\n",
       " '5728f50baf94a219006a9e56': 'explaining their actions, in allocution',\n",
       " '5728f50baf94a219006a9e57': 'a speech explaining their actions, in allocution',\n",
       " '5728f50baf94a219006a9e58': 'even a likelihood of repeating her illegal actions',\n",
       " '5728fb6a1d04691400778ef6': 'neither conscientious nor of social benefit',\n",
       " '5728fb6a1d04691400778ef8': 'avoiding attribution',\n",
       " '5728fc9e1d04691400778f15': '\"prosecutors have reasoned (correctly) that if they arrest fully informed jury leafleters',\n",
       " '5727d1c93acd2414000ded41': 'the program was believed to disadvantage low-income and under-represented minority applicants applying to selective universities',\n",
       " '572810ec2ca10214002d9d08': 'cattle were brought across the river there',\n",
       " '5727e6cbff5b5019007d97f2': 'criticism about debt to be a \"silly argument\"',\n",
       " '5729d36b1d0469140077960b': 'a greater tendency to take on debts',\n",
       " '5729d44b1d04691400779613': 'Inherited wealth',\n",
       " '5729d609af94a219006aa662': 'competitive pressure to reduce costs and maximize profits',\n",
       " '5729da0faf94a219006aa677': 'the expendable nature of the worker in relation to his or her particular job',\n",
       " '5729f60caf94a219006aa6f0': 'human capital is neglected for high-end consumption',\n",
       " '572a0ecb1d0469140077971a': 'it is a waste of resources',\n",
       " '572a1c943f37b319004786e3': 'pay higher market rate for housing and left lower income families without rental units',\n",
       " '572a2224af94a219006aa826': 'fear of their lives',\n",
       " '5727f2583acd2414000df08b': 'to a malfunction in the chameleon circuit',\n",
       " '5727f44c2ca10214002d9a34': 'as the programme was not permitted to contain any \"bug-eyed monsters\"',\n",
       " '57282f204b864d190016468e': 'the death of Elisabeth Sladen',\n",
       " '57284456ff5b5019007da05f': 'underwent many changes during his 24-year tenure',\n",
       " '57284618ff5b5019007da0a9': 'increasing crime and poverty',\n",
       " '572864542ca10214002da2e2': 'withdraw',\n",
       " '572872822ca10214002da375': 'all influence on other Mongol lands across Asia',\n",
       " '57287338ff5b5019007da234': 'fear of betrayal',\n",
       " '572881022ca10214002da41a': 'it ensured a high income and medical ethics were compatible with Confucian virtues',\n",
       " '572885023acd2414000dfa85': 'southern China withstood',\n",
       " '572885023acd2414000dfa86': 'southern China withstood',\n",
       " '5728855d3acd2414000dfa90': 'violently resisting',\n",
       " '572914441d04691400779026': 'democratic freedoms',\n",
       " '572914441d04691400779028': 'democratic freedoms',\n",
       " '572915e43f37b31900478006': 'Because the operations of the armed forces have been traditionally cloaked',\n",
       " '57291a7b1d04691400779040': 'drought resistant',\n",
       " '57292046af94a219006aa0be': 'climate will be a central issue',\n",
       " '572926653f37b3190047807b': 'the new structure would enable school drop-outs',\n",
       " '572927d06aef051400154ae1': 'it is open to all irrespective of age, literacy level',\n",
       " '572928bf6aef051400154af3': 'the defection of a number of Kenyan athletes to represent other countries',\n",
       " '572928bf6aef051400154af4': 'economic or financial factors',\n",
       " '57293f8a6aef051400154be1': 'requested by governments',\n",
       " '57295b5b1d04691400779316': \"accessory pigments that override the chlorophylls' green colors\",\n",
       " '57296de03f37b3190047839d': \"to increase the chloroplast's surface area for cross-membrane transport\",\n",
       " '57296eb01d04691400779439': 'translation initiation in most chloroplasts and prokaryotes',\n",
       " '57297103af94a219006aa425': 'They help transfer and dissipate excess energy',\n",
       " '5729735c3f37b319004783fe': 'can orient themselves to best suit the available light',\n",
       " '572976791d046914007794b1': 'Rubisco cannot distinguish between oxygen and carbon dioxide',\n",
       " '57297991af94a219006aa4b7': 'caught attention by developers of genetically modified crops',\n",
       " '57296d571d04691400779417': 'because one can include arbitrarily many instances of 1 in any factorization',\n",
       " '572fe393947a6a140053cdbe': \"The river length is significantly shortened from the river's natural course\",\n",
       " '572f5875947a6a140053c89c': 'strong sedimentation in the western Rhine Delta',\n",
       " '572fe53104bcaa1900d76e6b': 'in order to counteract the constant flooding and strong sedimentation in the western Rhine Delta',\n",
       " '572f59b4a23a5019007fc587': 'the greater density of cold water',\n",
       " '573003dd947a6a140053cf43': 'Rates of sea-level rise had dropped',\n",
       " '573003dd947a6a140053cf45': 'ongoing tectonic subsidence',\n",
       " '57300c67947a6a140053cff3': 'the generally accepted length of the Rhine was 1,230 kilometres (764 miles)',\n",
       " '572facb0a23a5019007fc865': 'the outbreak of the First World War',\n",
       " '572fbea404bcaa1900d76c5c': 'desire to encourage consensus amongst elected members',\n",
       " '572fc659b2c2fd1400568449': \"interest to a particular area such as a member's own constituency\",\n",
       " '572fcb6da23a5019007fc9f3': 'alter income tax',\n",
       " '572fd264b2c2fd14005684aa': 'for royal assent and it becomes an Act of the Scottish Parliament',\n",
       " '572fd8efb2c2fd14005684fd': 'due to their dispersed population and distance from the Scottish Parliament in Edinburgh',\n",
       " '572ff86004bcaa1900d76f69': 'to maintain their legitimacy',\n",
       " '572ffe6fb2c2fd14005686f1': \"prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups\",\n",
       " '57302cd004bcaa1900d772da': 'failure to consult and \"notorious intransigence',\n",
       " '57309bfb8ab72b1400f9c5e9': 'material resources and manpower in such a way that necessitated colonial expansion',\n",
       " '57309921396df919000961f8': 'Holy Club',\n",
       " '57309d31396df91900096211': 'the issue of laity having a voice and vote in the administration of the church',\n",
       " '57309d31396df91900096213': 'tensions over slavery and the power of bishops in the denomination',\n",
       " '5730aaa88ab72b1400f9c64e': 'the American Revolution',\n",
       " '5730c8a1f6cb411900e2449f': 'the George W. Bush Presidential Library',\n",
       " '5733d13e4776f419006612c6': 'to a combination of poor management, internal divisions',\n",
       " '5733f062d058e614000b6636': 'superior to that of the British',\n",
       " '5733f309d058e614000b664a': 'Washington continued toward Fort Duquesne and met with the Mingo leader',\n",
       " '573408ef4776f4190066175a': 'able to negotiate the retention of Saint Pierre and Miquelon',\n",
       " '573749741c4567190057445d': 'inertia',\n",
       " '5737a84dc3c5551400e51f5a': 'gradient of potentials'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at some predictions\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
